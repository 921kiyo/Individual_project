% \section{Learning Pipeline}
% \label{learning_pipeline_section}
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{./figures/architecture}
\caption{ILP(RL) pipeline. ILASP learns to generate a model of the environment, or hypothesis, and updates it based on the interaction with the environment. }
\label{fig:pipeline}
\end{figure}

The overall pipeline is shown in Figure \ref{fig:pipeline}. By interacting with the environment, an agent accumulates state transition experiences as positive examples, which is used by ILASP to learn and improve hypothesis.
The agent also records surrounding information it has seen as background knowledge, which is used to make a plan together with the hypothesis that ILASP learns by solving an answer set program.
Mechanisms of each step is explain in details in the following sections.

\section{Experience Accumulation}
\label{experience_accumulation}
\textcolor{red}{Draw an illustration of the difference between exploration and las part}\\
\textcolor{red}{Related these with Definition of ILASP}\\

The first step is to accumulate experience by interacting with the environment. Similar to an existing RL, an agent explores an environment following an exploration strategy
Every time the agent takes an action, these experiences are recorded in two different forms: \textit{state transition experience} as a positive example and \textit{environment experience} as background knowledge.

\subsection{State Transition Experience}
\textit{State transition experience} contains information about how the state transitions at each time step: the current state of the agent,
an action taken, the next state after the agent takes the action and surrounding information of the current state.

MDP
It is used as a positive example for inductive learning (E\textsuperscript{+} in ILASP), which is of the form:

% \begin{equation}
% \begin{split}
% \textsf{\#pos(} & \textsf{\{state\_after((X2,Y2))\}},\\
% & \{\textsf{all other state\_after that did not happen}\}, \\
% & \{\textsf{state\_before((X1,Y1)). action(A). surrounding information}\}).
% \end{split}
% \end{equation}
\begin{equation}
\begin{split}
\textsf{\#pos(} & \textsf{\{INC\}},\\
& \{\textsf{EXC}\}, \\
& \{\textsf{state\_before((X1,Y1)). action(A). surrounding information}\}).
\end{split}
\end{equation}

Where INC and EXC are inclusions and exclusions respectively. ASP is Answer Set Program P and H is the current hypothesis.

\begin{equation}
\begin{split}
    &\forall s \in \textsf{State, agent is at s, ASP of H does not contains s as state\_after, add s} \rightarrow \textsf{INC.} \\
    & \forall s \in \textsf{State, agent is not at s, ASP of H contains s as state\_after, add s} \rightarrow \textsf{EXC.}
\end{split}
\end{equation}
EXC is determined in two ways. First, they are determined by all other possible states that the agent did not take.
For example, if the agent takes an action "up" to move from (1,1) to (1,2), all other states that the agent could have taken but did not are exclusions ((1,0), (1,1), (0,1) and (2,1) in this case).

\begin{itemize}
    \item inclusions contain one state\_after((X2,Y2)), which represents the position of the agent in x and y axis after an action is taken
    \item exclusions contain all other state\_after((X,Y)) that did not occur
    \item context examples include state\_before((X1,Y1)), which represents the position of the agent in x and y axis before an action is taken,
    action(A) is the action the agent has taken, and surrounding information, such as surrounding walls.
\end{itemize}

Rewards are not used.
(Discussed in details in Chapter XX).


The inclusions and exclusions are determined by the following algorithms

context example are
\begin{itemize}
    \item the state that the agent was before taking action (represented as \textit{state\_before(x,y)})
    \item an action that the agent takes (representend as \textit{action(a)})
    \item surrounding information of \textit{state\_before(x,y)}, such as walls.
\end{itemize}

% \textcolor{red}{There is no negative example as XXXX.}

Using these positive examples, the agent is able to learn and improve hypothesis as it explore the environment and encounters new scenarios.

\begin{examp} \normalfont (Positive examples).

% \begin{wrapfigure}{r}{0.5\textwidth}
%   \begin{center}
%     \includegraphics[width=0.48\textwidth]{./figures/pipeline_example1}
%   \end{center}
%   \caption{A gull}
% \end{wrapfigure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.7\textwidth]{./figures/pipeline_example1}
\caption{Illustration of generating positive example of state transition}
\label{example_pos_example}
\end{figure}

We use a simple 5x5 gridworld environment to hilight each steps of the pipeline. To illustrate how an agent gains a positive example, suppose the agent takes an action "right" to move from (2,4) to (3,4) cell, as shown on Figure \ref{example_pos_example} on the right.
All other alternative states that the agent could have ended up by taking different actions
(down, up, and left) are in the exclusions.
Context examples are the state that the agent is before taking an action and surrounding walls information. The following positive example is generated.

\begin{equation}
\begin{split}
    \textsf{\#pos(} & \textsf{\{state\_after((3,4))\},}\\
                    & \textsf{\{state\_after((2,4)),state\_after((1,5)),state\_after((0,4)),state\_after((1,4))\},} \\
    & \textsf{\{state\_before((2,4)). action(right). wall((1, 4)). wall((4, 2)).\})}
\end{split}
\end{equation}

This example will be used to learn how to move up as one of the agent's hypotheses.

Similarly, the agent is at (4,4) and tries to move right, as shown on the left on Figure \ref{example_pos_example}. In this case, however, there is a wall at (5,4) and therefore the agent ends up in the same state. From this example, the following positive example is generated:

\begin{equation}
\begin{split}
\textsf{\#pos(} & \textsf{\{state\_after((4,4))}, \\
                & \textsf{\{state\_after((4,3)),state\_after((3,4)),state\_after((5,4)),state\_after((4,5))\}} \\
                & \textsf{\{state\_before((4,4)). action(right). wall((5,4)). wall((4,5)).\}).}
\end{split}
\end{equation}

\end{examp}
\label{state_transition_example}

\subsection{Environment Experience}

While the agent explores the environment, it also keeps all the surrounding information as background knowledge,
which will be stored in different repository, and are later used to generate a sequence of actions plan using H.

These background knowledge corresponds to B in ILASP definition.

This does not include state transition experience, as these state\_before and action takens are different at every timestep.
In static environment (e.g no moving enermy), environment information remain the same across time, and thus it will be beneficial to remember.

% This principle is different from most RL methods, as they are model-free learning.
% As described in XX,, model-based learning converge to optimal policy more efficiently.

In a simple maze, these could be all wall position that the agent has seen so far, which can be
wall((1, 5)). which represents the location of the wall.
Another example could be a location of a teleportation if the agent sees it.

These environment experiences are part of context examples in the positive examples.

\begin{examp} \normalfont (Background Knowledge).

Using the same example as in Figure \ref{example_pos_example},
The first postive examples contain two walls, wall((1,4)) and wall((4,2)), and they will be stored as background knoledge. These information are useful for the plan generation.
\end{examp}

\section{Inductive Learning}
\label{induction}
Throughout the learning process, the agent accumulates positive examples and learn hypothesis H.

Our learning task is to find a hypothesis H < SM such that B U H has at least one answer set that extends at least one postiive example and none of the negative example.

In order to execute inductive learning using ILASP, the following definitions are supplied as well as the positive examples,

\subsection{Search Space}

Hypotheses are a subset of a search space Sm using a language bias. In our case, the seach space should contain any conditions that defines a state after the transition.
In order to execute ILASP, a \textit{search space} of possible hypotheses is required, which is defined using a \textit{language bias}.

\begin{equation}
\begin{split}
&\textsf{\#modeh(state\_after(var(cell))).}\\
&\textsf{\#modeb(1, adjacent(const(action), var(cell), var(cell)), (positive)).} \\
&\textsf{\#modeb(1, state\_before(var(cell)), (positive)).} \\
&\textsf{\#modeb(1, action(const(action)),(positive)).} \\
&\textsf{\#modeb(1, wall(var(cell))).} \\
\end{split}
\end{equation}
Without these in the form of mode bias, the search space for ILASP will be empty.

(positive) states that the predicates only appears as positive predicates and not negation as failure, therefore reducing the seach space. In this case, wall(var(cell)) could appears as "not wall". 
var is variables of type cell. 
action is constant, means it has to be grounded as a particular action, because we want to learn different hypothesis for different action. 

where \textit{var(t)} and \textit{const(t)} are a placeholder for variable and constant terms of type t respectively.

\textit{const(t)} must be specified as \#constant(t,c), where t is a type and c is a constant term.
In our environment, action is specified as constant since ILASP should learn different hypothesis for each action.

\begin{equation}
\begin{split}
&\textsf{\#constant(action, right).}\\
&\textsf{\#constant(action, left).}\\
&\textsf{\#constant(action, down).}\\
&\textsf{\#constant(action, up).}\\
&\textsf{cell((0..7, 0..6)).}
\end{split}
\end{equation}

As we describe in XXX, the search space increases in propotion to the complexity of learning tasks, which slows down the learning process.
For example, the search space in this particular setting is in XX.

\subsection{Background Knowledge}

In addition to the above search space, the following background knowledge is given. This is an additional assumption, 
that the agent is assumed to be able to see surrounding cells. This definition of adjacent is required in order to learn the state transition.
In normal RL senarios, the agent is only able to perceive MDP of a state that the agent is currently at. 

\begin{equation}
\begin{split}
&\textsf{adjacent(right, (X+1,Y),(X,Y)) :- cell((X,Y)), cell((X+1,Y)).} \\
&\textsf{adjacent(left,(X,Y),  (X+1,Y)) :- cell((X,Y)), cell((X+1,Y)).} \\
&\textsf{adjacent(down, (X,Y+1),(X,Y)) :- cell((X,Y)), cell((X,Y+1)).} \\
&\textsf{adjacent(up,   (X,Y),  (X,Y+1)) :- cell((X,Y)), cell((X,Y+1)).} \\
\end{split}
\end{equation}

\#max\_penalty defines the maximum size of the hypothesis, by default it is 15.
Increasing \#max\_penalty allows ILASP to learn longer hypothesis in expense of longer computation.
\#max\_penalty(50).

Together with the above defition as well as accumulated positive examples, ILASP is able to learn an hypothesis. The quality of H depends on the experiences for the agent.
For example, In the early phase of learning, the agent does not have many examples, and learns an hypothesis that may not be insightfull.
For example, if the agent has only one positive example,

Next, the scope of \textit{cell} are defined, as cell((0..X, 0..Y)), where X and Y are size of width and height respectively.

Finally, since our learning task is the rule of the game, which involve state transition, it needs to know how it means to be "being next to XX",
Therefore the following assumptions are provided as background knowledge.
\begin{equation}
\begin{split}
&\textsf{state\_after(V0) :- adjacent(right, V0, V1), state\_before(V1), action(right), not wall(V0).}\\
&\textsf{state\_after(V0) :- adjacent(left, V0, V1), state\_before(V1), action(left), not wall(V0).}\\
&\textsf{state\_after(V0) :- adjacent(down, V0, V1), state\_before(V1), action(down), not wall(V0).}\\
&\textsf{state\_after(V0) :- adjacent(up, V0, V1), state\_before(V1), action(up), not wall(V0).}
\end{split}
\end{equation}

Learnt hypotheses are improved when a new positive example is added and the current hypothesis does not cover the new positive example.
It it does cover it, there is no need to rerun ILASP again. 

\begin{examp} \normalfont (Inductive Learning).
Using the same example as
XXX
\end{examp}

This definition itself could be learnt by setting another learning tasks, and it is a potential learning problem.
% TODO state represetation
However, we focus on learning task of the rule of the game in this paper.

The full details for ILASP learning tasks is described in Appendix XXX.
Positive excludes the possibility of negation as a failure in order to reduce the search space.

Future research will relax these assumptions and attemp to learn more general hypothesis,
e.g learning adjacent defintion.

These learnt H will be used to generate a plan in the abduction phase.

After executing the plan, the agent will have more positive examples, which will be used to improve the quality of H.

The learnt hypothesis is XXX

This hypothesis, for example, does not explain how to move "down". In order to learn how to move "down", it needs an positive example of moving up.

later on H improving as we collect more examples as well as background knowledge.

\section{Plan Genreation}
\label{Plan_genreation}

The plan generation is executed only after the agent finds the goal. Untile the goal is found, the agent keeps exploraing randomly.
In RL algorithm, the agent also needs the positive reward by reaching a termination state. 
Once the agent find the goal once, we can generate a plan using the current hypothesis by solving Answer Set Program.

If the hypotheses were not accurate, clingo might not generate all the actions leading to the goals.

The syntax of ASP is different from ILASP phase, because we need to include time sequence when solving ASP.
In ILASP, it is only state\_before and state\_after, but in plan generation, there will be more than one state transition.
These syntax conversion needs to be done for learnt hypothesis

\begin{equation}
\begin{split}
&\textsf{1\{action(down,T); action(up,T); action(right,T); action(left,T); action(non,T)\}1} \\
&\textsf{ :- time(T), not finished(T).}\\
\end{split}
\end{equation}

This choice rule states that action must be one of four actions (defined maximum and minimum numbers in 1),
T is the time step at which the agent takes each action, unless \textit{finished(T)} is grounded.

\textit{finished(T)} is associated with goal definition, and it is defined as:

\begin{equation}
\begin{split}
&\textsf{finished(T):- goal(T2), time(T), T} \geq \textsf{T2.}\\
&\textsf{goal(T):- state\_at((5, 1), T), not finished(T-1).}\\
&\textsf{goalMet:- goal(T).}\\
&\textsf{:- not goalMet.}
\end{split}
\end{equation}

\begin{examp} \normalfont (Plan Generation).
As shown in example XX, the learnt example will be converted by adding time sequence for ASP plan generation.

Together with hypothesis, the background knowledge will used to solve for answer sets program. 

However, since hypothesis is not complete, there is more than one answer set at each time step. Since one of the answer sets state\_at is correct, the rest will be in the exclusions in the answer set, 
which further improve the hypothesis.


In this example, the following is the answer set program

The answer set using the hypothesis XXX is 

XXX. 

The answer set using the improved hypothesis is XXX, 

Which correctly returns a sequence of actions and predicted states. 

\end{examp}

\section{Plan Execution}
\label{Plan execution}

The answer sets returned by clingo is a set of states and actions, which is the plan of the agent at each time step.

The set of states is of the form state\_at((X,Y),T), where X and Y represent x-axi and y-axi in a maze respectively, T represents a time that the agent is at
this particular X,Y cell.

action(A,T) tells which action the agent should take at each time. By following the actions, the agent should collect both predicted state that the
agent will end up, and the observed state that the agent actually end up. If there is a difference between these two, either B or H do not correctly represent
the model of the environment, so needs to be improved.

When the agent encounters a new environment (e.g a new wall), this new information will be added to its background, which will be used to improved the hypothesis. 
Similarly, after executing an action by following the generated plan, the agent receives a new positive example. If the new positive example is not covered by the current hypotheses, 
ILASP reruns using the new example to improve the hypotheses. 
next time ILASP gets executed.

For example,
\begin{equation*}
\begin{split}
&\textsf{state\_at((1,1),1), action(right,1)}\\
&\textsf{state\_at((2,1),2), action(right,2)}\\
&\textsf{state\_at((3,1),3), action(right,3)}\\
&\textsf{state\_at((4,1),4), action(right,4)}\\
&\textsf{state\_at((5,1),5)}, \cdots
\end{split}
\end{equation*}

At the start of the learning, H is usually not correct or too general, using this H will generate lots of answer sets that are not useful for the planning.
These examples will be collected and included as exclusions of a new positive example.

To avoid the agent from being stuck in a sub-optimal plan, the agent deliberately discards the plan and takes an random action with a probability of
epsilon (which is less than 1) TODO define this mathematically.
When the agent deviates from the planning, it often discovers new information, which will be added to B.
Exploration is necessary to make sure that the agent might discovers a shorter path than the current plan, which will be demonstrated in the experiment.

Define them here

ILP(RL) works by

It buids the model of the environment by improving two internal concepts: hypothesis H and background knowledge B.
% First, an agent explores an environment by taking random actions until it reaches the goal.

In the further research, we could experiment with a more sophisticated exploration strategy, such as XXX and YYY.

This is formally defined in Algorithm.

\begin{algorithm}
\caption{ILP(RL)}\label{euclid}
\begin{algorithmic}[1]
\Procedure{ILP(RL) (B and E)}{}

\While {True}
    \State $\textit{H (inductive solutions)} \gets \text{run ILASP(T)}$
    \State $\textit{plan(actions, states) answer sets} \gets \text{AS(B, H)}$
    \While {actions in P}
        \State $\textit{observed state} \gets \text{run clingo(T)}$
        \If {$ \textit{observed state} \neq \textit{predicted state} $}
            \State $\textit{H} \gets \text{run ILASP(T)}$
            \EndIf
        % \If {$ \textit{observed state not equal \textit{predicted state $}
        % \EndIf
    \EndWhile
    % \If {$ new \ background \ encountered $}
    %     % \State $\textit{H} \gets \text{run ILASP(T)}$
    % \EndIf
    % \For{i from 0 to N}
    %     \If {$ A[i]\ is \ in \ T$}
    %         \State \Return $FALSE$
    %     \Else
    %         \State Add A[i] to T
    %     \EndIf
    % \EndFor
% \State \Return $TRUE$
\EndWhile

\EndProcedure
\caption{ILP(RL) }
\end{algorithmic}
\end{algorithm}

Everytime the agent executes an action by following the plan, it checks whether the observed state is that is expected.

If there is a difference between the two, either

B is incorrect

H is not sophisticated enough,

If that is the case, the agent runs ILASP again using more positive examples it collected during the plan execution.

\section{Exploration}
\label{exploration}

In RL, there is a tradeoff between exploration and exploitation. While the agent exploit what is already experienced and knows the best action selection so far, 
it also needs to explore by taking a new action to discover a new state, which might make the agent discover an even shorter path and therefore higher total rewards in the future or in the long term. 
This exploration-exploitation issues has been an active reseach area in RL. 
In our case, if the goal is found and the model is correct, it is likely that following the plan will maximize the total rewards, 

ILP(RL) kicks in once the agent reaches a goal once. However it is likely that the agent has not seen all the environment
and therefore is likely to be in a sub-optimal plan. Therefore, similar to RL algorithm, the agent also has to explore a new state.
There are a number of exploration strategy in RL (such as Boltzman approach, Count-based and Optimistic Initial value TODO REFERENCE).
One of the most commonly used strategy is $\epsilon$-greedy strategy. As described in Chapter XXX, the agent takes an random action

Another common way is to decay the $\epsilon$ by the number of episodes, since at the beginning it is more likely that the agent has not fully explored the environment and therefore higher epsilon.



While statistical RL algorithms, such as Q-learning or Tile-coding explained in XX, update Q-value by a facter of alpha, 
ILP(RL) tends to strictly follow the plan the generated. Also it is known that model-based RL tends to get stuck in a sub-optimal if the model is incorrect, ILP(RL) also needs a exploration
And since the agent does not know when the perfect hypothesis in a particular environment is fully realised, it needs to keep exploring the environment even though



This strategy may not be appropriate in cases there safety is a priority (since it is random action.)
It is simple to implement.
In the case of ILP(RL), the agent discard the plan from the abduction with a probably of epsilon and takes a random action in order to avoid getting stuck in a sub-optimal path.
When the agent takes an random action and move into a new state, the agents creates a new plan from the new state and continue to move forward.



This exploration point will be highlighted in Experiment XXX.

Epsilon needs to be larger than Q-learnig because

The reason for using random exploration is that it can be used for both benchmark and ILP(RL) and thus enables us to do a fair comparision between them.

\begin{examp} \normalfont (Plan Generation).
Suppose the plan of the agent is the following. 

In our implementation, exploration strategy is epsilon, so with the probablity of XX, the agent discards the plan and randomly selects an actions. 

Suppose the agent decides to take an random action and moves "up", which may help it discover a new state. 
From the new state, the agent again plan to the goal from the current state. In this case, the new plan is 

XXX. 

After taking an random action, the agent again has a probably of epsilon for taking an random action. 
Throughout following the new plan, there is a small probalbility of chance that the agent takes an random action.

\end{examp}
    
The current pipeline simply uses a simple random exploration, therefore even if the agent takes an random action and goes to a different state other than the planed one, 
the agent does the replan from the new state and quickly correct to the original plan path. This means it is likely that the agent of ILP(RL) only explores the adjacent cells and if there is a shorter path or new state

far from the current state, the agent is unlikely be able to find the new state unless epsilon value is very high. 


\section{Implementation}
\label{Implementation}

% We use GVG-AI Framework for our experiments, which was created for the General Video Gamea AI Competition\footnote{http://www.gvgai.net/},
% a game environment for an agent that should be able to play a wide variety of games without knowing which games are to be played.
\subsection{Experiment Platform}

\begin{figure}[!ht!b]
\centering
\includegraphics[width=0.5\textwidth]{./figures/env_sample}
\caption{VGDL game example}
\label{VGDL_sample}
\end{figure}

We use the Video Game Definition Language (VGDL), which is a high-level description language for 2D video games providing a platform for computational intelligence research (\cite{Schaul2013}).
The VGDL allows users to easily craft their own environments, which makes us possible to do various experiments without relying on a default environment. The VGDL platform provides an interface with OpenAI Gym (\cite{Brockman2016}), which is a commonly used benchmark platform.
The base game is a simple maze as shown in Figure \ref{VGDL_sample}.
There are 3 different types of cells: a goal cell, walls and paths. The agent can take 4 different actions: up, down, right and left.
The environment is not known to the agent in advance, and it attempts to find the goal by exploring the environment.
In all experiments, the agent receives -1 in any states except the goal state, where it gains a reward of 10.
Once the agent reaches the goal, or termination state, that episode is finished and the agent start the next episode from the starting point.

\subsection{Technology}

All of the codes are written in Python 3 for the whole pipeline described in \ref{fig:pipeline}, where ILASP 2i\footnote{https://sourceforge.net/projects/spikeimperial/files/ILASP/} is used for inductive learning and clingo 5 is used for solving answer sets for a plan.

ILASP version 2i, which is designed to scale with the numbers of examples.

ILASP cache caches relevant sets of examples, so everytime ILASP runs the same task except extra examples each time, ILASP runs from where it finished the learning last time and start from there rather than going through all the examples again.
The code is available in https://github.com/921kiyo/ILPRL.

The bottleneck for the learning in terms of learning time is hypothesis improvement. In order to optimise it,

ILASP 2i
