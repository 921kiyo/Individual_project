In this Chapter, we conducted 4 different evaluations in simple maze environments to investigate how the ILP(RL) agent learns and finds the optimal policy.

\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Evaluation Metrics}
\label{subsec:evaluation_metrics}

As introduced in Section \ref{sec:motivation}, our motivation is to improve the learning efficiency and capability of transfer learning in RL.
Therefore, these are the two main measurements for the performance of ILP(RL).
The learning efficiency is measured in three different ways: performance of optimal policy, convergence of inductive learning and runtime of ILP(RL). Due to the random exploration for both ILP(RL) and a benchmark, the performance of each experiment varies. Especially ASP planning of ILP(RL) starts only when the agent finds a terminal state and it is dependent on an random exploration. In order to smooth the impact of the randomness, we ran 30 experiments per evaluation and computed an average for all the evaluation metrics. Within an experiment, an agent is allocated 250 time steps per episode, 100 episode per experiment.

In order to compare the performance of ILP(RL) with an existing RL method, we use Q-learning as our base benchmark.
Q-learning is widely used RL technique, and given the environments used for the experiments are a simple environment in that
it is discrete and deterministic, this method is sufficient as a benchmark.

% TODO Q-leaning Implementation
% As defined in Algorithm XX (Q-learning) in Section \ref{td_learning_section}, we randomly 
% randomly initialise the state-action value functions except the terminal state, and we measure the same evaluation metrics as ILP(RL), 
% and compare the performance of the two algorithms in order to examine how well ILP(RL) learns faster than Q-learning.
% (Optimistic and pessimistic initialisation.)

\subsubsection{Performance of optimal policy}
The performance of ILP(RL) is compared with a benchmark in terms of
 optimal policy, which is measured in terms of the total reward that an agent gains per episode by following its optimal policy.
For all the evaluations, the agent receives -1 for any states except a terminal state and receives +10 when it reaches the terminal state.
For example, if an agent requires the minimum 10 actions to get to a terminal state, the maximum total reward that the agent could gain per episode is 0 (-1 reward at each action +10 for reaching the terminal state).

In RL analysis, the optimal policy is measured without exploration nor learning. For ILP(RL), for example, the agent might still take an random action even when the ASP planning is already optimal. Thus, the exploration policy does not allow us to measure the performance of it's optimal policy. Thus at after every episode, we disable an random exploration and inductive learning, and run the same experiment, so that the agent can only follow its optimal policy. In the case of ILP(RL), if the agent does not know a terminal state or hypothesis, it does not have any policy because ASP planning cannot be executed, and therefore the agent gets -250 total reward.
\subsubsection{Convergence of Inductive Learning}
The convergence of inductive learning is measured to see the learning curve of inductive learning phase in ILP(RL), which is specified as follows:
\begin{equation}
\begin{split}
\frac{\textsf{The cumulative number of ILASP calls per time step}}{\textsf{The total number of ILASP calls in all episodes}} \in [0,1]
\end{split}
\end{equation}
This gives a normalised convergence rate of inductive learning with the maximum 1. For example, if the total number of ILASP calls in all episodes is 10 and there is a ILASP call at time step 1 episode 1, it is recorded as 0.1. When there is another ILASP call at time step 2 episode 1 after the first ILASP call, it is recorded as 0.2. The 10th ILASP call in this evaluation is recorded as 1. 
\subsubsection{Runtime}
We recorded runtime of both ILP(RL) and a benchmark at each episode, and plot the cumulative runtime over episodes. For ILP(RL), we also recorded the average runtime of ILASP calls per evaluation. All the evaluations were conducted in Linux Operating System with Intel i7-6560U CPU and 8GB RAM.

\subsection{Parameters}
\begin{table}[!ht!b]
\centering
\begin{tabular}{lll}
\hline
Parameter            & ILP(RL)    & Q-learning      \\ \hline
The number of experiment per evaluation& 30       & 30       \\
The number of episode per evaluation& 100        & 100        \\
Time steps per episode& 250        & 250        \\
% Discount rate        & 0,5       & 1.4e-2       \\
$\alpha$ (learning rate)        & N/A       & 0.5       \\
$\epsilon$ (epsilon)         & 0.1        & 0.1        \\
Reward for any states except a terminal state  & -1        & -1       \\
Reward for a terminal state     & 10        & 10       \\
\end{tabular}
\caption{List of parameters used in the evaluations}
\label{table:parameter}
\end{table}

All the parameters used in the evaluations are summarised in Table \ref{table:parameter}.
We trained the agents with maximum 250 time step, 100 episodes, and conducted the same experiment 30 times in each environment. 
The number of time steps should be sufficient for the both algorithms to reach a terminal state by the $\epsilon$-greedy exploration strategy, 
which we specify 250 time steps for all experiments. 
The number of episode is specified such that both ILP(RL) and Q-learning eventually reaches the optimal policy.
Every episode starts from a starting state. If the agent reaches the terminal state within 250 time steps, the episode is complete and the next episode starts with the fixed starting state. 

The learning rate  $\alpha$ for Q-learning, as shown in Equation \ref{eq:q_learning}, determines how much Q-value is updated each time. Since our environments are relatively simple, we use 0.5 for $\alpha$. For the same reason, $\epsilon$ for both ILP(RL) and Q-learning is 0.1.

The rewards were arbitrarily assigned -1 for all states except the terminal state, and 10 for the terminal state.

We conducted 4 different evaluations using different environments to highlight each aspect of ILP(RL) algorithm.
\section{Learning Evaluation}
\label{sec:learning_evaluation}

\subsection{Evaluation 1: Baseline}
\label{subsec:experiement1_setup}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.5\textwidth]{./figures/experiment1}
\caption{Game environment for Evaluation 1}
\label{fig:experiment1}
\end{figure}
    
The purpose of the first evaluation is to highlight how ILP(RL) agent learns the hypotheses using inductive learning and executes an ASP planning.
The environment is a simple maze where the terminal state is located the right upper corner as shown in Figure \ref{fig:experiment1}.
The shortest time step between the agent's starting state and the terminal state is 18 steps, thus the maximum total reward the agent could gain is -8.

\subsection{Evaluation 1: Result}
\label{subsec:experiment1_result}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.7\textwidth]{./figures/experiment1_test}
\caption{Evaluation 1: optimal policy }
\label{experiment1_result}
\end{figure}

Figure \ref{experiment1_result} shows the performance of optimal policy.
ILP(RL) reaches the optimal policy faster than Q-learning: ILP(RL) reaches the optimal policy at between 40 and 50 episode, 
whereas Q-learning reaches the optimal policy at around 60 episode. ILP(RL) learns the optimal policy in earlier episode because once the terminal state is found, the planning is always correct since there is only one way to reach the terminal state. The variations of the convergence to a optimal policy is dependent on how quickly the agent finds the terminal state. 
Since the exploration of ILP(RL) is based on $\epsilon$-greedy policy, this result shows that there is a potential that a better exploration policy further improves the ILP(RL) learning.

The convergence of Q-learning for reaching an optimal policy is steeper than that of ILP(RL). This is because the state-action value functions are updated with the rate of $\alpha$, and the optimal policy for Q-learning is achieved when the optimal state-action value functions are achieved. By contrast, 
ILP(RL) gradually learns state transition of the environment using inductive learning and uses the background knowledge to accurately plan. 

Overall this result shows that ILP(RL) converges to the optimal policy faster than Q-learning in this simple scenario, achieving more data-efficient learning.

\lstinputlisting[
%   language = Prolog,
  caption  = {Hypotheses for experiment 1},
  label = {list:experiment1_hypothesis}
]{experiment1_hypothesis.pl}

In addition to the data-efficient learning, what the agent has learnt with ILP(RL) is expressive.
Learnt hypotheses are shown in \ref{list:experiment1_hypothesis}, which are state transition for all directions, both when an adjacent state is a wall or not wall. The learnt state transitions are easy to understand for human users, and general in that they can be applied to a different similar environment.

Next, we see how the agent learns the hypotheses over time. We plot part of the learning convergence for ILASP at episode 0 in Figure \ref{experiment1_ilasp}.
It converges to 1 after 200 episode, and shows that the agent learns the target hypotheses at the episode 0.
Since the two variables that the agent require to construct optimal answer set planning are the hypotheses and background knowledge, and the target hypotheses are learnt at episode 0, the convergence of finding an optimal policy for ILP(RL) is dependent on how quickly the agent finds a terminal state. 

\begin{figure}[!htb]
\centering
\includegraphics[width=0.7\textwidth]{./figures/experiment1_ilasp}
\caption{Normalised learning convergence by ILASP for experiment 1}
\label{experiment1_ilasp}
\end{figure}

Finally, we compare the runtime of two algorithms. 
The Figure \ref{exp1_runtime} shows that the runtime of ILP(RL) in the first few episodes is significantly high. 
This is due to the fact that ILP(RL) runs ILASP calls to learn the hypotheses at the beginning of episodes.
The Figure \ref{exp1_runtime} therefore highlights an issue that inductive learning is likely the bottleneck in terms of computational time.
This issue may not be critical in cases where the time of the time between the time steps is not an issue. If the performance is measured in terms of computation time rather than the number of iterations, 
ILP(RL) does not perform better than Q-learning. 
The average runtime of inductive learning is 5.579 seconds, and there are on average 12.83 times inductive learning per episode in this environment.
% RUNTIME FOR ILASP: 5.579039041812603
% ILASP RUNS TOTAL: 12.83 times
The ASP planning is not a bottleneck of ILP(RL), but still takes longer time than Q-learning, as can be observed by the divergence of cumulative runtime between the two algorithms. 
This experiment show that while ILP(RL) learns faster than Q-learning in terms of the number of episodes, it suffers from increasing computational time due to inductive learning as well as ASP planning.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.7\textwidth]{./figures/experiment1_runtime}
\caption{Runtime comparison}
\label{exp1_runtime}
\end{figure}

\subsection{Evaluation 2: Extended Baseline}
\label{subsec:experiement2_setup}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.5\textwidth]{./figures/experiment2_setup}
\caption{Game environment for Evaluation 2}
\label{experiment2}
\end{figure}
Experiment 2 was conducted to see if the agent finds a optimal path using a teleport. The environment is the same as experiment 1 except the presence of teleport link and three extra walls to surround the destination teleport.
In the environment shown in Figure \ref{experiment3},
there are two ways to reach the goal: using a floor path to get the goal located on the top right corner, or using a teleport.
The environment is designed such that using a teleport is a shorter path and therefore gives higher total reward.
Compared to Experiment 1, two extra language biases are added as follows:
\begin{equation*}
\begin{split}
&\textsf{\#modeb(1, link\_start(var(cell)), (positive)).}\\
&\textsf{\#modeb(1, link\_dest(var(cell)), (positive)).}
\end{split}
\end{equation*}

\textsf{link\_start(var(cell))} is a state for departure of the teleport and \textsf{link\_dest(var(cell))} is the destination of the teleport. 
The teleport link is one-way: \textsf{link\_start} takes the agent to \textsf{link\_dest}, but \textsf{link\_dest} does not take the agent back to \textsf{link\_start}.
This extra types allows ILASP to learn additional hypothesis.
The full learning task for this experiment is in Appendix XX.

In VDGL game environment, when the agent steps onto \textsf{link\_start}, it immediately takes the agent to \textsf{link\_dest}. This means that the agent moves two states in one time step.
Since our ASP translation part takes the state transition as \textsf{state\_before} and \textsf{state\_after}, 
we deal with this situation such that the agent receives two positive experiences in this time step rather than two time steps.

Also link\_start and link\_dest need to be stored in background knowledge rather than as context examples,
because ILP(RL) needs to generate exclusions regarding the link behaviour.
% (TODO EXPLAIN MORE).
The link locations need to be available for all positive examples so that ILASP correctly learns different a valid move for floor and teleport, which is shown in Figure XX below.

Because of the teleport link, the shortest path is 13 steps to reach the terminal state. Thus the maximum total reward that the agent could gain is -3.

\subsection{Evaluation 2: Result}
\label{subsec:experiment2_result}
% The game is designed such that
    
\begin{figure}[!htb]
\centering
% \includegraphics[width=0.55\textwidth]{./figures/experiment2_training}
\includegraphics[width=0.7\textwidth]{./figures/experiment2_test}
\caption{Results of experiment 3: learning curve (left) and testing (right)}
\label{experiment2_training}
\end{figure}

The same as the Evaluation 1, both training and test performance converges faster than that of Q-learning.

\lstinputlisting[
  caption  = {Incomplete hypotheses for experiment 2},
]{experiment2_hypothesis_intermediate.pl}

To highlight the learning process of the new concept of teleport link, Figure \ref{experiment2_ilasp_imcomplete} is an intermediate incomplete hypothesis learnt by ILASP.
These hypotheses are generated just after the agent steps onto the \textsf{link\_start}. However, the first hypothesis says
when link\_dest is available state\_after is true. Since link\_dest is available in background knowledge rather than context,
when solving for answer sets to generate a plan, it generates incorrect state\_after at every time step.

However, as shown in Definition \ref{def:ILPRL_exc}, these generated state\_after are all incorrect and therefore will be added to exclusions of the next positive examples.
These exclusions will later refines hypotheses and results in Figure \ref{experiment2_ilasp_complete}.
The final complete hypotheses are show in Listing \ref{list:exp2_final_hypotheses}.

\lstinputlisting[
  caption  = {Complete hypotheses for experiment 2},
  label = {list:exp2_final_hypotheses}
]{experiment2_hypothesis.pl}

% \begin{equation}
% \begin{split}
% &\textsf{state\_after(V1) :- link\_start(V0), link\_dest(V1), state\_before(V0).}\\
% &\textsf{state\_after(V0) :- link\_dest(V0), state\_before(V0), action(right).}\\
% &\textsf{state\_after(V1) :- adjacent(left, V0, V1), state\_before(V0), action(right), not wall(V1).}\\
% &\textsf{state\_after(V0) :- adjacent(left, V0, V1), state\_before(V1), action(left), not wall(V0).}\\
% &\textsf{state\_after(V1) :- adjacent(up, V0, V1), state\_before(V0), action(down), not wall(V1).}\\
% &\textsf{state\_after(V0) :- adjacent(up, V0, V1), state\_before(V1), action(up), not wall(V0).}\\
% &\textsf{state\_after(V1) :- adjacent(left, V0, V1), state\_before(V1), action(left), wall(V0).}\\
% &\textsf{state\_after(V1) :- adjacent(down, V0, V1), state\_before(V1), action(down), wall(V0).}\\
% &\textsf{state\_after(V1) :- adjacent(up, V0, V1), state\_before(V1), action(up), wall(V0).}
% \end{split}
% \label{experiment2_ilasp_complete}
% \end{equation}

Compared to the Evaluation 1, there are two new hypotheses due to the presence of the teleport links.
These learnt hypotheses are also applicable to an environment where there is no link, such as a game in experiment 1.
In this case, the first two hypotheses in Listing \ref{list:exp2_final_hypotheses} are never be used 
since the body predicates relating to link\_start(V0), link\_dest(V1) are never be satisfied.
% RUNTIME AVERAGE: 95.47274640401204
% RUNTIME COUNTS: 16.233333333333334

Figure \ref{experiment2_ilasp} shows the learning convergence of inductive learning at episode 0.
Similar to Experiment 1, most of inductive learning occur at the beginning of the episode, as shown in Figure \ref{experiment2_runtime}. 
This result confirms that the agent learns inductive learning at the beginning of episode.

Figure \ref{experiment2_runtime} show the runtime of ILP(RL) and Q-learning. 
Despite the fact that the size of the environment is the same as Experiment 1, 
there is a significant increase of runtime for ILP(RL) at the beginning of episode. 
This is because of the increase of search space as we added extra language bias.

% There are increase of runtime after episode 0.
% This is due to the fact that the agent later discovers a teleport link, which is a new hypothesis to be learnt. 
The average runtime of inductive learning is 95.47 seconds, and there are on average 16.23 times inductive learning per episode.

\begin{table}[!ht!b]
\centering
\begin{tabular}{lll}
\hline
Parameter            & ILP(RL)    & Benchmarks      \\ \hline
Runtime per episode (seconds) & 95.47        & 1        \\
Average ILASP time (seconds)& XXX        & N/A        \\
The number of ILASP calls &  16.23      & N/A       \\
Hypothesis space &  XXX      & N/A       \\
\end{tabular}
\caption{Comparison of runtime}
\label{param}
\end{table}
TODO is the table a good idea?

While ILP(RL) still learns faster than Q-learning in terms of the number of iterations, 
the result of Experiment 2 shows that, the learning time per episode increases with respect to the size of search space, 
which corresponds to the number of symbolic representations that the agent needs to learn in the environment.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.7\textwidth]{./figures/experiment2_ilasp}
\caption{Normalised learning convergence by ILASP for experiment 2}
\label{experiment2_ilasp}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.7\textwidth]{./figures/experiment2_runtime}
\caption{Runtime comparison}
\label{experiment2_runtime}
\end{figure}

% \clearpage
% \newpage
\section{Transfer Learning Evaluation}
\label{sec:transfer_learning_evaluation}

\subsection{Evaluation 3: Transfer Learning}
\label{subsec:experiement3_setup}

\begin{figure}[!htb]
\centerline{
\includegraphics[width=0.5\textwidth]{./figures/experiment3_before}
\includegraphics[width=0.5\textwidth]{./figures/experiment3_after}
}
\caption{Game environment for Evaluation 3: before (left) and after (right) transfer learning}
\label{experiment3_setup}
\end{figure}

In Experiment 3, we investigated the possibilities of transfer learning between similar environments.
We trained the ILP(RL) agent using the environment on the left in Figure \ref{experiment4}, 
and transfer the learnt hypothesis as well as positive examples to a new environment on the right in Figure \ref{experiment4}.
The learnt hypotheses are valid moves in the environment and a general concept that is applicable to any similar environments. 
Positive examples are also transferred because, if there is a new hypothesis that the agent needs to learn in a new environment, 
ILP(RL) needs to refine the hypothesis by running ILASP in the new environment.
Thus the all the positive examples are also transferred as well as the learnt hypotheses. 
Background knowledge are not transferred since the wall locations are different in a new environment.
The agent therefore starts the exploration of the new environment with an empty background knowledge and gradually collects them over time.
The terminal state is the same as that in the first environment, but the shortest path to the goal is different between the two environments as new shorter path is introduced in the right environment in Figure \ref{experiment3_setup}.

In addition to Q-learning, we use three extra agents for comparison as follows. 

\begin{itemize}
    \item Agent(TL): The agent with transferred hypotheses, examples and also remembers the location of the terminal state.
    \item Agent(noTL)\textsubscript{Goal}: The agent with transferred hypotheses, examples, but does not know the location of the terminal state.
    \item Agent(noTL)\textsubscript{noGoal}: The agent with no transferred information, neither the hypotheses nor the location of the terminal state is known to the agent.
    \item Q-learning
\end{itemize}

While this is a limited transfer learning since the terminal state is known in advance, this is still a useful transfer in cases where the terminal state is the same but the rest of the environment changes.

Listing \ref{exp3_hypotheses} is the hypotheses that is transferred to a new environment, which is acquired by training the agent in the environment once on the left of Figure \ref{experiment3_setup}.
The positive examples to be transferred are those the agent accumulated in the left environment, XXX examples in total.

\lstinputlisting[
%   language = Prolog,
  caption  = {Hypotheses for experiment 3},
  label = {exp3_hypotheses}
]{experiment3_hypothesis.pl}

\subsection{Evaluation Result 3}
The result is shown in Figure \ref{experiment3_training}.
For Agent(TL), since the complete hypotheses are already known to the agent as well as the terminal state, the agent can do ASP planning from episode 0.
The only information required is background knowledge for the ASP planning, namely the locations of the walls, which are quickly acquired and reached the maximum total reward at the very beginning of episodes.

The next best agent in terms of the convergence to the maximum total reward is Agent(noTL)\textsubscript{Goal}. Since the terminal state is known to the agent, 
the agent can do the planning from episode 0. However, the agent needs to learn the hypotheses. The reason that the convergence rate is almost the same as that of Agent(TL) is that
, the ILP(RL) learns the almost complete hypotheses in episode 0, as observed in Experiment 1 and 2. 
Thus there is no difference between Agent(TL) and Agent(noTL)\textsubscript{Goal} in terms of learning speed. 

What makes a difference for the convergence is whether the agent knows the terminal state, which can be seen by compering between Agent(noTL)\textsubscript{Goal} and Agent(noTL)\textsubscript{noGoal}.
The difference in terms of the iterations is that Agent(noTL)\textsubscript{noGoal} needs to find the terminal state first before starting the planning, which is a random exploration.

This observation shows that there is a promising potential for improving the exploration strategy to find the terminal state as soon as possible.

There was no ILASP calls in the new environments since the transferred hypotheses are already optimal and cover all the examples the agent encounters in the new environment.

\begin{figure}[!htb]
\centering
% \includegraphics[width=0.55\textwidth]{./figures/experiment3_after_training}
\includegraphics[width=0.7\textwidth]{./figures/experiment3_after_test}
\caption{Results of experiment 3: learning curve (left) and testing (right)}
\label{experiment3_training}
\end{figure}

\subsection{Evaluation 4: Extended Transfer Learning}
\label{subsec:experiement4_setup}
\begin{figure}[!htb]
\centering
% \includegraphics[width=0.5\textwidth]{./figures/experiment4_before}
\includegraphics[width=0.7\textwidth]{./figures/experiment4_after}
\caption{Game environment for Evaluation 4: before (left) and after (right) transfer learning}
\label{experiment4_setup}
\end{figure}

The transferred hypotheses are the same as that in Experiment 3, and the positive examples were collected the environment on the left in Figure \ref{experiment4_setup}.
The new environment, shown on the right in Figure \ref{experiment4_setup}, is the same except that there is teleport links.
This is a new concept that did not exist in the trained environment and therefore the agent needs to learn it after the hypotheses are transferred.
The same as Experiment 3, we use four different agents.

\subsection{Evaluation 4: Result}
\label{subsec:experiment_result_4}

Figure \ref{experiment4_training_test} show the results of the learning and test performance. 
The agent is able to successfully learn the new concept and quickly finds the optimal policy at the early episodes.
This confirms that the transferred agents learns on top of what is already learnt. This experiment shows that the hypotheses is transferable even in cases where there is something the agent needs to learn in a new environment.

Also the difference of the state where the link is located does not cause any problems even when the positive examples in the previous environment is transferred, since the surrounding information is within the context rather than background knowledge. 
This shows the power of context dependent examples in RL scenarios.

\begin{figure}[!htb]
\centerline{
\includegraphics[width=0.55\textwidth]{./figures/experiment4_training}
\includegraphics[width=0.55\textwidth]{./figures/experiment4_test}
}
\caption{Result of experiment 4 (learning curve)}
\label{experiment4_training_test}
\end{figure}

The new hypotheses the agent learns are the same as that of Experiment 2. The hypothesis containing \textsf{link\_start} and \textsf{link\_dest} is what the agent learnt in the new environment.

\lstinputlisting[
%   language = Prolog,
  caption  = {Hypotheses for experiment 4},
]{experiment4_hypothesis.pl}

\clearpage

\section{Discussion}
\label{sec:discussion}

We investigated the properties of ILP(RL) using a simple maze environment. While the development of ILP(RL) is still in early stage and only a proof-of-concept, 
we observe both strengths as well as weakness of the current approach. We summarise both of these in the following sections.

\subsection{Strengths of ILP(RL)}
As observed in the 4 experiments, there are several advantages of ILP(RL) over Q-learning.

\begin{description}
\item[Faster learning convergence]
The ILP(RL) agent learns the valid move of the environment at the very early stage of learning and, as soon as it finds the terminal state, is able to generate a plan as a sequence of actions to reach the goal.
While this is a proof-of-concept approach, this way of RL is a new and the experiments show that this is a promising direction of the research.

\item[Transfer Learning]
Unlike existing RL algorithms, where it learns value functions or Q values, ILP(RL) learns a valid move as a hypotheses, which can be applied to similar but different environments.
We confirmed this with the experiments and, especially when the goal is known to the agent in experiment 3 and 4.
We also show that the agent can learn a new hypothesis on top of the transferred ones, which is very flexible in terms of applicability of the learnt hypotheses.

\item[Symbolic learning]
Since both planning and learning can be expressed in ASP syntax, the learning process is easy to understand for human users, and the learnt hypotheses are a very general rule of the game.
\end{description} 

% The full hypotheses were learnt in the very early phase of learning and exploration phase. Thus with sufficient exploration, the model of the environment is correct
% and therefore it is able to find the optimal policy/path. 

% We show that ILP(RL) is able to solve a reduced MDP where the rewards are assumed to be associated with a sequence of actions planned as answer sets.
% Although this is a limited solution, there is a potential to expand it to solve full MDP as discussed in Further Research. 

\subsection{Limitations of the current framework}
\label{subsec:limitations}
Although the this first version of the ILP(RL) framework using ILASP and ASP show potentials, there are a number of limitations with the current framework.
Some of these limitations are further elaborated in Further Research in the concluding chapter.
% It was implement from scratch.

\begin{description}
\item[Learning time]
While we show that ILP(RL) improves the learning convergence in terms of the number of episodes required, the computational time is significantly longer than that of Q-learning due to the computation required for inductive learning with ILASP as well as ASP planning.
This limitation indicates that ILP(RL) may not be suitable in an environment where there is a moving object based on time rather than time steps.

\item[Scalability issue for more complex environment]
ILP framework is known to be less scalable. The current framework is tested in a relatively simple environments, 
and proven to be work better than RL algorithms in terms of the number of episodes that is needed to converge to an optimal policy.
However, learning in each episode is relatively slower than that of RL. 

This limitation is theoretically discussed in XXX, where the complexity of deciding satisfiability is XX. (TODO Need to understand NP complete)
As shown in Experiment 1 and Experiment 2, adding two language bias significantly increased the runtime of the algorithm, since the search space grows significantly with respect to the language bias.

$\sum_{2}^{P}$-complete. Since there is no negative examples used in our current framework, the complexity is NP-complete.

Whereas Q-learning update value function regardless of whether there is a new concept such as teleport links.

Another question remains to how to extend the current framework to more realistic scenarios. RL works in more complex environments such as 3D or real physical environment, 
whereas the experiences of the agent in the current framework need to be expressed as ASP syntax, thus expressing continuous states rather than discrete states is challenging.

\item[Requirements of assumptions]

Language bias
While most of existing reinforcement learning works in different kinds of environment without pre-configuration, our algorithm
needs to define search space for learning hypothesis. As explained in the experiment 3, it was necessary to add two extra modeb before training.
Thus the algorithm may not be feasible in cases where these learning concepts were unknown or difficult to define with language bias. 

In addition, not only it needs search space, surrounding information is assumed to be known to the agent. 
While this assumption may be reasonable in many cases, this is not common in traditional reinforcement learning setting.

\item[limited MDP]
The current framework does not make use of rewards the agent collects and mainly uses the location of the goal for planning.
In other environments, however, there may not be a termination state (goal) and instead there may be a different purpose to gain these rewards. 
Since the current implementation is dependent on finding the goal for planning rather than maximising total rewards, 
which is the common objective for most of RL algorithms,
the application of the current framework is limited to particular types of problems.

There are some promising solutions for some of these limitations, which we discuss in the next chapter.

\end{description}  

