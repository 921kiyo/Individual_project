\section{Summary of Work}
\label{sec:summary_of_work}

In this paper, we developed a new RL algorithm by applying ILP to develop a new learning process.
We summarise some of the works we have done throughout the project.

\begin{itemize}
\item We started off by looking at existing symbolic reinforcement learning approaches to understand the research fields and see the potentials of improving further researchs.
Due to advances of ASP-based ILP frameworks and there is no existing works of applying ASP-based ILP into RL senarios, this motivates us to pursuing the potentials of ILP to solve RL problems.
Because of the flexibility and experise of ILASP, we decided to apply ILASP as our core learning framework.

\item We considerered the target hypotheses and how to construct learning tasks. Our target learning is a valid move of the agent, and whic can be expressed with state transition for each action.
Also all the components of learning tasks have to be in ASP-syntax, we developed a Python engine for translating all output of the environment into ASP-syntax.

surrounding information

\item As a way to use our learnt hypotheses to execute a sequence of actions, we used the Clingo ASP solver for our plan execution. 
The use of ASP optimisation is based on the assumption that the goal of the game is to find a shortest path to a terminal state.

\item We considered which kind of RL problems we would like to test with our new framework. Since this is a new proof-of-concept and testing core concepts were required, we chose a custom-maze game provided by VDGL and created original environments.

\item We tested our new framework in a various maze environments to highlight each aspect of the algorithm. We show that ILP(RL) learns faster than benchmarks for finding a shortest path, and show a capability of transfer learning.
While the experiments were conducted in a simple limited conditions, we show some promissing potentials of the ILP-based approach for RL problems.

\end{itemize}

We used a latest ILP algorithm called ILASP, Learning from Answer Set Program to iteratively improve hypotheses.

\section{Further Research}
\label{sec:further_research}

\begin{description}
    \item[Learning time]
\end{description}    

Having stated the limitations of the current framework, we discuss some of the possible improments and further research in this section.
Since this new approach is a proof of concept, there are a number of possible directions.

More complicated environment

More general transfer learning.

Only empirically correct, no theoreticaly guarantee

Dynamic environment like moving enermy etc.

Non-stationality possible to be handled??

Our approach is similar to experience replay ??

More promissing approach is to combine RL algorithm and using ILP approach to complement each other, rather than replacing the bellman equation altogether. 

\subsection{Value Iteration Approach}

The proposed architecture is not finalised and will be reviewed regularly as we do more research.
More research needs to be devoted to finalising the overall architecture, and the following issues in particular need to be considered.

\subsection{Weak Constraint}

This approach, however, might also suffer from the computing time, as discussed in \ref{sec:scalability}

\begin{itemize}

\item Further investigation of whether ILASP can learn the concept of adjacent, which is crucial concept to know in any environment.
\item How to generalise the agent's model when the environment changes. The new environment could be very similar to the previous one, or could be a completely different environment thus the agent should create a new internal model rather than generalising the existing model.
\item The current proposed architecture is based on Dyna with simulated experiences. However, this might not be the best overall architecture, and the feasibility of using simulated experience with the learnt model with ILASP needs to be further investigated.

\item Possibility of using other representational concepts such as \textit{Predictive Representations of State} or \textit{Affordance} \cite{Sridharan2017} for the agent's learning task. These concept have not been considered at the moment, but could help better transfer learning.

\item Preparation for a backup plan in case ILASP approach does not work, so that the researchs feasible within 3 months of the researcheriod.

\end{itemize}

\subsection{Generalisation of the Current Approach}

Learning the concept of being adjacent