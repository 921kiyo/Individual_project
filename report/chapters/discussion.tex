\section{Contribution}
\label{contribution}

To my knowledge, this is the first attemp that inductive logic programming is incorporated into a reinforcement learning senario to facilitate learning process.
In simple environments, we show that the agent learns rule of the game faster than existing RL algorithms, learnt concepts is easy to understand for human users.
We also show that the learnt hypothesis is general concept and can be applied to other environment to mitigate learnig process.

\section{Limitations}

Although this is the first time and inductive logic programming is applied into reinforcement leaning, 
there are two major limitations with the current framework.

\subsection{Scalability}
The first limitation is scalability. As pointed in XXX or XXX,
ILP framework is known to be less scalable. The current framework is tested in a relatively simple environments, 
and proven to be work better than RL algorithsm in terms of the number of episodes that is needed to converge to an optimal policy.
However, learning in each episode is relatively slower than that of RL. 
This is shown in XXX, which shows average learnint time for ILASP. 

This limitation is theoretically discussed in XXX, where the complexity of deciding satisfiability is 
$\sum_{2}^{P}$-complete. Since there is no negative examples used in our current framework, the complexity is NP-complete.

Whereas Q-learning update value function in the same way whether there is a new concept such as teleport links.

Figure XXX shows traning times for Experiment 1 and 2.

ILASP learning time for Experiment 1 and 2. 

Unlike existing reinforcement learning,
out algorithm refines hypothesis at every time steps within the same episode.
Thus even though the efficiency in terms of the number of iteration is higher,
training time within each iteration tends to be lower.

\subsection{Flexibility}
While most of existing reinforcement learning works in different kinds of environment without pre-configuration, our algorithm
needs to define search space for learning hypothesis. As explained in the experiment 3, it was necessary to add two extra modeb before training.
Thus the algorithm may not be feasible in cases where these learning concepts were unknown or difficult to define. 
In addition, not only it needs search space, surrounding information is assumed to be known to the agent. 
While this assumption may be reasonable in many cases, this is not common in traditional reinforcement learnig setting.

The current framework does not make use of rewards the agent collects and mainly uses the location of the goal for planning.
In some senarios, there may not be a termination state (goal) and instead there may be a different purpose to gain these rewards. 
Since the current implementation is dependent on finding the goal for planning rather than maximing total rewards, which is the common objective for most of RL algorithms,
the application of the current framework may be limited to particular types of problems.

Another question remains to how to extend the framework to more realistic senarios. RL works in more complex environments such as 3D or real physical environment, 
whereas the experiences of the agent in the current framework need to be expressed as ASP syntax, thus expressing continuous states rather than discrete states is challenging.

\section{Further Research}
\label{further_research}

Having stated the limitations of the current framework, we discuss some of the possible improments and further research in this section.

This is a proof of concept, a new type of model-based reinforcement learning using inductive logic programming. 

More complicated environment

Dynamic environment like moving enermy etc.

Non-stationality possible to be handled??

Our approach is similar to experience replay ??

More promissing approach is to combine RL algorithm and using ILP approach to complement each other, rather than replacing the bellman equation altogether. 

\subsection{Value iteration approach}

The proposed architecture is not finalised and will be reviewed regularly as we do more research.
More research needs to be devoted to finalising the overall architecture, and the following issues in particular need to be considered.

\subsection{Weak Constraint}

\begin{itemize}

\item Further investigation of whether ILASP can learn the concept of adjacent, which is crucial concept to know in any environment.
\item How to generalise the agent's model when the environment changes. The new environment could be very similar to the previous one, or could be a completely different environment thus the agent should create a new internal model rather than generalising the existing model.
\item The current proposed architecture is based on Dyna with simulated experiences. However, this might not be the best overall architecture, and the feasibility of using simulated experience with the learnt model with ILASP needs to be further investigated.

\item Possibility of using other representational concepts such as \textit{Predictive Representations of State} or \textit{Affordance} \cite{Sridharan2017} for the agent's learning task. These concept have not been considered at the moment, but could help better transfer learning.

\item Preparation for a backup plan in case ILASP approach does not work, so that the researchs feasible within 3 months of the researcheriod.

\end{itemize}

\subsection{probabilistic inductive logic programming}
instead of ASP

\subsection{generalisation of the current approach}

Learning the concept of being adjacent