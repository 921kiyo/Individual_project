@article{Law2014,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2014.Existing work on Inductive Logic Programming (ILP) has focused mainly on the learning of definite programs or normal logic programs. In this paper, we aim to push the computational boundary to a wider class of programs: Answer Set Programs. We propose a new paradigm for ILP that integrates existing notions of brave and cautious semantics within a unifying learning framework whose inductive solutions are Answer Set Programs and examples are partial interpretations We present an algorithm that is sound and complete with respect to our new notion of inductive solutions. We demonstrate its applicability by discussing a prototype implementation, called ILASP (Inductive Learning of Answer Set Programs), and evaluate its use in the context of planning. In particular, we show how ILASP can be used to learn agent's knowledge about the environment. Solutions of the learned ASP program provide plans for the agent to travel through the given environment.},
archivePrefix = {arXiv},
arxivId = {1608.01946},
author = {Law, Mark and Russo, Alessandra and Broda, Krysia},
eprint = {1608.01946},
file = {:home/kiyo/Dropbox/Individual Project/ILP/ILASP{\_}Paper.pdf:pdf},
issn = {16113349},
journal = {European Conference on Logics in Artificial Intelligence (JELIA)},
keywords = {inductive reasoning,learning answer set programs,monotonic inductive logic programming,non-},
number = {Ray 2009},
pages = {311--325},
title = {{Inductive Learning of Answer Set Programs}},
volume = {2},
year = {2014}
}
@article{Law2016,
abstract = {In recent years, several frameworks and systems have been proposed that extend Inductive Logic Programming (ILP) to the Answer Set Programming (ASP) paradigm. In ILP, examples must all be explained by a hypothesis together with a given background knowledge. In existing systems, the background knowledge is the same for all examples; however, examples may be context-dependent. This means that some examples should be explained in the context of some information, whereas others should be explained in different contexts. In this paper, we capture this notion and present a context-dependent extension of the Learning from Ordered Answer Sets framework. In this extension, contexts can be used to further structure the background knowledge. We then propose a new iterative algorithm, ILASP2i, which exploits this feature to scale up the existing ILASP2 system to learning tasks with large numbers of examples. We demonstrate the gain in scalability by applying both algorithms to various learning tasks. Our results show that, compared to ILASP2, the newly proposed ILASP2i system can be two orders of magnitude faster and use two orders of magnitude less memory, whilst preserving the same average accuracy. This paper is under consideration for acceptance in TPLP.},
archivePrefix = {arXiv},
arxivId = {1608.01946},
author = {Law, Mark and Russo, Alessandra and Broda, Krysia},
eprint = {1608.01946},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Law, Russo, Broda - 2016 - Iterative Learning of Answer Set Programs from Context Dependent Examples(2).pdf:pdf},
month = {aug},
title = {{Iterative Learning of Answer Set Programs from Context Dependent Examples}},
url = {http://arxiv.org/abs/1608.01946},
year = {2016}
}
@article{Law2015,
abstract = {This paper contributes to the area of inductive logic programming by presenting a new learning framework that allows the learning of weak constraints in Answer Set Programming (ASP). The framework, called Learning from Ordered Answer Sets, generalises our previous work on learning ASP programs without weak constraints, by considering a new notion of examples as ordered pairs of partial answer sets that exemplify which answer sets of a learned hypothesis (together with a given background knowledge) are preferred to others. In this new learning task inductive solutions are searched within a hypothesis space of normal rules, choice rules, and hard and weak constraints. We propose a new algorithm, ILASP2, which is sound and complete with respect to our new learning framework. We investigate its applicability to learning preferences in an interview scheduling problem and also demonstrate that when restricted to the task of learning ASP programs without weak constraints, ILASP2 can be much more efficient than our previously proposed system.},
archivePrefix = {arXiv},
arxivId = {1507.06566},
author = {Law, Mark and Russo, Alessandra and Broda, Krysia},
doi = {10.1017/S1471068415000198},
eprint = {1507.06566},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Law, Russo, Broda - 2015 - Learning Weak Constraints in Answer Set Programming.pdf:pdf},
month = {jul},
title = {{Learning Weak Constraints in Answer Set Programming}},
url = {http://arxiv.org/abs/1507.06566 http://dx.doi.org/10.1017/S1471068415000198},
year = {2015}
}
@article{Garcez2018,
archivePrefix = {arXiv},
arxivId = {1804.08597},
author = {d'Avila Garcez, Artur and Dutra, Aimore Resende Riquetti and Alonso, Eduardo},
eprint = {1804.08597},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcez, Dutra, Alonso - 2018 - Towards Symbolic Reinforcement Learning with Common Sense.pdf:pdf},
month = {apr},
title = {{Towards Symbolic Reinforcement Learning with Common Sense}},
url = {http://arxiv.org/abs/1804.08597},
year = {2018}
}
@article{Garnelo2016,
abstract = {Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.},
archivePrefix = {arXiv},
arxivId = {1609.05518},
author = {Garnelo, Marta and Arulkumaran, Kai and Shanahan, Murray},
eprint = {1609.05518},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garnelo, Arulkumaran, Shanahan - 2016 - Towards Deep Symbolic Reinforcement Learning.pdf:pdf},
month = {sep},
title = {{Towards Deep Symbolic Reinforcement Learning}},
url = {http://arxiv.org/abs/1609.05518},
year = {2016}
}

@article{Gelfond1988,
abstract = {We propose a new declarative semantics for logic programs with negation. Its formulation is quite simple; at the same time, it is more general than the iterated fixed point semantics for stratied programs, and is applicable to some useful programs that are not stratified.},
author = {Gelfond, Michael and Lifschitz, Vladimir},
doi = {10.1.1.24.6050},
file = {:homes/kk3317/Downloads/The{\_}Stable{\_}Model{\_}Semantics{\_}For{\_}Logic{\_}Programming.pdf:pdf},
isbn = {0262610566},
issn = {00224812},
journal = {5th International Conf. of Symp. on Logic Programming},
number = {December 2014},
pages = {1070--1080},
title = {{The stable model semantics for logic programming}},
year = {1988}
}


@article{Ebner2013,
abstract = {This chapter is a direct follow-up to the chapter on General Video Game Playing (GVGP). As that group recognised the need to create a Video Game Description Language (VGDL), we formed a group to address that challenge and the results of that group is the current chapter. Unlike the VGDL envisioned in the previous chapter, the language envisioned here is not meant to be supplied to the game-playing agent for automatic reasoning; instead we argue that the agent should learn this from interaction with the system. The main purpose of the language proposed here is to be able to specify complete video games, so that they could be compiled with a special VGDL compiler. Implementing such a compiler could provide numerous opportunities; users could modify existing games very quickly, or have a library of existing implementations defined within the language (e.g. an Asteroids ship or a Mario avatar) that have pre-existing, parameterised behaviours that can be customised for the users specific purposes. Provided the language is fit for purpose, automatic game creation could be explored further through experimentation with machine learning algorithms, furthering research in game creation and design. In order for both of these perceived functions to be realised and to ensure it is suitable for a large user base we recognise that the language carries several key requirements. Not only must it be human-readable, but retain the capability to be both expressive and extensible whilst equally simple as it is general. In our preliminary discussions, we sought to define the key requirements and challenges in constructing a new VGDL that will become part of the GVGP process. From this we have proposed an initial design to the semantics of the language and the components required to define a given game. Furthermore, we applied this approach to represent classic games such as Space Invaders, Lunar Lander and Frogger in an attempt to identify potential problems that may come to light. Work is ongoing to realise the potential of the VGDL for the purposes of Procedural Content Generation, Automatic Game Design and Transfer Learning.},
author = {Ebner, Marc and Levine, John and Lucas, SM},
doi = {10.4230/DFU.Vol6.12191.85},
file = {:home/kiyo/Dropbox/Individual Project/GVGAI/Towards a Video Game Description Language.pdf:pdf},
isbn = {9783939897620},
journal = {Dagstuhl Follow- {\ldots}},
pages = {85--100},
title = {{Towards a video game description language}},
url = {http://drops.dagstuhl.de/opus/volltexte/2013/4338/},
volume = {6},
year = {2013}
}
@article{Bianchi2017,
abstract = {Reinforcement Learning (RL) is a well-known technique for learning the solutions of control problems from the interactions of an agent in its domain. However, RL is known to be inefficient in problems of the real- world where the state space and the set of actions grow up fast. Recently, heuristics, case-based reasoning (CBR) and transfer learning have been used as tools to accel- erate the RL process. This paper investigates a class of algorithms called Transfer Learning Heuristically Acceler- ated Reinforcement Learning (TLHARL) that uses CBR as heuristics within a transfer learning setting to accelerate RL. The main contributions of this work are the proposal of a new TLHARL algorithm based on the traditional RL algo- rithm Q($\lambda$) and the application of TLHARL on two distinct real-robot domains: a robot soccer with small-scale robots and the humanoid-robot stability learning. Experimental results show that our proposed method led to a significant improvement of the learning rate in both domains.},
author = {Bianchi, Reinaldo A.C. and Santos, Paulo E. and da Silva, Isaac J. and Celiberto, Luiz A. and {Lopez de Mantaras}, Ramon},
doi = {10.1007/s10846-017-0731-2},
file = {:home/kiyo/Dropbox/Individual Project/meta{\_}learning/Heuristically Accelerated Reinforcement Learning.pdf:pdf},
issn = {15730409},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Case-based reasoning,Reinforcement learning,Robotics,Transfer learning},
pages = {1--12},
publisher = {Journal of Intelligent {\&} Robotic Systems},
title = {{Heuristically Accelerated Reinforcement Learning by Means of Case-Based Reasoning and Transfer Learning}},
year = {2017}
}
@article{Savarese2016,
abstract = {We propose a new layer design by adding a linear gating mechanism to shortcut connections. By using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter. We build upon the motivation behind Residual Networks, where a layer is reformulated in order to make learning identity mappings less problematic to the optimizer. The augmentation introduces only one extra parameter per layer, and provides easier optimization by making degeneration into identity mappings simpler. We propose a new model, the Gated Residual Network, which is the result when augmenting Residual Networks. Experimental results show that augmenting layers provides better optimization, increased performance, and more layer independence. We evaluate our method on MNIST using fully-connected networks, showing empirical indications that our augmentation facilitates the optimization of deep models, and that it provides high tolerance to full layer removal: the model retains over 90{\%} of its performance even after half of its layers have been randomly removed. We also evaluate our model on CIFAR-10 and CIFAR-100 using Wide Gated ResNets, achieving 3.65{\%} and 18.27{\%} error, respectively.},
archivePrefix = {arXiv},
arxivId = {1611.01260},
author = {Savarese, Pedro H. P. and Mazza, Leonardo O. and Figueiredo, Daniel R.},
eprint = {1611.01260},
file = {:home/kiyo/Dropbox/Individual Project/meta{\_}learning/LEARNING INVARIANCES FOR POLICY.pdf:pdf},
isbn = {0897914597},
journal = {Analysis},
number = {2},
pages = {245--278},
title = {{Learning Identity Mappings with Residual Gates}},
url = {http://arxiv.org/abs/1611.01260},
volume = {29},
year = {2016}
}
@article{Taylor2009,
abstract = {The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.},
archivePrefix = {arXiv},
arxivId = {0803.0476},
author = {Taylor, Matthew E and Stone, Peter},
doi = {10.1007/978-3-642-27645-3},
eprint = {0803.0476},
file = {:home/kiyo/Dropbox/Individual Project/meta{\_}learning/Transfer Learning for Reinforcement Learning Domains$\backslash$: A Survey.pdf:pdf},
isbn = {15324435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {1,1998,actions with goal,example,leaning agents take sequential,maximizing a reward,multi task learning,problems,reinforcement learning,rl,signal,sutton barto,transfer learning,transfer learning objectives,which may time delayed},
pages = {1633--1685},
pmid = {260529900010},
title = {{Transfer Learning for Reinforcement Learning Domains : A Survey}},
url = {http://portal.acm.org/citation.cfm?id=1755839},
volume = {10},
year = {2009}
}
@article{Perez-Liebana2017,
abstract = {—This paper presents a study on the robustness and variability of performance of general video game-playing agents. Agents analyzed includes those that won the different legs of the 2014 and 2015 General Video Game AI Competitions, and two sample agents distributed with its framework. Initially, these agents are run in four games and ranked according to the rules of the competition. Then, different modifications to the reward signal of the games are proposed and noise is introduced in either the actions executed by the controller, their forward model, or both. Results show that it is possible to produce a significant change in the rankings by introducing the modifications proposed here. This is an important result because it enables the set of human-authored games to be automatically expanded by adding parameter-varied versions that add information and insight into the relative strengths of the agents under test. Results also show that some controllers perform well under almost all conditions, a testament to the robustness of the GVGAI benchmark.},
author = {Perez-Liebana, Diego and Samothrakis, Spyridon and Togelius, Julian and Schaul, Tom and Lucas, Simon M.},
doi = {10.1109/CIG.2016.7860430},
file = {:home/kiyo/Dropbox/Individual Project/GVGAI/Analyzing the Robustness of FVGPA.pdf:pdf},
isbn = {9781509018833},
issn = {23254289},
journal = {IEEE Conference on Computatonal Intelligence and Games, CIG},
title = {{Analyzing the robustness of general video game playing agents}},
year = {2017}
}
@article{Neufeld2015,
abstract = {—This paper proposes an automatic way of evolving level generators for arbitrary 2D games, which are described in the Video Game Description Language (VGDL). The process works as follows: a game described in VGDL is interpreted and transformed in a set of rules defined in Answer Set Programming (ASP), along with other general and customizable rules. Although a set of rules described in ASP can generate multiple levels, not all of them will be playable or well designed. Therefore, an evolutionary process is run to determine the values of the parameters of those customizable rules. The different level generators are evaluated with general video game playing agents, which are able to play any game and level in the framework. The aim is to maximize the difference between their performance in the levels generated, under the assumption that levels are better designed if good skilled players play better than poor agents. This work presents some initial experiments that suggest that it is possible to evolve interesting level generators using this technique, and outlines some lines of future work.},
author = {Neufeld, Xenija and Mostaghim, Sanaz and Perez-Liebana, Diego},
doi = {10.1109/CEEC.2015.7332726},
file = {:home/kiyo/Dropbox/Individual Project/GVGAI/Procedural Level Generation with Answer Set.pdf:pdf},
isbn = {9781467394819},
journal = {2015 7th Computer Science and Electronic Engineering Conference, CEEC 2015 - Conference Proceedings},
pages = {207--212},
title = {{Procedural level generation with answer set programming for general Video Game playing}},
year = {2015}
}
@article{Wang2018,
abstract = {Despite the recent successes of deep neural networks in various fields such as image and speech recognition, natural language processing, and reinforcement learning, we still face big challenges in bringing the power of numeric optimization to symbolic reasoning. Researchers have proposed different avenues such as neural machine translation for proof synthesis, vectorization of symbols and expressions for representing symbolic patterns, and coupling of neural back-ends for dimensionality reduction with symbolic front-ends for decision making. However, these initial explorations are still only point solutions, and bear other shortcomings such as lack of correctness guarantees. In this paper, we present our approach of casting symbolic reasoning as games, and directly harnessing the power of deep reinforcement learning in the style of Alpha(Go) Zero on symbolic problems. Using the Boolean Satisfiability (SAT) problem as showcase, we demonstrate the feasibility of our method, and the advantages of modularity, efficiency, and correctness guarantees.},
archivePrefix = {arXiv},
arxivId = {1802.05340},
author = {Wang, Fei and Rompf, Tiark},
eprint = {1802.05340},
file = {:home/kiyo/Dropbox/Individual Project/symbolic{\_}approach/FROM GAMEPLAY TO SYMBOLIC REASONING- LEARNING SAT SOLVER HEURISTICS IN THE STYLE OF ALPHA(GO) ZERO.pdf:pdf},
pages = {1--4},
title = {{From Gameplay to Symbolic Reasoning: Learning SAT Solver Heuristics in the Style of Alpha(Go) Zero}},
url = {http://arxiv.org/abs/1802.05340},
year = {2018}
}
@article{Apeldoorn2017,
author = {Apeldoorn, Daan and Gabriele, Kern-Isberner},
file = {:home/kiyo/Dropbox/Individual Project/PaperOnReinforcementLearning.pdf:pdf},
journal = {Proceedings of the Thirteenth International Symposium on Commonsense Reasoning},
pages = {1--8},
title = {{An Agent-Based Learning Approach for Finding and Exploiting Heuristics in Unknown Environments}},
year = {2017}
}
@article{Russo,
author = {Russo, Alessandra and Law, Mark and Broda, Krysia},
file = {:home/kiyo/Dropbox/Individual Project/ILP/IL of Human Behaviours.pdf:pdf},
pages = {1--3},
title = {{Inductive Learning of Human Behaviours}}
}
@article{Dragiev2017,
abstract = {The integration of abduction and induction has lead to a variety of non-monotonic ILP systems. XHAIL is one of these systems, in which abduction is used to compute hypotheses that subsume Kernel Sets. On the other hand, Peircebayes is a recently proposed logic-based probabilistic programming approach that combines abduction with parameter learning to learn distributions of most likely explanations. In this paper, we propose an approach for integrating probabilistic inference with ILP. The basic idea is to redefine the inductive task of XHAIL as a statistical abduction, and to use Peircebayes to learn probability distribution of hypotheses. An initial evaluation of the proposed algorithm is given using synthetic data.},
author = {Dragiev, Stanislav and Russo, Alessandra and Broda, Krysia and Law, Mark and Turliuc, Rares},
file = {:home/kiyo/Dropbox/Individual Project/ILP/An Abductive-Inductive Algorithm for Probabilistic Inductive Logic Programming.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
pages = {20--26},
title = {{An abductive-inductive algorithm for probabilistic inductive logic programming}},
volume = {1865},
year = {2017}
}
@article{Muggleton1999,
author = {Muggleton, S H},
file = {:home/kiyo/Dropbox/Individual Project/ILP/ILP$\backslash$: Challenges:},
journal = {The MIT Encyclopedia of the Cognitive Sciences (MITECS)},
keywords = {What's Hot Abstracts},
pages = {4330--4332},
title = {{Inductive {\{}L{\}}ogic {\{}P{\}}rogramming}},
year = {1999}
}
@article{Arulkumaran2017,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
file = {:home/kiyo/Dropbox/Individual Project/DRL/a{\_}brief{\_}survey{\_}of{\_}DRL.pdf:pdf},
isbn = {9781424469178},
issn = {1701.07274},
pages = {1--16},
pmid = {25719670},
title = {{A Brief Survey of Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.05866{\%}0Ahttp://dx.doi.org/10.1109/MSP.2017.2743240},
year = {2017}
}
@article{Yang2017,
abstract = {Modern machine learning methods are increasingly powerful and opaque. This opaqueness is a concern across a variety of domains in which algorithms are making important decisions that should be scrutable. The explainabilty of machine learning systems is therefore of increasing interest. We propose an explanation-by-examples approach that builds on our recent research in Bayesian teaching in which we aim to select a small subset of the data that would lead the learner to similar conclusions as the entire dataset. We discuss this approach, explicating several key advantages. First, the ability to cover any model with a probabilistic interpretation including supervised, unsupervised, and reinforcement learning (including deep learning). Second, we discuss the empirical foundations of this approach in the cognitive science of learning from other agents. Third, we outline challenges to full realization of the promise of this approach. We conclude by discussing implications for machine learning and applications to real-world problems.},
author = {Yang, Scott Chen-Hsin and Shafto, Patrick},
file = {:home/kiyo/Dropbox/Individual Project/meta{\_}learning/Explainable Artificial Intelligence via Bayesian.pdf:pdf},
journal = {Conference on Neural Information Processing Systems},
number = {Nips},
title = {{Explainable Artificial Intelligence via Bayesian Teaching}},
year = {2017}
}
@article{Felix2016,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2016. Machine learning processes consist in collecting data, obtaining a model and applying it to a given task. Given a new task, the standard approach is to restart the learning process and obtain a new model. However, previous learning experience can be exploited to assist the new learning process. The two most studied approaches for this are metalearning and transfer learning. Metalearning can be used for selecting the predictive model to use on a new dataset. Transfer learning allows the reuse of knowledge from previous tasks. However, when multiple heterogeneous tasks are available as potential sources for transfer, the question is which one to use. One approach to address this problem is metalearning. In this paper we investigate the feasibility of this approach. We propose a method to transfer weights from a source trained neural network to initialize a network that models a potentially very different target dataset. Our experiments with 14 datasets indicate that this method enables faster convergence without significant difference in accuracy provided that the source task is adequately chosen. This means that there is potential for applying metalearning to support transfer between heterogeneous datasets.},
author = {F{\'{e}}lix, Catarina and Soares, Carlos and Jorge, Al{\'{i}}pio},
doi = {10.1007/978-3-319-32034-2_28},
file = {:home/kiyo/Dropbox/Individual Project/meta{\_}learning/Can metalearning be applied to transfer on.pdf:pdf},
isbn = {9783319320335},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {332--343},
title = {{Can metalearning be applied to transfer on heterogeneous datasets?}},
volume = {9648},
year = {2016}
}
@article{Desjardins-Proulx2017,
abstract = {Artificial Intelligence presents an important paradigm shift for science. Science is traditionally founded on theories and models, most often formalized with mathematical formulas handcrafted by theoretical scientists and refined through experiments. Machine learning, an important branch of modern Artificial Intelligence, focuses on learning from data. This leads to a fundamentally different approach to model-building: we step back and focus on the design of algorithms capable of building models from data, but the models themselves are not designed by humans. This is even more true with deep learning, which requires little engineering by hand and is responsible for many of Artificial Intelligence's spectacular successes. In contrast to logic systems, knowledge from a deep learning model is difficult to understand, reuse, and may involve up to a billion parameters. On the other hand, probabilistic machine learning techniques such as deep learning offer an opportunity to tackle large complex problems that are out of the reach of traditional theory-making. It is possible that the more intuition-like reasoning performed by deep learning systems is mostly incompatible with the logic formalism of mathematics. Yet recent studies have shown that deep learning can be useful to logic systems and vice versa. Success at unifying different paradigms of Artificial Intelligence from logic to probability theory offers unique opportunities to combine data-driven approaches with traditional theories. These advancements are susceptible to impact significantly biological sciences, where dimensionality is high and limit the investigation of traditional theories.},
author = {Desjardins-Proulx, Philippe and Poisot, Timoth{\'{e}}e and Gravel, Dominique},
doi = {10.1101/161125},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Desjardins-Proulx, Poisot, Gravel - 2017 - Scientific Theories and Artificial Intelligence.pdf:pdf},
journal = {bioRxiv},
month = {oct},
pages = {161125},
publisher = {Cold Spring Harbor Laboratory},
title = {{Scientific Theories and Artificial Intelligence}},
url = {https://www.biorxiv.org/content/early/2017/10/22/161125},
year = {2017}
}
@article{Lake2016,
abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
archivePrefix = {arXiv},
arxivId = {1604.00289},
author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
eprint = {1604.00289},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf:pdf},
month = {apr},
title = {{Building Machines That Learn and Think Like People}},
url = {http://arxiv.org/abs/1604.00289},
year = {2016}
}
@article{Penkov2017,
archivePrefix = {arXiv},
arxivId = {1705.08320},
author = {Penkov, Svetlin and Ramamoorthy, Subramanian},
eprint = {1705.08320},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Penkov, Ramamoorthy - 2017 - Explaining Transition Systems through Program Induction.pdf:pdf},
month = {may},
title = {{Explaining Transition Systems through Program Induction}},
url = {http://arxiv.org/abs/1705.08320},
year = {2017}
}
@article{Ferreira2017,
abstract = {Non-stationary domains, where unforeseen changes happen, present a challenge for agents to find an optimal policy for a sequential decision making problem. This work investigates a solution to this problem that combines Markov Decision Processes (MDP) and Reinforcement Learning (RL) with Answer Set Programming (ASP) in a method we call ASP(RL). In this method, Answer Set Programming is used to find the possible trajectories of an MDP, from where Reinforcement Learning is applied to learn the optimal policy of the problem. Results show that ASP(RL) is capable of efficiently finding the optimal solution of an MDP representing non-stationary domains.},
archivePrefix = {arXiv},
arxivId = {1705.01399},
author = {Ferreira, Leonardo A. and Bianchi, Reinaldo A. C. and Santos, Paulo E. and de Mantaras, Ramon Lopez},
eprint = {1705.01399},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferreira et al. - 2017 - Answer Set Programming for Non-Stationary Markov Decision Processes.pdf:pdf},
month = {may},
title = {{Answer Set Programming for Non-Stationary Markov Decision Processes}},
url = {http://arxiv.org/abs/1705.01399},
year = {2017}
}
@article{Ferreira2017a,
abstract = {Non-stationary domains, that change in unpredicted ways, are a challenge for agents searching for optimal policies in sequential decision-making problems. This paper presents a combination of Markov Decision Processes (MDP) with Answer Set Programming (ASP), named {\{}$\backslash$em Online ASP for MDP{\}} (oASP(MDP)), which is a method capable of constructing the set of domain states while the agent interacts with a changing environment. oASP(MDP) updates previously obtained policies, learnt by means of Reinforcement Learning (RL), using rules that represent the domain changes observed by the agent. These rules represent a set of domain constraints that are processed as ASP programs reducing the search space. Results show that oASP(MDP) is capable of finding solutions for problems in non-stationary domains without interfering with the action-value function approximation process.},
archivePrefix = {arXiv},
arxivId = {1706.01417},
author = {Ferreira, Leonardo A. and Bianchi, Reinaldo A. C. and Santos, Paulo E. and de Mantaras, Ramon Lopez},
eprint = {1706.01417},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferreira et al. - 2017 - A method for the online construction of the set of states of a Markov Decision Process using Answer Set Program.pdf:pdf},
month = {jun},
title = {{A method for the online construction of the set of states of a Markov Decision Process using Answer Set Programming}},
url = {http://arxiv.org/abs/1706.01417},
year = {2017}
}
@article{Cai2017,
abstract = {This paper introduces an SLD-resolution technique based on deep learning. This technique enables neural networks to learn from old and successful resolution processes and to use learnt experiences to guide new resolution processes. An implementation of this technique is named SLDR-DL. It includes a Prolog library of deep feedforward neural networks and some essential functions of resolution. In the SLDR-DL framework, users can define logical rules in the form of definite clauses and teach neural networks to use the rules in reasoning processes.},
archivePrefix = {arXiv},
arxivId = {1705.02210},
author = {Cai, Cheng-Hao},
eprint = {1705.02210},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai - 2017 - SLDR-DL A Framework for SLD-Resolution with Deep Learning.pdf:pdf},
month = {may},
title = {{SLDR-DL: A Framework for SLD-Resolution with Deep Learning}},
url = {http://arxiv.org/abs/1705.02210},
year = {2017}
}
@article{Penkov2017a,
abstract = {Explaining and reasoning about processes which underlie observed black-box phenomena enables the discovery of causal mechanisms, derivation of suitable abstract representations and the formulation of more robust predictions. We propose to learn high level functional programs in order to represent abstract models which capture the invariant structure in the observed data. We introduce the {\$}\backslashpi{\$}-machine (program-induction machine) -- an architecture able to induce interpretable LISP-like programs from observed data traces. We propose an optimisation procedure for program learning based on backpropagation, gradient descent and A* search. We apply the proposed method to two problems: system identification of dynamical systems and explaining the behaviour of a DQN agent. Our results show that the {\$}\backslashpi{\$}-machine can efficiently induce interpretable programs from individual data traces.},
archivePrefix = {arXiv},
arxivId = {1708.00376},
author = {Penkov, Svetlin and Ramamoorthy, Subramanian},
eprint = {1708.00376},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Penkov, Ramamoorthy - 2017 - Using Program Induction to Interpret Transition System Dynamics.pdf:pdf},
month = {jul},
title = {{Using Program Induction to Interpret Transition System Dynamics}},
url = {http://arxiv.org/abs/1708.00376},
year = {2017}
}
@article{Verma2018,
abstract = {We study the problem of generating interpretable and verifiable policies through reinforcement learning. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, in which the policy is represented by a neural network, the aim in Programmatically Interpretable Reinforcement Learning is to find a policy that can be represented in a high-level programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maxima reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural "oracle". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also find that a well-designed policy language can serve as a regularizer, and result in the discovery of policies that lead to smoother trajectories and are more easily transferred to environments not encountered during training.},
archivePrefix = {arXiv},
arxivId = {1804.02477},
author = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
eprint = {1804.02477},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Verma et al. - 2018 - Programmatically Interpretable Reinforcement Learning.pdf:pdf},
month = {apr},
title = {{Programmatically Interpretable Reinforcement Learning}},
url = {http://arxiv.org/abs/1804.02477},
year = {2018}
}
@article{Law2016a,
abstract = {In recent years, several frameworks and systems have been proposed that extend Inductive Logic Programming (ILP) to the Answer Set Programming (ASP) paradigm. In ILP, examples must all be explained by a hypothesis together with a given background knowledge. In existing systems, the background knowledge is the same for all examples; however, examples may be context-dependent. This means that some examples should be explained in the context of some information, whereas others should be explained in different contexts. In this paper, we capture this notion and present a context-dependent extension of the Learning from Ordered Answer Sets framework. In this extension, contexts can be used to further structure the background knowledge. We then propose a new iterative algorithm, ILASP2i, which exploits this feature to scale up the existing ILASP2 system to learning tasks with large numbers of examples. We demonstrate the gain in scalability by applying both algorithms to various learning tasks. Our results show that, compared to ILASP2, the newly proposed ILASP2i system can be two orders of magnitude faster and use two orders of magnitude less memory, whilst preserving the same average accuracy. This paper is under consideration for acceptance in TPLP.},
archivePrefix = {arXiv},
arxivId = {1608.01946},
author = {Law, Mark and Russo, Alessandra and Broda, Krysia},
eprint = {1608.01946},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Law, Russo, Broda - 2016 - Iterative Learning of Answer Set Programs from Context Dependent Examples.pdf:pdf},
month = {aug},
title = {{Iterative Learning of Answer Set Programs from Context Dependent Examples}},
url = {http://arxiv.org/abs/1608.01946},
year = {2016}
}
@article{Zhou2018,
abstract = {Few-shot learning remains challenging for meta-learning that learns a learning algorithm (meta-learner) from many related tasks. In this work, we argue that this is due to the lack of a good representation for meta-learning, and propose deep meta-learning to integrate the representation power of deep learning into meta-learning. The framework is composed of three modules, a concept generator, a meta-learner, and a concept discriminator, which are learned jointly. The concept generator, e.g. a deep residual net, extracts a representation for each instance that captures its high-level concept, on which the meta-learner performs few-shot learning, and the concept discriminator recognizes the concepts. By learning to learn in the concept space rather than in the complicated instance space, deep meta-learning can substantially improve vanilla meta-learning, which is demonstrated on various few-shot image recognition problems. For example, on 5-way-1-shot image recognition on CIFAR-100 and CUB-200, it improves Matching Nets from 50.53{\%} and 56.53{\%} to 58.18{\%} and 63.47{\%}, improves MAML from 49.28{\%} and 50.45{\%} to 56.65{\%} and 64.63{\%}, and improves Meta-SGD from 53.83{\%} and 53.34{\%} to 61.62{\%} and 66.95{\%}, respectively.},
archivePrefix = {arXiv},
arxivId = {1802.03596},
author = {Zhou, Fengwei and Wu, Bin and Li, Zhenguo},
eprint = {1802.03596},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Wu, Li - 2018 - Deep Meta-Learning Learning to Learn in the Concept Space.pdf:pdf},
month = {feb},
title = {{Deep Meta-Learning: Learning to Learn in the Concept Space}},
url = {http://arxiv.org/abs/1802.03596},
year = {2018}
}
@article{Franceschi2017,
abstract = {We consider a class of a nested optimization problems involving inner and outer objectives. We observe that by taking into explicit account the optimization dynamics for the inner objective it is possible to derive a general framework that unifies gradient-based hyperparameter optimization and meta-learning (or learning-to-learn). Depending on the specific setting, the variables of the outer objective take either the meaning of hyperparameters in a supervised learning problem or parameters of a meta-learner. We show that some recently proposed methods in the latter setting can be instantiated in our framework and tackled with the same gradient-based algorithms. Finally, we discuss possible design patterns for learning-to-learn and present encouraging preliminary experiments for few-shot learning.},
archivePrefix = {arXiv},
arxivId = {1712.06283},
author = {Franceschi, Luca and Donini, Michele and Frasconi, Paolo and Pontil, Massimiliano},
eprint = {1712.06283},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Franceschi et al. - 2017 - A Bridge Between Hyperparameter Optimization and Larning-to-learn.pdf:pdf},
month = {dec},
title = {{A Bridge Between Hyperparameter Optimization and Larning-to-learn}},
url = {http://arxiv.org/abs/1712.06283},
year = {2017}
}
@article{Sung2017,
abstract = {We propose a novel and flexible approach to meta-learning for learning-to-learn from only a few examples. Our framework is motivated by actor-critic reinforcement learning, but can be applied to both reinforcement and supervised learning. The key idea is to learn a meta-critic: an action-value function neural network that learns to criticise any actor trying to solve any specified task. For supervised learning, this corresponds to the novel idea of a trainable task-parametrised loss generator. This meta-critic approach provides a route to knowledge transfer that can flexibly deal with few-shot and semi-supervised conditions for both reinforcement and supervised learning. Promising results are shown on both reinforcement and supervised learning problems.},
archivePrefix = {arXiv},
arxivId = {1706.09529},
author = {Sung, Flood and Zhang, Li and Xiang, Tao and Hospedales, Timothy and Yang, Yongxin},
eprint = {1706.09529},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sung et al. - 2017 - Learning to Learn Meta-Critic Networks for Sample Efficient Learning.pdf:pdf},
month = {jun},
title = {{Learning to Learn: Meta-Critic Networks for Sample Efficient Learning}},
url = {http://arxiv.org/abs/1706.09529},
year = {2017}
}
@article{Duan2016,
abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL{\$}{\^{}}2{\$}, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL{\$}{\^{}}2{\$} experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL{\$}{\^{}}2{\$} is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL{\$}{\^{}}2{\$} on a vision-based navigation task and show that it scales up to high-dimensional problems.},
archivePrefix = {arXiv},
arxivId = {1611.02779},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
eprint = {1611.02779},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duan et al. - 2016 - RL{\$}2{\$} Fast Reinforcement Learning via Slow Reinforcement Learning.pdf:pdf},
month = {nov},
title = {{RL{\$}{\^{}}2{\$}: Fast Reinforcement Learning via Slow Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.02779},
year = {2016}
}
@article{Mishra2017,
abstract = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.},
archivePrefix = {arXiv},
arxivId = {1707.03141},
author = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
eprint = {1707.03141},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mishra et al. - 2017 - A Simple Neural Attentive Meta-Learner.pdf:pdf},
month = {jul},
title = {{A Simple Neural Attentive Meta-Learner}},
url = {http://arxiv.org/abs/1707.03141},
year = {2017}
}
@article{Lesort2018,
abstract = {Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. As the representation learned captures the variation in the environment generated by agents, this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning. This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.},
archivePrefix = {arXiv},
arxivId = {1802.04181},
author = {Lesort, Timoth{\'{e}}e and D{\'{i}}az-Rodr{\'{i}}guez, Natalia and Goudou, Jean-Fran{\c{c}}ois and Filliat, David},
eprint = {1802.04181},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lesort et al. - 2018 - State Representation Learning for Control An Overview.pdf:pdf},
month = {feb},
title = {{State Representation Learning for Control: An Overview}},
url = {http://arxiv.org/abs/1802.04181},
year = {2018}
}
@misc{Stone,
author = {Stone, Josiah Hanna and Peter},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stone - Unknown - Towards a Data Efficient Off-Policy Policy Gradient.pdf:pdf},
title = {{Towards a Data Efficient Off-Policy Policy Gradient}},
url = {http://www.cs.utexas.edu/users/ai-lab/?AAAISSS2018-Hanna{\#}.Ww6m{\_}aBQDCU.mendeley}
}
@article{Perez-Liebana2018,
abstract = {General Video Game Playing (GVGP) aims at designing an agent that is capable of playing multiple video games with no human intervention. In 2014, The General Video Game AI (GVGAI) competition framework was created and released with the purpose of providing researchers a common open-source and easy to use platform for testing their AI methods with potentially infinity of games created using Video Game Description Language (VGDL). The framework has been expanded into several tracks during the last few years to meet the demand of different research directions. The agents are required to either play multiples unknown games with or without access to game simulations, or to design new game levels or rules. This survey paper presents the VGDL, the GVGAI framework, existing tracks, and reviews the wide use of GVGAI framework in research, education and competitions five years after its birth. A future plan of framework improvements is also described.},
archivePrefix = {arXiv},
arxivId = {1802.10363},
author = {Perez-Liebana, Diego and Liu, Jialin and Khalifa, Ahmed and Gaina, Raluca D. and Togelius, Julian and Lucas, Simon M.},
eprint = {1802.10363},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perez-Liebana et al. - 2018 - General Video Game AI a Multi-Track Framework for Evaluating Agents, Games and Content Generation Algorith.pdf:pdf},
month = {feb},
title = {{General Video Game AI: a Multi-Track Framework for Evaluating Agents, Games and Content Generation Algorithms}},
url = {http://arxiv.org/abs/1802.10363},
year = {2018}
}
@article{Woof2018,
abstract = {Deep reinforcement learning (DRL) has proven to be an effective tool for creating general video-game AI. However most current DRL video-game agents learn end-to-end from the video-output of the game, which is superfluous for many applications and creates a number of additional problems. More importantly, directly working on pixel-based raw video data is substantially distinct from what a human player does.In this paper, we present a novel method which enables DRL agents to learn directly from object information. This is obtained via use of an object embedding network (OEN) that compresses a set of object feature vectors of different lengths into a single fixed-length unified feature vector representing the current game-state and fulfills the DRL simultaneously. We evaluate our OEN-based DRL agent by comparing to several state-of-the-art approaches on a selection of games from the GVG-AI Competition. Experimental results suggest that our object-based DRL agent yields performance comparable to that of those approaches used in our comparative study.},
archivePrefix = {arXiv},
arxivId = {1803.05262},
author = {Woof, William and Chen, Ke},
eprint = {1803.05262},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Woof, Chen - 2018 - Learning to Play General Video-Games via an Object Embedding Network.pdf:pdf},
month = {mar},
title = {{Learning to Play General Video-Games via an Object Embedding Network}},
url = {http://arxiv.org/abs/1803.05262},
year = {2018}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
eprint = {1610.00633},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1312.5602},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
year = {2015}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Mnih2015a,
abstract = {Policy advice is a transfer learning method where a student agent is able to learn faster via advice from a teacher. However, both this and other reinforcement learning transfer methods have little theoretical analysis. This paper formally defines a setting where multiple teacher agents can provide advice to a student and introduces an algorithm to leverage both autonomous exploration and teacher's advice. Our regret bounds justify the intuition that good teachers help while bad teachers hurt. Using our formalization, we are also able to quantify, for the first time, when negative transfer can occur within such a reinforcement learning setting.},
archivePrefix = {arXiv},
arxivId = {1604.03986},
author = {Mnih, Volodymyr and Silver, David},
doi = {10.1038/nature14236},
eprint = {1604.03986},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {10450823},
journal = {Nature},
number = {7540},
pages = {2315--2321},
pmid = {25719670},
title = {{Human level control through Deep Reinforcement Learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {2016-Janua},
year = {2015}
}
@article{,
abstract = {Policy advice is a transfer learning method where a student agent is able to learn faster via advice from a teacher. However, both this and other reinforcement learning transfer methods have little theoretical analysis. This paper formally defines a setting where multiple teacher agents can provide advice to a student and introduces an algorithm to leverage both autonomous exploration and teacher's advice. Our regret bounds justify the intuition that good teachers help while bad teachers hurt. Using our formalization, we are also able to quantify, for the first time, when negative transfer can occur within such a reinforcement learning setting.},
archivePrefix = {arXiv},
arxivId = {1604.03986},
doi = {10.1038/nature14236},
eprint = {1604.03986},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2016 - Human-level control through deep reinforcement learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
number = {7540},
pages = {2315--2321},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {2016-Janua},
year = {2016}
}
@article{Sakama2009,
abstract = {Abstract  This paper introduces a novel logical framework for concept-learning called brave induction. Brave induction uses brave inference for induction and is useful for learning from incomplete information. Brave induction$\backslash$n  is weaker than explanatory induction which is normally used in inductive logic programming, and is stronger than learning from satisfiability, a general setting of concept-learning in clausal logic. We first investigate formal properties of brave induction, then$\backslash$n  develop an algorithm for computing hypotheses in full clausal theories. Next we extend the framework to induction in nonmonotonic logic programs. We analyze computational complexity of decision problems for induction on propositional theories. Further, we provide examples$\backslash$n  of problem solving by brave induction in systems biology, requirement engineering, and multiagent negotiation.},
author = {Sakama, Chiaki and Inoue, Katsumi},
doi = {10.1007/s10994-009-5113-y},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sakama, Inoue - 2009 - Brave induction A logical framework for learning from incomplete information.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Brave induction,Inductive logic programming,Nonmonotonic logic programming},
number = {1},
pages = {3--35},
title = {{Brave induction: A logical framework for learning from incomplete information}},
volume = {76},
year = {2009}
}
@article{Muggleton1991,
abstract = {A new research area, Inductive Logic Programming, is presently emerging. While inheriting various positive characteristics of the parent subjects of Logic Programming and Machine Learning, it is hoped that the new area will overcome many of the limitations of its forebears. The background to present developments within this area is discussed and various goals and aspirations for the increasing body of researchers are identified. Inductive Logic Programming needs to be based on sound principles from both Logic and Statistics. On the side of statistical justification of hypotheses we discuss the possible relationship between Algorithmic Complexity theory and Probably-Approximately-Correct (PAC) Learning. In terms of logic we provide a unifying framework for Muggleton and Buntine's Inverse Resolution (IR) and Plotkin's Relative Least General Generalisation (RLGG) by rederiving RLGG in terms of IR. This leads to a discussion of the feasibility of extending the RLGG framework to allow for the invention of new predicates, previously discussed only within the context of IR. {\textcopyright} 1991 Ohmsha, Ltd. and Springer.},
author = {Muggleton, S},
doi = {10.1007/BF03037089},
isbn = {0288-3635},
issn = {02883635 (ISSN)},
journal = {New Generation Computing},
keywords = {Learning,induction,information compression,inverse resolution,logic programming,predicate invention},
number = {4},
pages = {295--318},
title = {{Inductive logic programming}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000640432{\&}doi=10.1007{\%}2FBF03037089{\&}partnerID=40{\&}md5=eca5117a0efba780a7b1b156fdc19036},
volume = {8},
year = {1991}
}
