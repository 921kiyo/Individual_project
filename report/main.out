\BOOKMARK [0][]{chapter.1}{1 Introduction}{}% 1
\BOOKMARK [0][]{chapter.2}{2 Background}{}% 2
\BOOKMARK [1][]{section.2.1}{2.1 Inductive Logic Programming \(ILP\)}{chapter.2}% 3
\BOOKMARK [2][]{subsection.2.1.1}{2.1.1 Stable Model Semantics}{section.2.1}% 4
\BOOKMARK [2][]{subsection.2.1.2}{2.1.2 Anwer Set Programming \(ASP\) Syntax}{section.2.1}% 5
\BOOKMARK [2][]{subsection.2.1.3}{2.1.3 ILP under Answer Set Semantics}{section.2.1}% 6
\BOOKMARK [2][]{subsection.2.1.4}{2.1.4 Inductive Learning of Answer Set Programs \(ILASP\)}{section.2.1}% 7
\BOOKMARK [1][]{section.2.2}{2.2 Reinforcement Learning \(RL\)}{chapter.2}% 8
\BOOKMARK [2][]{subsection.2.2.1}{2.2.1 Markov Decision Process \(MDP\)}{section.2.2}% 9
\BOOKMARK [2][]{subsection.2.2.2}{2.2.2 Policies and Value Functions}{section.2.2}% 10
\BOOKMARK [2][]{subsection.2.2.3}{2.2.3 Model-based and Model-free Reinforcement Learning}{section.2.2}% 11
\BOOKMARK [2][]{subsection.2.2.4}{2.2.4 Temporal-Difference \(TD\) Learning}{section.2.2}% 12
\BOOKMARK [2][]{subsection.2.2.5}{2.2.5 Function Approximation}{section.2.2}% 13
\BOOKMARK [0][]{chapter.3}{3 Implementation and Methodology}{}% 14
\BOOKMARK [1][]{section.3.1}{3.1 Experience Accumulation}{chapter.3}% 15
\BOOKMARK [2][]{subsection.3.1.1}{3.1.1 State transition experience}{section.3.1}% 16
\BOOKMARK [2][]{subsection.3.1.2}{3.1.2 Environment experience}{section.3.1}% 17
\BOOKMARK [1][]{section.3.2}{3.2 Inductive Learning}{chapter.3}% 18
\BOOKMARK [2][]{subsection.3.2.1}{3.2.1 Search Space}{section.3.2}% 19
\BOOKMARK [1][]{section.3.3}{3.3 Generate a plan}{chapter.3}% 20
\BOOKMARK [1][]{section.3.4}{3.4 Plan execution}{chapter.3}% 21
\BOOKMARK [1][]{section.3.5}{3.5 Exploration}{chapter.3}% 22
\BOOKMARK [0][]{chapter.4}{4 Evaluation}{}% 23
\BOOKMARK [1][]{section.4.1}{4.1 Settings \(To Be Updated\)}{chapter.4}% 24
\BOOKMARK [1][]{section.4.2}{4.2 Benchmark \(To Be Updated\)}{chapter.4}% 25
\BOOKMARK [1][]{section.4.3}{4.3 Experiment Results \(To Be Updated\)}{chapter.4}% 26
\BOOKMARK [2][]{subsection.4.3.1}{4.3.1 Experiment1}{section.4.3}% 27
\BOOKMARK [2][]{subsection.4.3.2}{4.3.2 Experiment2}{section.4.3}% 28
\BOOKMARK [2][]{subsection.4.3.3}{4.3.3 Experiment3}{section.4.3}% 29
\BOOKMARK [1][]{section.4.4}{4.4 Transfer Learning Evaluation}{chapter.4}% 30
\BOOKMARK [2][]{subsection.4.4.1}{4.4.1 Experiment4}{section.4.4}% 31
\BOOKMARK [0][]{chapter.5}{5 Related Work}{}% 32
\BOOKMARK [0][]{chapter.6}{6 Conclusion and Further Research}{}% 33
\BOOKMARK [1][]{section.6.1}{6.1 Contribution}{chapter.6}% 34
\BOOKMARK [1][]{section.6.2}{6.2 Further Research}{chapter.6}% 35
\BOOKMARK [2][]{subsection.6.2.1}{6.2.1 Value iteration approach}{section.6.2}% 36
\BOOKMARK [2][]{subsection.6.2.2}{6.2.2 Weak Constraint}{section.6.2}% 37
\BOOKMARK [2][]{subsection.6.2.3}{6.2.3 probabilistic inductive logic programming}{section.6.2}% 38
\BOOKMARK [2][]{subsection.6.2.4}{6.2.4 generalisation of the current approach}{section.6.2}% 39
\BOOKMARK [1][]{section.6.3}{6.3 Conclusion}{chapter.6}% 40
\BOOKMARK [0][]{section*.28}{Appendices}{}% 41
\BOOKMARK [1][]{section.1..1}{.1 Ethics}{section*.28}% 42
\BOOKMARK [1][]{section.1..2}{.2 Learning tasks}{section*.28}% 43
\BOOKMARK [1][]{section.1..3}{.3 Abduction tasks}{section*.28}% 44
