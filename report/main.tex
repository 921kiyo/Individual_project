\documentclass[12pt,twoside]{report}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage[table]{xcolor}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
% Ctrl + Alt + B

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}
\DeclareMathOperator{\E}{\mathbb{E}}

\newcommand{\reporttitle}{Symbolic Reinforcement Learning using Inductive Logic Programming}
\newcommand{\reportauthor}{Kiyohito Kunii}
\newcommand{\supervisor}{Professor Alessandra Russo}
\newcommand{\degreetype}{MSc in Computing Science}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

\date{June 2018}


% \newtheoremstyle{definition}[section]
\newtheorem{examp}{example}[section]


\begin{document}


% load title page
\input{titlepage}



% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{abstract}
% Your abstract.
% \end{abstract}
%
% \cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Acknowledgments}
% Comment this out if not needed.
%
% \clearpage{\pagestyle{empty}\cleardoublepag e}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents

% ADD BLANK PAGE
% \clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
% IMPERIAL LOGO
% \begin{figure}[tb]
% \centering
% \includegraphics[width = 0.4\hsize]{./figures/imperial}
% \caption{Imperial College Logo. It's nice blue, and the font is quite stylish. But you can choose a different one if you don't like it.}
% \label{fig:logo}
% \end{figure}
% Figure~\ref{fig:logo} is an example of a figure.

There has been successful applications of deep reinforcement learning (DRL) a number of domains, such as video games (\cite{Mnih2015}), game of Go (\cite{Silver2016}) and robotics (\cite{Levine2015}). However, there are still a number of issues to overcome in this method.
First, it requires a large amount of data for training the model, which requires a long time of learning process.
Second, it is considered to be a black-box, meaning the decision making process is unknown to human user and therefore lacks explanation ability.
Third there is no thoughts process to the decision making, which, as XX points out, is a fundamental to the artificial general intelligence.
To tackle these problems, there are a number of different approaches the researchers have been experimenting.

One of the methods is to incorporate symbolic representations into the system to achive more data-efficient learning, which shows a promissing results of this approach (\cite{Garnelo2016}).
In this paper, we explore the potential of inductive logic problemming into reinforcement learning and attempt to solve the problems above.
The advantages of symbolic reinforcement learning are, first of all, the decision making mechanism is understandable by humans rather than being black-box.
Second, it resembles how human reason. There is some aspects of try-and-error in human learning, but they explore reasonings to efficiently learn the surrounding or situations.
Also they effectily use previous experience (background knowledge) when encountered a similar situation.
Finally, the recent advance of ILP research is remarkable and there are a number of new algorithsm that effectively work in non-monotonic senarios based on Answer Set Programmings (ASPs).
--------------------------------------------
Recently there are a number of research that attempted to incorporate symbolic reasoning into DRL, but the combining inductive logic programming and reinforcement learning has not been explored.
Because of the recent advancement of logic-based learning and deep reinforcement learning, combination of both approach would be a next explonation toward artificial general intelligence.

In this paper, our objectives is to explore this new reseach field and see how this combination of the two different learning methods could enhance the learning capability.

In this paper, I further explore incorporation of symbolic machine learning into reinforcement learning to achieve data-efficient learning using Inductive Learning of Answer Set Programs (ILASP), which is the state-of-art symbolic learning method that can be applied to imcomplete and more complex environment.
This research is inspired by \cite{Garnelo2016}, but in this paper I explore symbolic represetations process and include more learning aspect.

This background report will be the part of the final report and is organised as follows: In Chapter \ref{background}, necessary background of logic, inductive logic programming and reinforcement learning are described for this paper. Chapter \ref{related_work} discusses previous research in this particular approach and see the drawbacks of each approach. Chapter \ref{project_overview} shows the tentative architecture of our new approach, using Inductive Learning of Answer Set Programs (ILASP) to generate model of the environment. We also describe the outline of the project. Finally the ethics checklist is provided in Chapter \ref{ethics_checklist}.

\chapter{Background}
\label{background}

This section introduces necessary background of Inductive Logic Programming (Section \ref{ilp}) and Reinforcement Learning (Section \ref{rl}), which provide the foundations of this work.

\section{Inductive Logic Programming (ILP)}
\label{ilp}

Inductive Logic Programming (ILP) is a subfield of machine learning research area aimed at the intersection between machine learning and logic-based knowledge representation \cite{Muggleton1991}. The purpose of ILP is to inductively derive a hypothesis H that is a solution of a learning task, which coveres all positive examples and any of negative examples, given a hypothesis language for search space and cover relation (\cite{DeRaedt1997}). The possible explanation of covers relation is described by the entailment, as shown in Equation \ref{ilp_equation}.

\begin{equation}
B \cap H \models E
\end{equation}
\label{ilp_equation}

where E consists of positive examples (E\textsuperscript{+}) and none of the negative examples (E\textsuperscript{-}).

One of the advantage of ILP over statistical machine learning is that the hypothesis the agent learnt can be easily understood by a human, making the decision more transparent rather than black box.

% TODO Predicate invention

TODO Limitations of Inductive Logic Programming:

In this section, we briefly introduce basic logic nortions, Anser set programming (ASP)

\subsection{Logic Basics}

TODO Satisfiable

To compute an inference task, the syntax of the predicate logic program needs to be converted into Conjunctive Normal Form (CNF), or clausal theory, which consists of a conjunction of clauses, where a clause is disjunction of literals, and a literal can include positive literals and negative literals. A literal is either an atom \textit{p} or \textit{not p} (negation by failure).

\begin{examp} \normalfont Conjunctive Normal Form (Clausal Theory)
\begin{center}
 \textit{(~rain $\vee$ umbrella $\vee$ rain\_shoes) $\wedge$ (~sunny $\vee$ roofer)}
\end{center}
\end{examp}

A \textit{Horn clause} is a subset of CNF, which is a clause with at most one positive literal, where a \textit{definite clause} (also called a \textit{rule} or a \textit{fact}) contains exactly one positive literal, and a \textit{denial} (or a \textit{constraint}) is a clause with no positive literals.
A \textit{normal clause} extends Horn clause with negation as failure.
\\

\subsection{Stable Model Semantics}

Having defined the syntax of clausal logic, we now introduce its semantics under the context of stable model. The semantics of the logic is based on the notion of \textit{interpretation}, which is defined under a \textit{domain}. A \textit{domain} contains all the objects that exist.  For the convenient reasons, we focus on a special interpretations called \textit{Herbrand interpretations} rather than generation interpretations.

The \textit{Herbrand domain} (a.k.a \textit{Herbrand universe}) of clause sets \textit{Th} is the set of all ground terms that are constants and function symbols appeared in \textit{Th}.

%\begin{examp} \normalfont (Herbrand Domain)
%\end{examp}
The \textit{Herbrand base} of \textit{Th} is the set of all ground predicates that are formed by predicate symbols in \textit{Th} and terms in the \textit{Herbrand domain}.

%\begin{examp} \normalfont (Herbrand Base)
%\end{examp}
The \textit{Herbrand interpretation} of a set of definite clauses \textit{Th} is a subset of the Herbrand base of \textit{Th}, which is a set of ground atoms that are true in terms of interpretation.

Interpretation evaluate it to true
Interpretation evaluate it to false

A \textit{Herbrand Model} is a Herbrand interpretation if and only if a set \textit{Th} of clauses is satisfiable. In other words, a set of clauses \textit{Th} is unsatisfiable if no Herbrand model was found.

The \textit{Herbrand Model} is a minimum Herbrand model if and only if none of its subsets is an Herbrand model.
For definite logic programs, there is a unique minimal Herbrand model (the \textit{Least Herbrand Model} \textit{M(P)} ).
For normal logic programs, there may not be a least Herbrand Model.

\textit{Definite Logic Program} is a set of definite rule, and  a definite rule is of the form h $\leftarrow$ a\textsubscript{1}, ..., a\textsubscript{n}, where h , a\textsubscript{1}, ..., a\textsubscript{n} are all atoms. h is the \textit{head} of the rule and a\textsubscript{1}, ..., a\textsubscript{n} are the body of the rule.

\textit{Normal Logic Program} is a set of normal rule, and a normal rule is of the form h $\leftarrow$ a\textsubscript{1}, ..., a\textsubscript{n}, \textit{not} b\textsubscript{1}, ..., \textit{not}  b\textsubscript{n} where h is the head of the rule,
 and a\textsubscript{1}, ..., a\textsubscript{n}, b\textsubscript{1}, ..., b\textsubscript{n} are the body of the rule (both the head and body are all atoms).

To solve a normal logic program \textit{Th}, the program needs to be grounded. The \textit{grounding} of \textit{Th} is the set of all clauses that are c $\in$ \textit{Th} and variables are replaced by terms in the \textit{Herbrand Domain}.

\begin{examp} \normalfont (Grounding)

P = $\begin{cases}
	p  \leftarrow not \ q. \\
	q  \leftarrow p.
      \end{cases}$
\end{examp}
\label{grounding}

ground(P) in Example \ref{grounding} is p:- not q. and q:- p. The algorithm of grounding start with the empty program Q = \{  \} and the relevant grounding is constructed by adding to each rule R to Q given that R is a ground instance of a rule in P and their positive body literals already occurs in the in the of rules in Q. The algorithm terminates when no more rules can be added to Q.
%TODO Explain grounding in ASP context.
%The grounding of a normal logic program P can be obtained by replacing each rule in P with a ground instance of the rule, such that for each atom A in body\textsuperscript{+} (R) (TODO EXPLAIN WHAT THIS IS), already occurs in the head of another ground rule.
Not only the entire program needs to be grounded in order for ASP solver to work, and, unlike Prolog,  but also each rule must be \textit{safe}. A rule \textit{R} is safe if every variable that occurs in the head of the rule occurs at least once in body\textsuperscript{+}(R) .

Since there is no unique least Herbrand Model for a normal logic program, \cite{Gelfond1988} defined Stable Model of a normal logic program. In order to obtain the Stable model of P, P needs to be converted using \textit{Reduct} with respect to an interpretation X. First, if the body of any rule in P contains an atom which is not in X, those rules need to be removed. Second, all default negation atoms in the remaining rules in P need to be removed.

\begin{examp} \normalfont (Reduct)

X = {p}

P = $\begin{cases}
	p  \leftarrow not q. \\
  q  \leftarrow not p. \\
  r  \leftarrow p.
      \end{cases}$
\end{examp}
\label{reduct}

In the Example \ref{reduct}, P\textsuperscript{x} is {p, r $\leftarrow$ p.}.
%Any stable model is a minimal Herbrand model, and stable sets is stable models. The stable models can be found by constructing the result of the program with respect to sets of atoms X (P\textsuperscript{x} in the following 2 steps
A stable model of P is an interpretaiton X if and only if X is the unique least Herbrand Model of ground(P)\textsuperscript{x} in the logic programs.

\subsection{Anwer Set Programming (ASP) Syntax}

Answer set of normal logic program P is a stable model, and Answer Set Programming (ASP) is a normal logic program with extensions: constrains, choice rules and optimisation. ASP program consists of a set of rules, where each rule consists of an atom and literals.
A literal is either an atom \textit{p} or its \textit{default negation} not p (Negation as a failure).
A Constraint is of the form $\leftarrow$ a\textsubscript{1}, ..., a\textsubscript{n}, not b\textsubscript{1}, ..., not b\textsubscript{n}

which is to filtering any irrelevant answer sets.
There are two types of constraints: \textit{hard constraints} and \textit{hard constraints}. Hard constraints are strictly satisfied, whereas soft constraints are must not be satisfied but the sum of the violations should be minimised by assigning XXX.

Choice rule can express possible outcomes given an action choice, which is of the form
l{h\textsubscript{1},...,h\textsubscript{m}}u $\leftarrow$ a\textsubscript{1}, ..., a\textsubscript{n}, not b\textsubscript{1}, ..., not b\textsubscript{n}

where the head is called \textit{aggregates}.

An answer set of ASP program is interpretations that make all the ruls true.

Non-monotonicity.

ASP has true, false and unknown

for non-deterministic to describe transition possibilities.

% DO I NEED OPTIMISATION??
optimisation statement is of the form.

Which is useful to order the answer sets in terms of preference.

\textit{Clingo} is one of the programs to execute the ASP program and returns answer sets of the programs.


\subsection{ILP in Answer set semantics}

There are several ILP frameworks under the Answer set semantics.

Cautious and brave entailments can be defined in terms of induction in ILP \cite{Sakama2009}.

% Sakama 2008 has no concept of negative examples in this paper.

Cautious Induction task is of the form $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ where: \\
B is the background knowledge \\
E\textsuperscript{+} is a set of positive examples \\
E\textsuperscript{-} is a set of negative examples \\

 H $\in$ ILP\textsubscript{cautious} $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ if and only if  \\

 there is at least one answer set A of B $\cup$ H such that: \\
 for every anser set A of B $\cup$ H:
$\forall$ e $\in$ E\textsuperscript{+} : e $\in$ A \\
$\forall$ e $\in$ E\textsuperscript{-} : e $\notin$ A \\

\begin{examp} (Limitation of cautious induction)

1\{situation(P, awake), situation(P, sleep)\}1 :- person(P).

person(john).
\end{examp}
\label{limitation_cautious}

In the example \ref{limitation_cautious}, neither of situation(john, awake) or situation(john, sleep) is false in all answer sets. In this example, it only return person(john).

Cautious entailment may be too restrict.

Similarly, Brave Induction task is of the form $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ where: \\
B is the background knowledge \\
E\textsuperscript{+} is a set of positive examples \\
E\textsuperscript{-} is a set of negative examples \\

 H $\in$ ILP\textsubscript{brave} $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ if and only if there is at least one answer set A of B $\cup$ H such that: \\
$\forall$ e $\in$ E\textsuperscript{+} : e $\in$ A \\
$\forall$ e $\in$ E\textsuperscript{-} : e $\notin$ A \\


By contrast, Brave indction is that it cannot learn constrains as shown in the example \ref{limitation_brave}.

\begin{examp} (Limitation of brave induction)

XXX
\end{examp}
\label{limitation_brave}

\subsection{Learning from Anwer Sets (LAS)}

Cautious induction In order to overcome the limitations of cautious and brave induction,
Learning from Answer Sets (LAS) was developed in \cite{Law2014} to faciliate more complex learning task.

Examples used in LAS is converted from $\langle$ E\textsuperscript{+}, E\textsuperscript{-}$\rangle$ into

Partial Interpretations are of the form $\langle$ E\textsuperscript{inc}, E\textsuperscript{exc}$\rangle$

A Herbrand Interpretations extends a partial interpretation if it include all of the inclusions and none of the exclusions.

\subsection{Inductive Learning of Answer Set Programs (ILASP)}

ILASP is an algorithm that is capable of solving LAS tasks

It is based on two fundamental concepts: positive solutions and violating solutions.
A hypothesis H is a positive solution if and only if
1. H $\subseteq$ S\textsubscript{M} \\
2. $\forall$ e\textsuperscript{+} $\in$ $\exists$ A $\in$ AS(B $\cup$ H) subject to A extends e\textsuperscript{+}\\

A hypothesis H is a violating solution if and only if
1. H $\subseteq$ S\textsubscript{M} \\
2. $\forall$ e\textsuperscript{+} $\in$ E\textsuperscript{+} $\exists$ A $\in$ AS(B $\cup$ H) subject to A extends e\textsuperscript{+}\\
3. $\exists$ e\textsuperscript{-} $\in$ E\textsuperscript{-} $\exists$ A $\in$ AS(B $\cup$ H) subject to A extends e\textsuperscript{-}\\


ILP\textsubscript{LAS} is positive solutions that are not violating solutions.

ILASP task containing a contex-dependent example

\subsubsection{Learning from Ordered Answer Sets}

Weak constraints: useful for modelling the agent's preference

\subsubsection{A Context-dependent Learning from Ordered Answer Sets}


context-dependent partial interpretation (CDPI)


Context-dependent ordering example (CDOE)

Two advantages of adding contex-dependent are it increases the efficiency of learning tasks, and more expressive structure of the background knowlege to particular examples.


\section{Reinforcement Learning}
\label{rl}

\begin{figure}[!htb]
\centering
\includegraphics[width=5cm, height=5cm]{./figures/agent_env}
\caption{Agent and Environment}
\label{agent_env}
\end{figure}

% An RL agent may include either Policy, Value function or Model,

\subsection{Markov Decision Process(MDP)}

A state S\textsubscript{t} is Markov if and only if
P[S\textsubscript{t+1} $\vert$ S\textsubscript{t}] = P[S\textsubscript{t+1} $\vert$ S\textsubscript{1}, ..., S\textsubscript{t}] therefore the probability of reaching S\textsubscript{t+1} depends solely on S\textsubscript{t}, which captures all the relevant information from earilier history.
(Puterman, 1994)
There is an agent who interacts with an envirnment (Figure \ref{agent_env}). At each time step, an action taken by the agent affect the environment state and the reward (or penality) it receives from the action outcome. (TODO Observations)
When an agnet must make a sequence of decision, the sequential decision problem can be formalised using Markov decision process (MDP). MDPs formaly represent a fully observable environment of an agent for reinforcement learning.

A MDP is of the form $\langle$ S, A, T\textsubscript{a}, R\textsubscript{a}, $\gamma$ $\rangle$ where: \\

\begin{itemize}
\item S is the set of finite states that is observable in the environment
\item A is the set of finite actions taken by the agent
\item T\textsubscript{a}(s, s$^\prime$) is a state transition in the form of probability matrix Pr(S\textsubscript{t+1} = s$^\prime$ $\vert$ s\textsubscript{t} = s, a\textsubscript{t} = a), which is the probablity that action a in state s at time t will result in state s$^\prime$ at time t+1.
\item R is a reward function R\textsubscript{a}(s, s$^\prime$) = $\displaystyle \E[R\textsubscript{t+1} $ $\vert$ S\textsubscript{t} = s, A\textsubscript{t} = a], the expected immediate reward that action a in state s at time t will return
\item $\gamma$ is a discount factor $\gamma$ $\in$ [0,1], which represents the preference of the agent for present rewards over future rewards

\end{itemize}

In MDPs, there is an element of delayed reward tradeoff between immediate rewards and its delayed reward

A solution to the sequential decision problem is called a policy $\pi$, a sequence of actions that leads to a solution
% which is a distributin over actions given states.

The transition and reward functions are not necessary to be known to compute $\pi$.

An optimal policy $\pi^*$ is the one that maximise the total rewards in the environment.

The total reward with a discount factor is

The existence of the discount factor can be justified as follows:

TODO
$\displaystyle \E[R\textsubscript{t+1} \vert$ S\textsubscript{t} = s]

Reinforcement learning is a method to get approximated optimal solution.

\subsection{Policy and Value Function}

An agent follows a policy


A policy $\pi$ is optimal if it maximises the action-value function

TODO on-policy and off-policy learning.

A\textsubscript{t} = $\pi$(S\textsubscript{t})

One of the common possibilibies is that the agent chooses an action in order to maximise the discounted sum of future rewards.

A\textsubscript{t} to maximise R\textsubscript{t+1} + $\gamma$ R\textsubscript{t+2} + $\gamma^2$ R\textsubscript{t+3} + ...


An action-value function evaluate a particular state by taking an action according the policy

q\textsubscript{$\pi$} = $\displaystyle \E[R\textsubscript{t+1} $[R\textsubscript{t+1} + $\gamma$ R\textsubscript{t+2} + $\gamma^2$ R\textsubscript{t+3} + ... $\vert$ S\textsubscript{t} = s, A\textsubscript{t} = a, A\textsubscript{t+1 } = a,]




\subsection{Temporal-Difference (TD) Learning}

To solve MDP, one of the approaches is called Temporal-Difference (TD) Learning.

TD learns directly from episodes of experiences, which can be imcomplete.
TD does not require knowledge of MDP transitions and rewards (model-free)
%Sutton 1988

Update value

%\begin{equation}
%\textbf{	V(S\textsubscript{t} \leftarrow V(S\textsubscript{t} + \alpha (R\textsubscript{t+1} + \gamma V(S\textsubscript{t+1}) - V(S\textsubscript{t}))
%}\end{equation}

where R\textsubscript{t+1} + $\gamma$ V(S\textsubscript{t+1}) is the estimated return (a.k.a TD target)

R\textsubscript{t+1} + $\gamma$ V(S\textsubscript{t+1}) - V(S\textsubscript{t}) is TD error.sudo add-apt-repository ppa:texworks/stable

TD updates the estimate by using the estimates of XXX (bootstrap).

The advantages of TD methods
- does not require any model of an environment.
- online learning

\subsection{Q-Learning}

Q-learning is off-policy TD learning defined in [Watkin 1989], where the agent will not rely on the models of the environment (model-free leaning) and only knows about the possible states and actionns. The transition states and reward probability functions are unknown to the agent.
It is of the form:

%\begin{equation}
%Q(s\textsubscript{t},a\textsubscript{t}) \leftarrow Q(s\textsubscript{t},a\textsubscript{t}) + \alpha(R\textsubscript{t+1} + \gamma max (a+t) Q(s\textsubscript{t+1}, a\textsubscript{t+1}) - Q(s\textsubscript{t}, a\textsubscript{t}))
%
%\end{equation}
where $\alpha$ is the learning rate, $\gamma$ is a discount rate between 0 and 1. The equation is used to update the state-action value function called Q function. The function Q(S,A) predicts the best action A in state S to maximise the total cumulative rewards.
The optimal Q-function Q\textsuperscript{*}(s,a) represents XXX for the agent to selection action a given that it is in state s.

% Model free can be done using Monte Carlo Policy evaluation
% One way to solve the Bellman Optimality equation is Q-leraning
% U(s) = max a Q(s,a)
% The function is estimated by Q-learning, which repeately updates Q(s,a) using the Bellman Equation.

TODO INSERT Q-learning ALGORITHM HERE

Epsilon greedy

\subsection{Model-based vs Model-free Learning}

A model M is a representation of an MDP where the state space and action space are assumed to be known. The model represents state transitions and rewards. In this case, the problem to be solved becomes a planning problem. A planning is for a series of actions to achive the agent's goal.
Most of the reinforcement learning problems are model-free learning, where M is not unknown and the agnet learns to achieve the goal by the interactions with the environment. Thus the agent knows only possible states and actions, and the transition state and reward probability functions are unknown.

the performance of model-based RL is limited to optimal policy given the model M. Thus when the model is not a representation of the true MDP, the planning algorithms will not lead to the optimal policy, but suboptimal policy. The solution to this problem is either to use model-free approach when the model is inaccurate or incorporate undertainty of the model using XXX.

One algorithm which combine both aspects of model learning is called Dyna (\cite{Sutton1990}), which is shown in Figure \ref{dyna}.

\begin{figure}[!htb]
\centering
\includegraphics[width=8cm, height=6cm]{./figures/dyna}
\caption{Relationships among learning, planning and acting \ref{Montague1999}}
\label{dyna}
\end{figure}

Dyna learns a model from real experience and use the model to generate simulated experience to update the evaluation functions.

This approach is more effective because the simulated experience is relatively easy to generate compared to having to have real experience, thus less iterations.

TODO ADD MATH

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
\label{related_work}
% \section{Deep Reinforcement Learning and its issues}

The use of symbolic representations to achieve data-efficient learning was traditionally discussed in relational reinforcement learnign (RLL).

More recently, \ref{Garnelo2016} introduced the proof of concept for incorporating symbolic front end as a means of converting low-dimensional symbolic representation into spatio-temporal representations, which will be the state transitions input of reinforcement learning.

just a symbolic representations and not learning
segmentation was done manually.
As demonstrated in XXX, a single pixel can dramatically influence the policy (Maybe not so important here??).


This symbolic approach was followed by \ref{Garcez2018}, which improved simbolic representations in two ways. First,



Transparency and interpretable capability of the model is another important aspect for machine learning applications.

XXX[Programmati...] developed a programmatically interpretable reinforcement learning which finds a policy that can be represented in a human-readable programming language.

Two most studied approach for using previous learning exprience is meta-learning and transfer learning

Artificial General Intelligence

Definision of AGI

The history of data-efficient learning
What other people have done in this.

The advance of statistical machine learning methods, especially deeep reinforcement learning

AlphaGo, and AlphaGo Zero

Study of symbolic machine learning roots from
% Relational Reinforcement Learning

Baysian Optimisation

Symbolic Deep reinforcement learning

Some implementation: German paper

The paper was the application of symbolic representatiojns into a very simple game to demonstrate this proof of concept would actually work.
By contract, there is active reserach in symbolic machine learning, which focuses on logic=based learning rather than statistical machine learning.
For example, XX shows the agent can learn XXX from a noisy examples, with only a very few training examples.


% \section{Symbolic Reinforcement Learning}
Incorporation of logic into reinforcement learning dates back to the study of relational reinforcement learning,


More recently there has been a number of attempts to incorporate ASP into reinforcement learning.

There are a number of reseached conducted in applying DNN to symbolic reasoning.
For example,

[From GamePlay to Symbolic Reasoning]


TODO Explain how symbolic learning works


\chapter{Project Overview}
\label{project_overview}

\section{Proposed Architecture}
\label{proposed_architecture_section}

The proposed tentative architecture is shown in Figure \ref{proposed_architecture}. The overall architecture resembles Sutton's Dyna architecture (\cite{Sutton1990}), which combined both model-free learning and model-based learning.

\begin{figure}[!htb]
\centering
\includegraphics[width=15cm, height=9cm]{./figures/ILASRL}
\caption{Proposed reinforcement learning architecture. ILASPRL learns to generate a model and updates based on the interaction with the environment, which is used to facilite the policy evaluation. }
\label{proposed_architecture}
\end{figure}

% TODO What would you learn in my context? Relashinship of the objects?
% Objects, types, locations and interactions.

\subsection{ASP Representation Generation}

The inputs from the experience needs to be converted in ASP form, which can be used to proceed the inductive learning in ILASP(RL).
Observations from the interaction with the environment are state transition, reward and action of the agent, which can be directly converted using XXX. Example \ref{ilasprl_input} is ASP input form that is feeded into ILASP(RL) phase.

\begin{examp} (Examples for ILASPRL (input))

agent\_before(S1, T1).

agent\_after(S1, T1).
reward(R).
action(A).

\end{examp}
\label{ilasprl_input}

\subsection{Model Generation and Update using ILASP(RL)}

Once the input is converted, the agent must learn the following definition of the model of the environment.

\begin{examp} (Example of learning task (output))

% valid\_move(C1, T):- \\
%   adjacent(C1, C2), \\
%   agent\_at(C2, T), \\
%   not obstacle(C1, C2), \\
%   not enemy(C1, C2). \\
% \\
% valid\_move(C1, T):- \\
%   link(C2, C1), \\
%   agent\_at(C2, T). \\
valid\_move(C0, C1):- previous(C0, C1).
\\
previous(C2, C1):-

agent\_at(C0, C1),
adjacent(C0, C2),
not obstacle(C0, C2),
not enemy(C0, C2).
\\
state\_transition(S1, S2, T2):-
  agent\_before(S1, T1),
  agent\_after(S2, T2).

\end{examp}
\label{learning_task}

The background knowldge is empty, and each example contain different transition of the agent.

The model is updated as the agent explore more environment.


In addition, when the environment has been changed during the exploration, the agent creates a new model for this new environment.


In two different but similar scenarios, the model is generalised based on the two models, and the agent's model is more general which covers both games. Thus in theory, the agent should still be able to exploit the efficient learning from the model-based learning, facilitating the transfer learning.


Example: model refinement in a simple Grid world


Implement baseline performance (DQN, Q-learning, ASPRL)

Due to the complex environment of the chosen game, I will not implemnet the original DSRL, since the feature extractions from the pixel would only work in a very simple pixel environment,

Pipeline

The basic architecture follows the similar method in XXX, but I also add symbolic learning to these extracted symbolic features using ILASP.

Apply ILASP to the ASP, which involves development of the pipeline of ILASP in Python

Finally use Q-learning that allow

which measurement would you use? (grid word, something else? GVGAL games)
Summarise different types of knowledge representations (Objects ?? relationship?)

Common sense

Lastly, I will also test the capability of transfer learning for this new method.

\section{Project Outline}

The next phase of the individual project is implementation and experiments. I will implement the proposed architecture using Python and ILASP (Clingo), and compare the performance

The platform I am planning to use to measure the performance of my approach will be GVG-AI Framework, which was created for the General Video Game AI Competition \footnote{http://www.gvgai.net/}, which provides game environments for an agent that should be able to play a wide variety of games without knowing which games are to be played.
The underline language is the Video Game Definition Language (VGDL), which is a high-level description language for 2D video games providing a platform for computational intelligence research (\cite{Schaul2013}).

Especially, I will measure the following aspects of the new Algorithms

Learning efficiency

Transfer Learning capability

XXX


The proposed architecture is not finalised and will be reviewed regularly as I proceed the development of the architecture.

% \section{Contribution}
%
% To my knowledge, this is the first time that both symbolic learning method is incorporated into a reinforcement learning to facilitate learning process
%
% \section{Methods}


\chapter{Ethics Checklist}
\label{ethics_checklist}

{
\renewcommand*{\arraystretch}{1.3}
\begin{longtable}{ |p{13.2cm}|p{0.6cm}|p{0.6cm}| }
\hline
 & \bf Yes & \bf No \\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 1: HUMAN EMBRYOS/FOETUSES} \\
\hline

Does your project involve Human Embryonic Stem Cells? & & \checkmark\\
\hline

Does your project involve the use of human embryos? & & \checkmark\\
\hline

Does your project involve the use of human foetal tissues / cells? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 2: HUMANS} \\
\hline

Does your project involve human participants? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 3: HUMAN CELLS / TISSUES} \\
\hline

Does your project involve human cells or tissues? (Other than from “Human Embryos/Foetuses” i.e. Section 1)? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 4: PROTECTION OF PERSONAL DATA} \\
\hline

Does your project involve personal data collection and/or processing? & & \checkmark\\
\hline

Does it involve the collection and/or processing of sensitive personal data (e.g. health, sexual lifestyle, ethnicity, political opinion, religious or philosophical conviction)? & & \checkmark\\
\hline

Does it involve processing of genetic information? & & \checkmark\\
\hline

Does it involve tracking or observation of participants? It should be noted that this issue is not limited to surveillance or localization data. It also applies to Wan data such as IP address, MACs, cookies etc. & & \checkmark\\
\hline

Does your project involve further processing of previously collected personal data (secondary use)? For example Does your project involve merging existing data sets? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 5: ANIMALS} \\
\hline

Does your project involve animals? & & \checkmark\\
\hline


\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 6: DEVELOPING COUNTRIES} \\
\hline

Does your project involve developing countries? & & \checkmark\\
\hline

If your project involves low and/or lower-middle income countries, are any benefit-sharing actions planned? & & \checkmark\\
\hline

Could the situation in the country put the individuals taking part in the project at risk? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 7: ENVIRONMENTAL PROTECTION AND SAFETY} \\
\hline

Does your project involve the use of elements that may cause harm to the environment, animals or plants? & & \checkmark\\
\hline

Does your project deal with endangered fauna and/or flora /protected areas? & & \checkmark \\
\hline

Does your project involve the use of elements that may cause harm to humans, including project staff? & & \checkmark\\
\hline

Does your project involve other harmful materials or equipment, e.g. high-powered laser systems? & & \checkmark\\
\hline


\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 8: DUAL USE} \\
\hline

Does your project have the potential for military applications? & & \checkmark\\
\hline

Does your project have an exclusive civilian application focus? & & \checkmark\\
\hline

Will your project use or produce goods or information that will require export licenses in accordance with legislation on dual use items? & & \checkmark\\
\hline

Does your project affect current standards in military ethics – e.g., global ban on weapons of mass destruction, issues of proportionality, discrimination of combatants and accountability in drone and autonomous robotics developments, incendiary or laser weapons? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 9: MISUSE} \\
\hline

Does your project have the potential for malevolent/criminal/terrorist abuse? & & \checkmark\\
\hline

Does your project involve information on/or the use of biological-, chemical-, nuclear/radiological-security sensitive materials and explosives, and means of their delivery? & & \checkmark\\
\hline

Does your project involve the development of technologies or the creation of information that could have severe negative impacts on human rights standards (e.g. privacy, stigmatization, discrimination), if misapplied? & \checkmark& \\
\hline

Does your project have the potential for terrorist or criminal abuse e.g. infrastructural vulnerability studies, cybersecurity related project? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 10: LEGAL ISSUES} \\
\hline

Will your project use or produce software for which there are copyright licensing implications? & \checkmark& \\
\hline

Will your project use or produce goods or information for which there are data protection, or other legal implications? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 11: OTHER ETHICS ISSUES} \\
\hline

Are there any other ethics issues that should be taken into consideration? & & \checkmark \\
\hline

\end{longtable}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Contribution}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Experimental Results}
%
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Conclusion}

% Further research
% probabilistic inductive logic programming instead of ASP.

%% bibliography
\bibliographystyle{plain}
\bibliography{references}

\end{document}
