\documentclass[12pt,twoside]{report}
\usepackage{fixltx2e}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}

\newcommand{\reporttitle}{Symbolic Reinforcement Learning using Inductive Logic Programming}
\newcommand{\reportauthor}{Kiyohito Kunii}
\newcommand{\supervisor}{Professor Alessandra Russo}
\newcommand{\degreetype}{MSc in Computing Science}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

\date{June 2018}

\begin{document}

% load title page
\input{titlepage}

% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{abstract}
% Your abstract.
% \end{abstract}
%
% \cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Acknowledgments}
% Comment this out if not needed.
%
% \clearpage{\pagestyle{empty}\cleardoublepag e}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents

% ADD BLANK PAGE
% \clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

% IMPERIAL LOGO
% \begin{figure}[tb]
% \centering
% \includegraphics[width = 0.4\hsize]{./figures/imperial}
% \caption{Imperial College Logo. It's nice blue, and the font is quite stylish. But you can choose a different one if you don't like it.}
% \label{fig:logo}
% \end{figure}
% Figure~\ref{fig:logo} is an example of a figure.


There has been successful applications of deep reinforcement learning (DRL) a number of domains, such as games and robotics (TODO INSERT REFERENCES).
DRL is considered to be a step towards artificial general intelligence.
As pointed by XXX, however, there are still a number of issues to overcome in this method.
First, it requires a large amount of data for training the model, which requires a long time of learning process.
Second, it is considered to be a black-box, meaning the decision making process is unknown to human user and therefore lacks explanation ability.
Third there is no thoughts process to the decision making, which, as XX points out, is a fundamental to the artificial general intelligence.
To tackle these problems, there are 3 main streams of research on this field.
First main research is focused on applying Baysian statistics XXX, the second main research is XXX.
Recently, the XXX attempted to incorporate symbolic representations into the system to achive more data-efficient learning, which shows a promissing results of this approach.

In this paper, I further explore incorporation of symbolic machine learning into reinforcement learning to achieve data-efficient learning using Inductive Learning of Answer Set Programs (ILASP), which is the state-of-art symbolic learning method that can be applied to imcomplete and more complex environment.
This research is inspired by \cite{garnelo2016towards}, but in this paper I explore symbolic represetations process and include more learning aspect.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}

\section{Deep Reinforcement Learning and its issues}

\subsection{Data efficient learning}

\subsection{Transfer learning}

\subsection{Planning}

\subsection{Transparency}

Transparency and interpretable capability of the model is another important aspect for machine learning applications.


XXX[Programmati...] developed a programmatically interpretable reinforcement learning which finds a policy that can be represented in a human-readable programming language.


Two most studied approach for using previous learning exprience is meta-learning and transfer learning

Artificial General Intelligence

Definision of AGI

The history of data-efficient learning
What other people have done in this.

The advance of statistical machine learning methods, especially deeep reinforcement learning

AlphaGo, and AlphaGo Zero

Study of symbolic machine learning roots from
% Relational Reinforcement Learning

Baysian Optimisation

RNN approach

Symbolic Deep reinforcement learning

Some implementation: German paper

The paper was the application of symbolic representatiojns into a very simple game to demonstrate this proof of concept would actually work.
By contract, there is active reserach in symbolic machine learning, which focuses on logic=based learning rather than statistical machine learning.
For example, XX shows the agent can learn XXX from a noisy examples, with only a very few training examples.


\section{Symbolic Reinforcement Learning}
Incorporation of logic into reinforcement learning dates back to the study of relational reinforcement learning,


More recently there has been a number of attempts to incorporate ASP into reinforcement learning.

There are a number of reseached conducted in applying DNN to symbolic reasoning.
For example,

[From GamePlay to Symbolic Reasoning]

\chapter{Background}
\section{Inductive Logic Programming}
Inductive Logic Programming (ILP) is a subfield of research area aimed at the intersection of machine learning and logic programming (MUGGLETON 1991). The purpose of ILP is to derive a hypothesis H that is a solution of a learning task, which coveres all positive examples and any of negative examples.

\begin{equation}
B \cap H \models E
\end{equation}

TODO Herbrand Model

TODO Least Herbrand Model

Definite Logic Program is a set of definite rule where: \newline

a definite rule is of the form h $\leftarrow$ a\textsubscript{1}, ..., a\textsubscript{n}, where h , a\textsubscript{1}, ..., a\textsubscript{n} are all atoms.

whereas

Normal Logic Program is a set of normal rule where

a normal rule is of the form h $\leftarrow$ a\textsubscript{1}, ..., a\textsubscript{n}, \textit{not} b\textsubscript{1}, ..., \textit{not}  b\textsubscript{n} where h is the head of the rule,
 and a\textsubscript{1}, ..., a\textsubscript{n}, b\textsubscript{1}, ..., b\textsubscript{n} are the body of the rule (both the head and body are all atoms).

\subsection{Stable Model Semantics}

Stable Model of a normal logic program

\subsection{Anwer Set Programming (ASP)}

Answer Set Programming (ASP) is a declarative programming


Literals

Negation as a failure

constrains is of the form

Clingo

\begin{theorem}
\leftarrow a\textsubscript{1}, ..., a\textsubscript{n}, not b\textsubscript{1}, ..., not b\textsubscript{n}
\end{theorem}

Constraints are to filtering any irrelevant answer sets.

Syntax Examples

There are two types of constraints: soft and hard constraints. Soft constraint is XXX

Hard constraint is XXX

optimisation statement is of the form.

Which is useful to order the answer sets in terms of preference.

Syntax examples.

Aggregate

choice rules

An ASP program P is normal logic program with addition of choice rules, constraints and optimisation statement.

Answer set of P is


\subsubsection{Cautious Induction}

Sakama 2008

no concept of negative examples in this paper.

Cautious Induction task is of the form $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ where: \\
B is the background knowledge \\
E\textsuperscript{+} is a set of positive examples \\
E\textsuperscript{-} is a set of negative examples \\

 H \in ILP\textsubscript{cautious} $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ if and only if  \\

 there is at least one answer set A of B $\cup$ H such that: \\
 for every anser set A of B $\cup$ H:
\forall e \in E\textsuperscript{+} : e \in A \\
\forall e \in E\textsuperscript{-} : e \notin A \\

One of the limitation of cautious indction is that XX

Sakama 2009


\subsubsection{Brave Induction}

Similarly, Brave Induction task is of the form $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ where: \\
B is the background knowledge \\
E\textsuperscript{+} is a set of positive examples \\
E\textsuperscript{-} is a set of negative examples \\

 H \in ILP\textsubscript{brave} $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ if and only if there is at least one answer set A of B $\cup$ H such that: \\
\forall e \in E\textsuperscript{+} : e \in A \\
\forall e \in E\textsuperscript{-} : e \notin A \\

One of the limitation of brave indction is that it cannot learn constrains as shown in the example

Sakama 2009

\subsection{Learning from Anwer Sets (LAS)}

Learning from Answer Sets was developed in [Law et al 2014] to overcome limitations of cautious induction and brave induction.


Examples used in LAS is converted from $\langle$ E\textsuperscript{+}, E\textsuperscript{-}$\rangle$ into

Partial Interpretations are of the form $\langle$ E\textsuperscript{inc}, E\textsuperscript{exc}$\rangle$

A Herbrand Interpretations extends a partial interpretation if it include all of the inclusions and none of the exclusions.

\subsection{Inductive Learning of Answer Set Programs (ILASP)}

ILASP is an algorithm that is capable of solving LAS tasks

It is based on two fundamental concepts: positive solutions and violating solutions.
A hypothesis H is a positive solution if and only if
1. H \subseteq S\textsubscript{M} \\
2. \forall e\textsuperscript{+} \in \exists A \in AS(B \cup H) subject to A extends e\textsuperscript{+}\\

A hypothesis H is a violating solution if and only if
1. H \subseteq S\textsubscript{M} \\
2. \forall e\textsuperscript{+} \in E\textsuperscript{+} \exists A \in AS(B \cup H) subject to A extends e\textsuperscript{+}\\
3. \exists e\textsuperscript{-} \in E\textsuperscript{-} \exists A \in AS(B \cup H) subject to A extends e\textsuperscript{-}\\


ILP\textsubscript{LAS} is positive solutions that are not violating solutions.

ILASP task containing a contex-dependent example

TODO Explain how symbolic learning works

TODO What would you learn in my context? Relashinship of the objects?
Objects, types, locations and interactions.

\section{Reinforcement Learning}
An RL agent may include either Policy, Value function or Model,

where policy

On-Policy
Off-policy

Bellman Equation

\subsection{Markov Decision Process(MDP)}

MDPs formaly represent a fully observable environment of an agent for reinforcement learning.

A state S\textsubscript{t} is Markov if and only if \\
P[S\textsubscript{t+1} $\vert$ S\textsubscript{t}] = P[S\textsubscript{t+1} $\vert$ S\textsubscript{1}, ..., S\textsubscript{t}] therefore the probability of reaching S\textsubscript{t+1} depends solely on S\textsubscript{t}, which captures all the relevant information from earilier history.

A MDP is of the form \langle S, A, T, R, \gamma \rangle where: \\

\begin{itemize}
\item S is the set of finite states that is observable in the environment
\item A is the set of finite actions executable by the agent
\item T is a state transition in the form of probability matrix XXXXX
\item R is a reward function
\item $\gamma$ is a discount factor $\gamma$ $\ooalign$ [0,1]
\end{itemize}


\subsection{Temporal-Difference (TD) Learning}

TD learns directly from episodes of experiences, which can be imcomplete.
TD does not require knowledge of MDP transitions and rewards (model-free)
Sutton 1988

Update value
\begin{equation}
V(S\textsubscript{t} \leftarrow V(S\textsubscript{t} + \alpha (R\textsubscript{t+1} + \gamma V(S\textsubscript{t+1}) - V(S\textsubscript{t}))
\end{equation}

where R\textsubscript{t+1} + $\gamma$ V(S\textsubscript{t+1}) is the estimated return (a.k.a TD target)

R\textsubscript{t+1} + $\gamma$ V(S\textsubscript{t+1}) - V(S\textsubscript{t}) is TD error.


TD updates the estimate by using the estimates of XXX (bootstrap).

The advantages of TD methods
- does not require any model of an environment.
- online learning
\subsection{Q-Learning}
what is Q function

Q-learning is off-policy TD learning defined in [Watkin 1989], which is of the form:

\begin{equation}
Q(s\textsubscript{t},a\textsubscript{t}) \leftarrow Q(s\textsubscript{t},a\textsubscript{t}) + \alpha(R\textsubscript{t+1} + \gamma max (a+t) Q(s\textsubscript{t+1}, a\textsubscript{t+1}) - Q(s\textsubscript{t}, a\textsubscript{t}))

\end{equation}
\\
where $\alpha$ is the learning rate, $\gamma$ is a discount rate between 0 and 1.

this equation is used to upate action-value function

Model free learning: derectly derive an optimal policy by interacting with the environment without the model

Model free can be done using Monte Carlo Policy evaluation

One way to solve the Bellman Optimality equation is Q-leraning

U(s) = max a Q(s,a)


a function Q(S,A) which predicts the best action A in state S to maximise the total cumulative rewards.

The function is estimated by Q-learning, which repeately updates Q(s,a) using
the Bellman Equation.

TODO INSERT Q-learning ALGORITHM HERE


Epsilon greedy

\section{Deep
Reinforcement Learning}

\subsection{Convolutional Neural Networks (CNN)}

\subsection{Deep Q-Networks (DQNs)}

\section{Symbolic Reinforcement Learning}


Explain the paper
\section{GVGAI Framework}

The Video Game Definition Language (VGDL)

SpriteSet defines all the sprites available in the game.
LevelMapping defines relationships among characters, and the sprites available.
InteractionSet defines what events occur when two sprites collide
TerminationSet defines the end conditions of the game, and decides whether the player wins or not.

% Timeout and SpriteCounter

\chapter{Project Overview}
\section{Motivation}

Why symbolic reinforcement learning is good attempt


Reason 1: Complehensive by humans $\rightarrow$ Explanable rather than black-box

Reason 2: Similar to the human, it uses reasoning

Use of previous experience (background knowledge)
As discussed in XX, there are many research that attempted to incorporate symbolic reasoning into DRL, but there is not much research on incorporating symbolc learning with reinforcement learning

However there is a room for exploration on this field.

Reason 3: Recent advance of ILASP is promissing

Because of the recent advancement of logic-based learning and deep reinforcement learning, combination of both approach would be a next explonation toward artificial general intelligence.

\section{Objectives}

combining the two novel approaches to overcome the problems of the QND

\section{Project outline}

The probject outline

Implement baseline performance (DQN, Q-learning, ASPRL)

Due to the complex environment of the chosen game, I will not implemnet the original DSRL, since the feature extractions from the pixel would only work in a very simple pixel environment,

Pipeline

The basic architecture follows the similar method in XXX, but I also add symbolic learning to these extracted symbolic features using ILASP.

Apply ILASP to the ASP, which involves development of the pipeline of ILASP in Python

Finally use Q-learning that allow

which measurement would you use? (grid word, something else? GVGAL games)
Summarise different types of knowledge representations (Objects ?? relationship?)

Common sense

Implement based on \"Towards Deep Symbolic Reinforcement Learning \"

Lastly, I will also test the capability of transfer learning for this new method. 

\section{Contribution}

To my knowledge, this is the first time that both symbolic learning method is incorporated into a reinforcement learning to facilitate learning process

\section{Methods}

\section{Legal and Ethical Issues}

???


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Contribution}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Experimental Results}
%
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Conclusion}

\section{References}
%% bibliography
\bibliographystyle{apa}
\bibliography{references}

\end{document}
