\documentclass[12pt,twoside]{report}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage[table]{xcolor}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{amsthm}
\usepackage{subdepth}
\newtheorem{theorem}{Theorem}
% Ctrl + Alt + B

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}
\DeclareMathOperator{\E}{\mathbb{E}}

\newcommand{\reporttitle}{Symbolic Reinforcement Learning using Inductive Logic Programming}
\newcommand{\reportauthor}{Kiyohito Kunii}
\newcommand{\supervisor}{Professor Alessandra Russo}
\newcommand{\degreetype}{MSc in Computing Science}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

\date{June 2018}


% \newtheoremstyle{definition}[section]
\newtheorem{examp}{example}[section]


\begin{document}


% load title page
\input{titlepage}



% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{abstract}
% Your abstract.
% \end{abstract}
%
% \cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Acknowledgments}
% Comment this out if not needed.
%
% \clearpage{\pagestyle{empty}\cleardoublepag e}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents

% ADD BLANK PAGE
% \clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
% IMPERIAL LOGO
% \begin{figure}[tb]
% \centering
% \includegraphics[width = 0.4\hsize]{./figures/imperial}
% \caption{Imperial College Logo. It's nice blue, and the font is quite stylish. But you can choose a different one if you don't like it.}
% \label{fig:logo}
% \end{figure}
% Figure~\ref{fig:logo} is an example of a figure.

There has been successful applications of deep reinforcement learning (DRL) a number of domains, such as video games (\cite{Mnih2015}), game of Go (\cite{Silver2016}) and robotics (\cite{Levine2015}). However, there are still a number of issues to overcome in this method.
First, it requires a large amount of data for training the model, which requires a long time of learning process.
Second, it is considered to be a black-box, meaning the decision making process is unknown to human user and therefore lacks explanation ability. Third there is no thoughts process to the decision making, such as understanding relational representations or planning. 
To tackle these problems, there are a number of different approaches the researchers have been experimenting.
One of the methods is to incorporate symbolic representations into the system to achive more data-efficient learning, which shows a promissing potential of this approach (\cite{Garnelo2016}).
In this paper, we explore a potential of incorporating inductive logic problemming into reinforcement learning and attempt to solve the problems above.
The advantages of symbolic reinforcement learning are, first of all, the decision making mechanism is understandable by humans rather than being black-box.
Second, it resembles how human reason. There is some aspects of try-and-error in human learning, but humans exploit reasonings to efficiently learn the surrounding or situations. Also they effectively use previous experience (background knowledge) when encountered a similar situation.
Finally, the recent advance of ILP research enabled us to apply ILP in more complex situations and there are a number of new algorithms that effectively work in non-monotonic senarios based on Answer Set Programmings (ASPs).

After \cite{Garnelo2016},  there are several researches that further explored incorporation of symbolic reasoning into DRL, but the combining inductive logic programming and reinforcement learning has not been explored.
Because of the recent advancement of logic-based learning and deep reinforcement learning, combination of both approach would be a next explonation toward artificial general intelligence.

In this paper, our objectives is to explore this new reseach field and see how this combination of the two different learning methods could enhance the learning capability.

In this paper, I further explore incorporation of symbolic machine learning into reinforcement learning to achieve data-efficient learning using Inductive Learning of Answer Set Programs (ILASP), which is the state-of-art symbolic learning method that can be applied to imcomplete and more complex environment.
This research is inspired by \cite{Garnelo2016}, but in this paper I explore symbolic represetations process and include more learning aspect.

This background report will be the part of the final report and is organised as follows: In Chapter \ref{background}, necessary background of logic, inductive logic programming and reinforcement learning are described for this paper. Chapter \ref{related_work} discusses previous research in this particular approach and see the drawbacks of each approach. Chapter \ref{project_overview} shows the tentative architecture of our new approach, using Inductive Learning of Answer Set Programs (ILASP) to generate model of the environment. We also describe the outline of the project. Finally the ethics checklist is provided in Chapter \ref{ethics_checklist}.

\chapter{Background}
\label{background}

This section introduces necessary background of Inductive Logic Programming (Section \ref{ilp}) and Reinforcement Learning (Section \ref{rl}), which provide the foundations of this work.

\section{Inductive Logic Programming (ILP)}
\label{ilp}

Inductive Logic Programming (ILP) is a subfield of machine learning research area aimed at the intersection between machine learning and logic-based knowledge representation \cite{Muggleton1991}. The purpose of ILP is to inductively derive a hypothesis H that is a solution of a learning task, which coveres all positive examples and any of negative examples, given a hypothesis language for search space and cover relation (\cite{DeRaedt1997}). The possible explanation of covers relation is described by the entailment, as shown in Equation \ref{ilp_equation}.

\begin{equation}
B \cap H \models E
\end{equation}
\label{ilp_equation}

where E consists of positive examples (E\textsuperscript{+}) and none of the negative examples (E\textsuperscript{-}).

One of the advantage of ILP over statistical machine learning is that the hypothesis the agent learnt can be easily understood by a human, making the decision more transparent rather than black box.

% TODO Predicate invention

TODO Limitations of Inductive Logic Programming:

In this section, we briefly introduce basic logic notions, Anser set programming (ASP)

\subsection{Logic Basics}

TODO Satisfiable

To compute an inference task, the syntax of the predicate logic program needs to be converted into Conjunctive Normal Form (CNF), or clausal theory, which consists of a conjunction of clauses, where a clause is disjunction of literals, and a literal can include positive literals and negative literals. A literal is either an atom \textit{p} or \textit{not p} (negation by failure).

\begin{examp} \normalfont Conjunctive Normal Form (Clausal Theory)
\begin{center}
 \textit{(~rain $\vee$ umbrella $\vee$ rain\_shoes) $\wedge$ (~sunny $\vee$ roofer)}
\end{center}
\end{examp}

A \textit{Horn clause} is a subset of CNF, which is a clause with at most one positive literal, where a \textit{definite clause} (also called a \textit{rule} or a \textit{fact}) contains exactly one positive literal, and a \textit{denial} (or a \textit{constraint}) is a clause with no positive literals.
A \textit{normal clause} extends Horn clause with negation as failure.
\\

\subsection{Stable Model Semantics}

Having defined the syntax of clausal logic, we now introduce its semantics under the context of stable model. The semantics of the logic is based on the notion of \textit{interpretation}, which is defined under a \textit{domain}. A \textit{domain} contains all the objects that exist.  For the convenient reasons, we focus on a special interpretations called \textit{Herbrand interpretations} rather than generation interpretations.

The \textit{Herbrand domain} (a.k.a \textit{Herbrand universe}) of clause sets \textit{Th} is the set of all ground terms that are constants and function symbols appeared in \textit{Th}.

%\begin{examp} \normalfont (Herbrand Domain)
%\end{examp}
The \textit{Herbrand base} of \textit{Th} is the set of all ground predicates that are formed by predicate symbols in \textit{Th} and terms in the \textit{Herbrand domain}.

%\begin{examp} \normalfont (Herbrand Base)
%\end{examp}
The \textit{Herbrand interpretation} of a set of definite clauses \textit{Th} is a subset of the Herbrand base of \textit{Th}, which is a set of ground atoms that are true in terms of interpretation.

Interpretation evaluate it to true
Interpretation evaluate it to false

A \textit{Herbrand Model} is a Herbrand interpretation if and only if a set \textit{Th} of clauses is satisfiable. In other words, a set of clauses \textit{Th} is unsatisfiable if no Herbrand model was found.

The \textit{Herbrand Model} is a minimum Herbrand model if and only if none of its subsets is an Herbrand model.
For definite logic programs, there is a unique minimal Herbrand model (the \textit{Least Herbrand Model} \textit{M(P)} ).
For normal logic programs, there may not be a least Herbrand Model.

\textit{Definite Logic Program} is a set of definite rule, and  a definite rule is of the form h $\leftarrow$ a\textsubscript{1}, ..., a\textsubscript{n}, where h , a\textsubscript{1}, ..., a\textsubscript{n} are all atoms. h is the \textit{head} of the rule and a\textsubscript{1}, ..., a\textsubscript{n} are the body of the rule.

\textit{Normal Logic Program} is a set of normal rule, and a normal rule is of the form h $\leftarrow$ a\textsubscript{1}, ..., a\textsubscript{n}, \textit{not} b\textsubscript{1}, ..., \textit{not}  b\textsubscript{n} where h is the head of the rule,
 and a\textsubscript{1}, ..., a\textsubscript{n}, b\textsubscript{1}, ..., b\textsubscript{n} are the body of the rule (both the head and body are all atoms).

To solve a normal logic program \textit{Th}, the program needs to be grounded. The \textit{grounding} of \textit{Th} is the set of all clauses that are c $\in$ \textit{Th} and variables are replaced by terms in the \textit{Herbrand Domain}.

\begin{examp} \normalfont (Grounding)

P = $\begin{cases}
	p  \leftarrow not \ q. \\
	q  \leftarrow p.
      \end{cases}$
\end{examp}
\label{grounding}

ground(P) in Example \ref{grounding} is p:- not q. and q:- p. The algorithm of grounding start with the empty program Q = \{  \} and the relevant grounding is constructed by adding to each rule R to Q given that R is a ground instance of a rule in P and their positive body literals already occurs in the in the of rules in Q. The algorithm terminates when no more rules can be added to Q.
%TODO Explain grounding in ASP context.
%The grounding of a normal logic program P can be obtained by replacing each rule in P with a ground instance of the rule, such that for each atom A in body\textsuperscript{+} (R) (TODO EXPLAIN WHAT THIS IS), already occurs in the head of another ground rule.
Not only the entire program needs to be grounded in order for ASP solver to work, and, unlike Prolog,  but also each rule must be \textit{safe}. A rule \textit{R} is safe if every variable that occurs in the head of the rule occurs at least once in body\textsuperscript{+}(R) .

Since there is no unique least Herbrand Model for a normal logic program, \cite{Gelfond1988} defined Stable Model of a normal logic program. In order to obtain the Stable model of P, P needs to be converted using \textit{Reduct} with respect to an interpretation X. First, if the body of any rule in P contains an atom which is not in X, those rules need to be removed. Second, all default negation atoms in the remaining rules in P need to be removed.

\begin{examp} \normalfont (Reduct)

X = {p}

P = $\begin{cases}
	p  \leftarrow not\ q. \\
  q  \leftarrow not\ p. \\
  r  \leftarrow p.
      \end{cases}$
\end{examp}
\label{reduct}

In the Example \ref{reduct}, P\textsuperscript{x} is {p, r $\leftarrow$ p.}.
%Any stable model is a minimal Herbrand model, and stable sets is stable models. The stable models can be found by constructing the result of the program with respect to sets of atoms X (P\textsuperscript{x} in the following 2 steps
A stable model of P is an interpretaiton X if and only if X is the unique least Herbrand Model of ground(P)\textsuperscript{x} in the logic programs.

\subsection{Anwer Set Programming (ASP) Syntax}

Answer set of normal logic program P is a stable model, and Answer Set Programming (ASP) is a normal logic program with extensions: constrains, choice rules and optimisation. ASP program consists of a set of rules, where each rule consists of an atom and literals.A literal is either an atom \textit{p} or its default negation (negation as a failure) \textit{not p} .

A Constraint of the program P is of the form $\leftarrow$ a\textsubscript{1}, ..., a\textsubscript{n}, not b\textsubscript{1}, ..., not b\textsubscript{n}, where the rule has an empty head. The constraint filters any irrelevant answer sets. When computing ground(P)\textsubscript{x}, the empty head becomes $\perp$, which cannot be in the Answer Set.
There are two types of constraints: \textit{hard constraints} and \textit{hard constraints}. Hard constraints are strictly satisfied, whereas soft constraints are must not be satisfied but the sum of the violations should be minimised when solving ASP.

Choice rule can express possible outcomes given an action choice, which is of the form
l\{h\textsubscript{1},...,h\textsubscript{m}\}u $\leftarrow$ a\textsubscript{1}, ..., a\textsubscript{n}, not b\textsubscript{1}, ..., not b\textsubscript{n} where  l and u are integers and h\textsubscript{i} for 1 $\leq$ i $\leq$ m are atoms. The head is called \textit{aggregates}.

optimisation statement is useful to order the answer sets in terms of preference, which is of the form
\#minimize[a\textsubscript{1}=w\textsubscript{1},...a\textsubscript{n}=w\textsubscript{n}] or \#maximize[a\textsubscript{1}=w\textsubscript{1},...a\textsubscript{n}=w\textsubscript{n}] where w\textsubscript{1},..., w\textsubscript{n} is an integer weight and a\textsubscript{1},...,a\textsubscript{n} is an ground atom.  ASP solvers compute the scores of the weighted sum of the sets of ground atoms based on the true answer sets, and find optimal Answer Sets which either maximise or minimise the score.

\textit{Clingo} is one of ASP modern solvers that executes the ASP program and returns answer sets of the programs (\cite{Gebser2011}), and we will use \textit{Clingo} for the implementation of this research.

%An answer set of ASP program is interpretations that make all the ruls true.
%Non-monotonicity.
TODO ASP has true, false and unknown

\subsection{ILP Under Answer Set Semantics}

There are several ILP non-monotoic learning frameworks under the Answer set semantics . We introduce two of them: \textit{Cautious Induction} and \textit{Brave Induction} (\cite{Sakama2009}), which are foundations of \textit{Learning from Answer Sets} discussed in Section \ref{section_las}.  (for other ILP frameworks, see \cite{Otero2001}, \cite{Inoue2014}, \cite{Corapi2012} and \cite{DeRaedt1997}).
\subsubsection{Cautious Induction}
% Sakama 2008 has no concept of negative examples in this paper.
Cautious Induction task is of the form $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$, where B is the background knowledge, E\textsuperscript{+} is a set of positive examples and E\textsuperscript{-} is a set of negative examples.

 H $\in$ ILP\textsubscript{cautious} $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ if and only if  there is at least one answer set A of B $\cup$ H (B $\cup$ H is satisfiable) such that for every anser set A of B $\cup$ H: \\
$\forall$ e $\in$ E\textsuperscript{+} : e $\in$ A \\
$\forall$ e $\in$ E\textsuperscript{-} : e $\notin$ A \\

\begin{examp} \normalfont Cautious Induction (Example)

B = $\begin{cases}
	exercises  \leftarrow not \ eat\_out. \\
	eat\_out \leftarrow exercises. \\
      \end{cases}$ \\
E\textsubscript{+} = tennis \\
E\textsubscript{-} = eat\_out \\
One possible  H $\in$ ILP\textsubscript{cautious} is \{tennis$ \leftarrow$ exercises, $\leftarrow$ not tennis \}.
\end{examp}
\label{cautious_induction_example}

The limitation of Cautious Induction is that positive examples must be true for all Answer Sets and negative examples must not be included in any of the Answer Sets. These conditions may be too strict in some cases, and Cautious Induction is not able to accept the case where positive examples are true in some of the Answer Sets but not all Answer Sets of the program.

\begin{examp} \normalfont Limitation of Cautious Induction (Example)

1\{situation(P, awake), situation(P, sleep)\}1 :- person(P).

person(john).

\end{examp}
\label{limitation_cautious}

In the example \ref{limitation_cautious}, neither of situation(john, awake) or situation(john, sleep) is false in all answer sets. In this example, it only return person(john). Thus no examples could be given to learn the choice rule.

\subsubsection{Brave Induction}
Brave Induction task is of the form $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ where, B is the background knowledge, E\textsuperscript{+} is a set of positive examples and E\textsuperscript{-} is a set of negative examples.
 H $\in$ ILP\textsubscript{brave} $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ if and only if there is at least one answer set A of B $\cup$ H such that: \\
$\forall$ e $\in$ E\textsuperscript{+} : e $\in$ A \\
$\forall$ e $\in$ E\textsuperscript{-} : e $\notin$ A \\

\begin{examp} \normalfont Brave Induction (Example)

B = $\begin{cases}
	exercises  \leftarrow not \ eat\_out. \\
	tennis \leftarrow holiday \\
      \end{cases}$ \\
E\textsubscript{+} = tennis \\
E\textsubscript{-} = eat\_out \\
One possible  H $\in$ ILP\textsubscript{brave} is \{tennis\}, which returns \{tennis, holidy, exercises\} as Answer Sets.
\end{examp}
\label{brave_induction_example}

The limitation of Brave indction that it cannot learn constraints as shown in the example \ref{limitation_brave}.

TODO MORE EXPLANATION

\begin{examp} \normalfont Limitation of Brave Induction (Example)

B = $\begin{cases}
	1\{situation(P, awake), situation(P, sleep)\}1 \leftarrow person(P). \\
	person(C) \leftarrow super\_person(C). \\
	super\_person(john).
	\end{cases}$

In order to learn the  constraint hypothesis H = \{ $\leftarrow$ not situation(P, awake), super\_person(P)\}, it is not possible to find an optimal solution.
\end{examp}
\label{limitation_brave}

\subsection{Inductive Learning of Answer Set Programs (ILASP)}
\label{section_lasp}

\subsubsection{Learning from Answer Sets (LAS)}
\textit{Learning from Answer Sets (LAS)} was developed in \cite{Law2014} to faciliate more complex learning task that neither Cautious Induction nor Brave Induction could learn.
Examples used in LAS are \textit{Partial Interpretations}, which are of the form $\langle$ e\textsuperscript{inc}, e\textsuperscript{exc}$\rangle$. (called \textit{inclusions} and \textit{exclusions} of e respectively).  A Herbrand Interpretations extends a partial interpretation if it include all of e\textsuperscript{inc} and none of e\textsuperscript{exc}.

LAS is of the form $\langle$ B, S\textsubscript{M}, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$, where B is background knowledge, S\textsubscript{M} is hypothesis space, and E\textsuperscript{+} and E\textsuperscript{-} are examples of positive and negative partial interpretations. S\textsubscript{M} consists of a set of normal rules, choice rules and constraints. S\textsubscript{M} is specified by \textit{language bias} of the learning task using \textit{mode declaration}. Mode declaration, and specifies what can occur in an hypothesis by specifying the predicate, and has two parts: \textit{modeh} and \textit{modeb}.  modeh and modeb are the predicates that can occur in the head of the rule and body of the rule respectively. Language bias is the specification of the language in the hypothesis in order to reduce search space for the hypothesis.

Given a learning task T, the set of all possible inductive solutions of T is denoted as ILP\textsubscript{LAS}(T), and a hypothesis H is an inductive solution of ILP\textsubscript{LAS}(T) $\langle$ B, S\textsubscript{M}, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ such that
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M}
\item $\forall$ e $\in$ E\textsuperscript{+} : $\exists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e
\item $\forall$ e $\in$ E\textsuperscript{-} : $\nexists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e
\end{enumerate}

%S\textsubscript{M} consists of all rules given the language bias.

\begin{examp} \normalfont LAS (Example)

TODO
%B = $\begin{cases}
%	exercises  \leftarrow not \ eat\_out. \\
%	tennis \leftarrow holiday \\
%      \end{cases}$ \\
%E\textsubscript{+} = tennis \\
%E\textsubscript{-} = eat\_out \\
%One possible  H $\in$ ILP\textsubscript{brave} is \{tennis\}, which returns \{tennis, holidy, exercises\} as Answer Sets.

\end{examp}
\label{las_example}

% Limitation of LAS??

\subsubsection{Inductive Learning of Answer Set Programs (ILASP)}

ILASP is an algorithm that is capable of solving LAS tasks, and is based on two fundamental concepts: positive solutions and violating solutions.

A hypothesis H is a positive solution if and only if
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M}
\item $\forall$ e\textsuperscript{+} $\in$ $\exists$ A $\in$ AS(B $\cup$ H) such that A extends e\textsuperscript{+}
\end{enumerate}
A hypothesis H is a violating solution if and only if
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M}
\item $\forall$ e\textsuperscript{+} $\in$ E\textsuperscript{+} $\exists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e\textsuperscript{+}
\item $\exists$ e\textsuperscript{-} $\in$ E\textsuperscript{-} $\exists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e\textsuperscript{-}\\
\end{enumerate}

Given both definitions of positive and violating solutions, ILP\textsubscript{LAS} is positive solutions that are not violating solutions.

\subsubsection{Learning from Ordered Answer Sets (LOAS)}

LOAS is an extension of ILASP by incorporating learning capability of weak constraints, is useful for modelling the agent's preferences (\cite{Law2015}).
Examples used for learning are ordered pairs of partial answer sets, which are of the form o = $\langle$ e\textsubscript{1},e\textsubscript{2} $\rangle$.  An ASP Program P \textit{bravely respects o } if and only if
\begin{enumerate}
\item $\exists$ A\textsubscript{1}, A\textsubscript{2} $\in$ Answer Sets(P) such that A\textsubscript{1} extends e\textsubscript{1}, A\textsubscript{2} extends e\textsubscript{2}, and A\textsubscript{1} $\succ$\textsubscript{P} A\textsubscript{1}
\end{enumerate}

P \textit{cautiously respects o } if and only if
\begin{enumerate}
\item $\forall$ A\textsubscript{1}, A\textsubscript{2} $\in$ Answer Sets(P) such that A\textsubscript{1} extends e\textsubscript{1}, A\textsubscript{2} extends e\textsubscript{2}, and A\textsubscript{1} $\succ$\textsubscript{P} A\textsubscript{1}
\end{enumerate}

LOAS task is of the form T = $\langle$ B, S\textsubscript{M}, E\textsubscript{+}, E\textsubscript{-}, O\textsuperscript{b},O\textsuperscript{c}$\rangle$ where O\textsuperscript{b} and O\textsuperscript{c} are brave and cautious orderings respectively, which are sets of ordering examples over set of positive partial interpretations E\textsuperscript{+}.
A hypothesis H is an inductive solution of T if and only if 
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M} in ILP\textsubscript{LOAS}(T)
\item H' $\in$ ILP\textsubscript{LAS}($\langle$ B, S\textsubscript{LAS}(M\textsubscript{h}, M\textsubscript{b}), E\textsuperscript{+}, E\textsuperscript{-}$\rangle$) where H' is the subset of H without weak constraints
\item $\forall$o $\in$ O\textsuperscript{b} B $\cup$ H bravely respects o
\item $\forall$o $\in$ O\textsuperscript{c} B $\cup$ H cautiously respects o
\end{enumerate}

\subsubsection{A Context-dependent Learning from Ordered Answer Sets }
\textit{Context-dependent learning from ordered answer sets ($ILP_{LOAS}^{context}$)} is a further generalisation of ILP\textsubscript{LOAS} with \textit{context-dependent examples}. 
Context-dependent examples are examples to which each unique background knowledge (context) only applies. This way the background knowledge is more structured rather than one fixed background knowledge that are applied to all examples. 
Formally, partial interpretation is of the form $\langle$ e, C $\rangle$ (called \textit{context-dependent partial interpretation (CDPI)}), where \textit{e} is a partial interpretation and C is called \textit{context}, or an ASP program without weak constraints.
A \textit{context-dependent ordering example (CDOE)} is of the form $\langle$ $\langle$e\textsubscript{1}, C\textsubscript{1} $\rangle$, $\langle$ e\textsubscript{2}, C\textsubscript{2} $\rangle$$\rangle$. An APS program P \textit{bravely} respects \textit{o} if and only if 
\begin{enumerate}
 \item $\exists$ $\langle$ A\textsubscript{1}, A\textsubscript{2} $\rangle$ such that A\textsubscript{1} $\in$ Answer Sets(P $\cup$ C\textsubscript{1}),  A\textsubscript{2}, $\in$ Answer Sets(P $\cup$ C\textsubscript{2}), A\textsubscript{1} extends e\textsubscript{1}, A\textsubscript{2} extends e\textsubscript{2} and A\textsubscript{1} $\prec$\textsubscript{P} A\textsubscript{2}
\end{enumerate}

 Similarly, an APS program P \textit{cautiously} respects \textit{o} if and only if 
\begin{enumerate}
 \item $\forall$ $\langle$ A\textsubscript{1}, A\textsubscript{2} $\rangle$ such that A\textsubscript{1} $\in$ Answer Sets(P $\cup$ C\textsubscript{1}),  A\textsubscript{2}, $\in$ Answer Sets(P $\cup$ C\textsubscript{2}), A\textsubscript{1} extends e\textsubscript{1}, A\textsubscript{2} extends e\textsubscript{2} and A\textsubscript{1} $\prec$\textsubscript{P} A\textsubscript{2}
\end{enumerate}

$ILP_{LOAS}^{context}$ task is of the form T = $\langle$ B, S\textsubscript{M}, E\textsubscript{+}, E\textsubscript{-}, O\textsuperscript{b},O\textsuperscript{c}$\rangle$ where O\textsuperscript{b} and O\textsuperscript{c} are brave and cautious orderings respectively, which are sets of ordering examples over set of positive partial interpretations E\textsuperscript{+}.
A hypothesis H is an inductive solution of T if and only if 
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M} in $ILP_{LOAS}^{context}$
\item $\forall$$\langle$ e, C$\rangle$ $\in$ E\textsuperscript{+}, $\exists$A $\exists$ A $\in$ Answer Sets (B $\cup$ C $\cup$ H) such that A extends e 
\item $\forall$$\langle$ e, C$\rangle$ $\in$ E\textsuperscript{-}, $\nexists$A $\exists$ A $\in$ Answer Sets (B $\cup$ C $\cup$ H) such that A extends e 
\item $\forall$o $\in$ O\textsuperscript{b} B $\cup$ H bravely respects o
\item $\forall$o $\in$ O\textsuperscript{c} B $\cup$ H cautiously respects o
\end{enumerate}

Two advantages of adding contex-dependent are it increases the efficiency of learning tasks, and more expressive structure of the background knowlege to particular examples.

\section{Reinforcement Learning (RL)}
\label{rl}
Reinforcement learning (RL) is another subfield of mathine learning regarding an agent's behaviour in an environment in order to maximise the total reward. As shown in Figure \ref{agent_env}, the agent interacts with an envirnment, and at each time step, an agent takes an action and receive observation, which affects the environment state and the reward (or penality) it receives by the action outcome. In this section, we briefly introduce necessary background in RL for our research. 

\begin{figure}[!htb]
\centering
\includegraphics[width=6cm, height=6cm]{./figures/agent_env}
\caption{Agent and Environment}
\label{agent_env}
\end{figure}

% An RL agent may include either Policy, Value function or Model,

\subsection{Markov Decision Process (MDP)}
An agent interacts with an environment at a sequence of discrete time step, which is a part of the sequential history of observations, actions and rewards. The sequential history is formalised as H\textsubscript{t} = O\textsubscript{1}, R\textsubscript{1}, A\textsubscript{1}, ..., A\textsubscript{t-1}, O\textsubscript{t}, R\textsubscript{t}.  A \textit{state} is a function of the history S\textsubscript{t} = f(H\textsubscript{t}), which determines the next environment.  A state S\textsubscript{t} is said to have Markov property if and only if
P[S\textsubscript{t+1} $\vert$ S\textsubscript{t}] = P[S\textsubscript{t+1} $\vert$ S\textsubscript{1}, ..., S\textsubscript{t}]. In other words, the probability of reaching S\textsubscript{t+1} depends only on S\textsubscript{t}, which captures all the relevant information from earilier history (\cite{Puterman1994}).

When an agent must make a sequence of decision, the sequential decision problem can be formalised using Markov decision process (MDP). MDPs formaly represent a fully observable environment of an agent for RL.

A MDP is of the form $\langle$ S, A, T\textsubscript{a}, R\textsubscript{a}, $\gamma$ $\rangle$ where:

\begin{itemize}
\item S is the set of finite states that is observable in the environment
\item A is the set of finite actions taken by the agent
\item T\textsubscript{a}(s, s$^\prime$) is a state transition in the form of probability matrix Pr(S\textsubscript{t+1} = s$^\prime$ $\vert$ s\textsubscript{t} = s, a\textsubscript{t} = a), which is the probablity that action a in state s at time t will result in state s$^\prime$ at time t+1.
\item R is a reward function R\textsubscript{a}(s, s$^\prime$) = $\displaystyle \E[R\textsubscript{t+1} $ $\vert$ S\textsubscript{t} = s, A\textsubscript{t} = a], the expected immediate reward that action a in state s at time t will return
\item $\gamma$ is a discount factor $\gamma$ $\in$ [0,1], which represents the preference of the agent for present rewards over future rewards
\end{itemize}

%$\displaystyle \E[R\textsubscript{t+1} \vert$ S\textsubscript{t} = s]
\subsection{Policies and Value Functions}
\textit{value functions} estimate the expected return, or expected future rewarded,  for a given action in a given state. The expected reward for an agent is dependent on the agent's action. A solution to the sequential decision problem is called a \textit{policy $\pi$}, a sequence of actions that leads to a solution.
% which is a distributin over actions given states.

The state value function v\textsubscript{$\pi$}(s) of an MDP under a policy $\pi$ is th the expected return starting from state , which is of the form: 

v\textsubscript{$\pi$}(s) = $\displaystyle \E$[G\textsubscript{t} $\vert$ S\textsubscript{t} = s]

where G\textsubscript{t} = R\textsubscript{t+1} + $\gamma$R\textsubscript{t+2} + ... (the total discounted reward from t). 

A policy $\pi$ is optimal if it maximises the action-value function
The transition and reward functions are not necessary to be known to compute $\pi$.

An optimal policy $\pi^*$ is the one that maximise the total rewards in the environment.

Reinforcement learning is a method to get approximated optimal solution.

TODO on-policy and off-policy learning.

A\textsubscript{t} = $\pi$(S\textsubscript{t})

One of the common possibilities is that the agent chooses an action in order to maximise the discounted sum of future rewards.

A\textsubscript{t} to maximise R\textsubscript{t+1} + $\gamma$ R\textsubscript{t+2} + $\gamma^2$ R\textsubscript{t+3} + ...

An action-value function evaluate a particular state by taking an action according the policy

q\textsubscript{$\pi$} = $\displaystyle \E[R\textsubscript{t+1} $[R\textsubscript{t+1} + $\gamma$ R\textsubscript{t+2} + $\gamma^2$ R\textsubscript{t+3} + ... $\vert$ S\textsubscript{t} = s, A\textsubscript{t} = a, A\textsubscript{t+1 } = a,]

\subsection{Model-based vs Model-free Learning}

A model M is a representation of an MDP where the state space and action space are assumed to be known. The model represents state transitions and rewards. In this case, the problem to be solved becomes a planning problem. A planning is for a series of actions to achive the agent's goal.
Most of the reinforcement learning problems are model-free learning, where M is not unknown and the agnet learns to achieve the goal by sorely interacticting with the environment. Thus the agent knows only possible states and actions, and the transition state and reward probability functions are unknown.

The performance of model-based RL is limited to optimal policy given the model M. Thus when the model is not a representation of the true MDP, the planning algorithms will not lead to the optimal policy, but suboptimal policy. The solution to this problem is either to use model-free approach when the model is inaccurate or incorporate undertainty of the model using XXX.

One algorithm which combine both aspects of model learning to solve the issue of sub-optimality is called Dyna (\cite{Sutton1990}), which is shown in Figure \ref{dyna}.

\begin{figure}[!htb]
\centering
\includegraphics[width=8cm, height=6cm]{./figures/dyna}
\caption{Relationships among learning, planning and acting \ref{Montague1999}}
\label{dyna}
\end{figure}

Dyna learns a model from real experience and use the model to generate simulated experience to update the evaluation functions.

This approach is more effective because the simulated experience is relatively easy to generate compared to having to have real experience, thus less iterations.

TODO ADD MATH

\subsection{Temporal-Difference (TD) Learning}

To solve MDP, one of the approaches is called Temporal-Difference (TD) Learning.

TD learns directly from episodes of experiences, which can be imcomplete.
TD does not require knowledge of MDP transitions and rewards (model-free)
%Sutton 1988

Update value

%\begin{equation}
%\textbf{	V(S\textsubscript{t} \leftarrow V(S\textsubscript{t} + \alpha (R\textsubscript{t+1} + \gamma V(S\textsubscript{t+1}) - V(S\textsubscript{t}))
%}\end{equation}

where R\textsubscript{t+1} + $\gamma$ V(S\textsubscript{t+1}) is the estimated return (a.k.a TD target)

R\textsubscript{t+1} + $\gamma$ V(S\textsubscript{t+1}) - V(S\textsubscript{t}) is TD

TD updates the estimate by using the estimates of XXX (bootstrap).

The advantages of TD methods
- does not require any model of an environment.
- online learning

\subsection{Q-Learning}

Q-learning is off-policy TD learning defined in \cite{Watkins}, where the agent will not rely on the models of the environment (model-free leaning) and only knows about the possible states and actionns. The transition states and reward probability functions are unknown to the agent.
It is of the form:

%\begin{equation}
Q(s\textsubscript{t},a\textsubscript{t}) $\leftarrow$ Q(s\textsubscript{t},a\textsubscript{t})
%Q(s\textsubscript{t},a\textsubscript{t}) $\leftarrow$ Q(s\textsubscript{t},a\textsubscript{t}) + \alpha(R\textsubscript{t+1} + $\gamma$ max (a+t) Q(s\textsubscript{t+1}, a\textsubscript{t+1}) - Q(s\textsubscript{t}, a\textsubscript{t}))
%\end{equation}
where $\alpha$ is the learning rate, $\gamma$ is a discount rate between 0 and 1. The equation is used to update the state-action value function called Q function. The function Q(S,A) predicts the best action A in state S to maximise the total cumulative rewards.
The optimal Q-function Q\textsuperscript{*}(s,a) represents XXX for the agent to selection action a given that it is in state s.

% Model free can be done using Monte Carlo Policy evaluation
% One way to solve the Bellman Optimality equation is Q-leraning
% U(s) = max a Q(s,a)
% The function is estimated by Q-learning, which repeately updates Q(s,a) using the Bellman Equation.

Epsilon greedy

\chapter{Related Work}
\label{related_work}

 \ref{Garnelo2016} introduced Deep Symbolic Reinforcement Learning (DSRL), a proof of concept for incorporating symbolic front end as a means of converting low-dimensional symbolic representation into spatio-temporal representations, which will be the state transitions input of reinforcement learning.
EXPLAIN MORE

WEAKNESS

just a symbolic representations and not learning

segmentation was done manually.

Also using deep neural network front-end might also cause a problem \cite{Su2017}. As demonstrated in XXX, a single pixel can dramatically influence the policy (Maybe not so important here??).


There are a number of further research to their DSRL. \ref{Garcez2018} further improved the symbolic abstraction phase by incorporating relative position of each object with respect to every other object rather than absolute object position. They also assign priority to each Q-value function based on the relative distance of objects from an agent. 

%The use of symbolic representations to achieve data-efficient learning was traditionally discussed in relational reinforcement learnign (RLL).

TODO "Relational Recurrent Neural Networks" \cite{Santoro2018}

TODO "Relational Deep Reinforcement Learning" \cite{Zambaldi2018}

%Transparency and interpretable capability of the model is another important aspect for machine learning applications.

%XXX[Programmati...] developed a programmatically interpretable reinforcement learning which finds a policy that can be represented in a human-readable programming language.
%
%Two most studied approach for using previous learning exprience is meta-learning and transfer learning

%The history of data-efficient learning

%Study of symbolic machine learning roots from Relational Reinforcement Learning


Another approach for using representation is using heuristics in \cite{Apeldoorn2017}. 

There are several studies attempting to combine ASP and RL. \cite{Ferreira2017a} (this is implementation)

The original idea of ASP + RL was in \cite{Ferreira2017a}. 

By contract, there is active reserach in symbolic machine learning, which focuses on logic-based learning rather than statistical machine learning.
For example, XX shows the agent can learn XXX from a noisy examples, with only a very few training examples.

Incorporation of logic into reinforcement learning dates back to the study of relational reinforcement learning,


More recently there has been a number of attempts to incorporate ASP into reinforcement learning.

There are a number of reseached conducted in applying DNN to symbolic reasoning.
For example,

[From GamePlay to Symbolic Reasoning]


\chapter{Project Overview}
\label{project_overview}

\section{Proposed Architecture}
\label{proposed_architecture_section}

The proposed tentative architecture is shown in Figure \ref{proposed_architecture}. The overall architecture resembles Sutton's Dyna architecture (\cite{Sutton1990}), which combined both model-free learning and model-based learning.

\begin{figure}[!htb]
\centering
\includegraphics[width=15cm, height=9cm]{./figures/ILASRL}
\caption{Proposed reinforcement learning architecture. ILASPRL learns to generate a model and updates based on the interaction with the environment, which is used to facilite the policy evaluation. }
\label{proposed_architecture}
\end{figure}

% TODO What would you learn in my context? Relashinship of the objects?
% Objects, types, locations and interactions.

TODO ADD GAME SCENARIOS

\subsection{ASP Representation Generation}

The inputs from the experience needs to be converted in ASP form, which can be used to proceed the inductive learning in ILASP(RL).
Observations from the interaction with the environment are state transition, reward and action of the agent, which can be directly converted using XXX. Example \ref{ilasprl_input} is ASP input form that is feeded into ILASP(RL) phase.

\begin{examp} \normalfont (Examples for ILASPRL (input))

agent\_before(S1, T1).

agent\_after(S1, T1).

reward(R).

action(A).

\end{examp}
\label{ilasprl_input}

\subsection{Model Generation and Update using ILASP(RL)}

Once the input is converted, the agent must learn the following definition of the model of the environment.

%\begin{examp} \normalfont LAS (Example)
%
%\end{examp}
%\label{las_example}

\begin{examp} \normalfont(Example of learning task (output))

% valid\_move(C1, T):- \\
%   adjacent(C1, C2), \\
%   agent\_at(C2, T), \\
%   not obstacle(C1, C2), \\
%   not enemy(C1, C2). \\
% \\
% valid\_move(C1, T):- \\
%   link(C2, C1), \\
%   agent\_at(C2, T). \\
valid\_move(C0, C1):- previous(C0, C1).
\\
previous(C2, C1):-

agent\_at(C0, C1),
adjacent(C0, C2),
not obstacle(C0, C2),
not enemy(C0, C2).
\\
state\_transition(S1, S2, T2):-
  agent\_before(S1, T1),
  agent\_after(S2, T2).

\end{examp}
\label{learning_task}

The background knowldge is empty, and each example contain different transition of the agent.

The model is updated as the agent explore more environment.

In addition, when the environment has been changed during the exploration, the agent creates a new model for this new environment.


In two different but similar scenarios, the model is generalised based on the two models, and the agent's model is more general which covers both games. Thus in theory, the agent should still be able to exploit the efficient learning from the model-based learning, facilitating the transfer learning.


Example: model refinement in a simple Grid world


Implement baseline performance (DQN, Q-learning, ASPRL)

Due to the complex environment of the chosen game, I will not implemnet the original DSRL, since the feature extractions from the pixel would only work in a very simple pixel environment,

Pipeline

The basic architecture follows the similar method in XXX, but I also add symbolic learning to these extracted symbolic features using ILASP.

Apply ILASP to the ASP, which involves development of the pipeline of ILASP in Python


\subsection{Generation of Simulated Experience using the Model}


Finally use Q-learning that allow

which measurement would you use? (grid word, something else? GVGAL games)
Summarise different types of knowledge representations (Objects ?? relationship?)

Common sense

Lastly, I will also test the capability of transfer learning for this new method.

\section{Project Outline}

The next phase of the research is further research on the proposed architecture and implementation and experiments. We will implement the proposed architecture using Python and ILASPv3.1.0 (Clingo) \cite{Law2017}, and compare the performance.

Investigations of whether ILASP can learn Adjacent is crucial for our proposed architecture. 
Other alternative is \textit{Predictive Representations of State}. 

The platform we are planning to use to measure the performance of my approach will be GVG-AI Framework, which was created for the General Video Game AI Competition \footnote{http://www.gvgai.net/}, game environments for an agent that should be able to play a wide variety of games without knowing which games are to be played.
The underline language is the Video Game Definition Language (VGDL), which is a high-level description language for 2D video games providing a platform for computational intelligence research (\cite{Schaul2013}).

The two main measurements for the performance of our new architecture are learning efficiency and transfer Learning capability. 


The proposed architecture is not finalised and will be reviewed regularly as I proceed the development of the architecture.

% \section{Contribution}
%
% To my knowledge, this is the first time that both symbolic learning method is incorporated into a reinforcement learning to facilitate learning process
%
% \section{Methods}


\chapter{Ethics Checklist}
\label{ethics_checklist}

{
\renewcommand*{\arraystretch}{1.3}
\begin{longtable}{ |p{13.2cm}|p{0.6cm}|p{0.6cm}| }
\hline
 & \bf Yes & \bf No \\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 1: HUMAN EMBRYOS/FOETUSES} \\
\hline

Does your project involve Human Embryonic Stem Cells? & & \checkmark\\
\hline

Does your project involve the use of human embryos? & & \checkmark\\
\hline

Does your project involve the use of human foetal tissues / cells? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 2: HUMANS} \\
\hline

Does your project involve human participants? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 3: HUMAN CELLS / TISSUES} \\
\hline

Does your project involve human cells or tissues? (Other than from “Human Embryos/Foetuses” i.e. Section 1)? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 4: PROTECTION OF PERSONAL DATA} \\
\hline

Does your project involve personal data collection and/or processing? & & \checkmark\\
\hline

Does it involve the collection and/or processing of sensitive personal data (e.g. health, sexual lifestyle, ethnicity, political opinion, religious or philosophical conviction)? & & \checkmark\\
\hline

Does it involve processing of genetic information? & & \checkmark\\
\hline

Does it involve tracking or observation of participants? It should be noted that this issue is not limited to surveillance or localization data. It also applies to Wan data such as IP address, MACs, cookies etc. & & \checkmark\\
\hline

Does your project involve further processing of previously collected personal data (secondary use)? For example Does your project involve merging existing data sets? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 5: ANIMALS} \\
\hline

Does your project involve animals? & & \checkmark\\
\hline


\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 6: DEVELOPING COUNTRIES} \\
\hline

Does your project involve developing countries? & & \checkmark\\
\hline

If your project involves low and/or lower-middle income countries, are any benefit-sharing actions planned? & & \checkmark\\
\hline

Could the situation in the country put the individuals taking part in the project at risk? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 7: ENVIRONMENTAL PROTECTION AND SAFETY} \\
\hline

Does your project involve the use of elements that may cause harm to the environment, animals or plants? & & \checkmark\\
\hline

Does your project deal with endangered fauna and/or flora /protected areas? & & \checkmark \\
\hline

Does your project involve the use of elements that may cause harm to humans, including project staff? & & \checkmark\\
\hline

Does your project involve other harmful materials or equipment, e.g. high-powered laser systems? & & \checkmark\\
\hline


\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 8: DUAL USE} \\
\hline

Does your project have the potential for military applications? & & \checkmark\\
\hline

Does your project have an exclusive civilian application focus? & & \checkmark\\
\hline

Will your project use or produce goods or information that will require export licenses in accordance with legislation on dual use items? & & \checkmark\\
\hline

Does your project affect current standards in military ethics – e.g., global ban on weapons of mass destruction, issues of proportionality, discrimination of combatants and accountability in drone and autonomous robotics developments, incendiary or laser weapons? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 9: MISUSE} \\
\hline

Does your project have the potential for malevolent/criminal/terrorist abuse? & & \checkmark\\
\hline

Does your project involve information on/or the use of biological-, chemical-, nuclear/radiological-security sensitive materials and explosives, and means of their delivery? & & \checkmark\\
\hline

Does your project involve the development of technologies or the creation of information that could have severe negative impacts on human rights standards (e.g. privacy, stigmatization, discrimination), if misapplied? & \checkmark& \\
\hline

Does your project have the potential for terrorist or criminal abuse e.g. infrastructural vulnerability studies, cybersecurity related project? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 10: LEGAL ISSUES} \\
\hline

Will your project use or produce software for which there are copyright licensing implications? & \checkmark& \\
\hline

Will your project use or produce goods or information for which there are data protection, or other legal implications? & & \checkmark\\
\hline

\multicolumn{3}{|l|}{\cellcolor{green!25}\bf Section 11: OTHER ETHICS ISSUES} \\
\hline

Are there any other ethics issues that should be taken into consideration? & & \checkmark \\
\hline

\end{longtable}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Contribution}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Experimental Results}
%
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Conclusion}

% Further research
% probabilistic inductive logic programming instead of ASP.

%% bibliography
\bibliographystyle{plain}
\bibliography{references}

\end{document}
