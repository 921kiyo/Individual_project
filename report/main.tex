\documentclass[12pt,twoside]{report}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage[table]{xcolor}
\usepackage[toc,page]{appendix}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{amsthm}
\usepackage{subdepth}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[chapter] % Reset theorem numbering  for each chapter

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition} % Definition numbers are dependent on theorem numbers
\newtheorem{exmp}[thm]{Example} % same for example numbers

% Ctrl + Alt + B to compile in Atom

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}
\DeclareMathOperator{\E}{\mathbb{E}}

\newcommand{\reporttitle}{Symbolic Reinforcement Learning using Inductive Logic Programming}
\newcommand{\reportauthor}{Kiyohito Kunii}
\newcommand{\supervisor}{Prof. Alessandra Russo \\ Mark Law \\  Ruben L Vereecken}
\newcommand{\degreetype}{MSc in Computing Science}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

\date{June 2018}


% \newtheoremstyle{definition}[section]
\newtheorem{examp}{example}[section]

\begin{document}

% load title page
\input{titlepage}

% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{abstract}

Reinforcement Learning (RL) is a field of machine learning techniques that has been applied and proven to be successful in many domains. One of the recent research has been focused around incorporating symbolic representation into RL to achieve data efficient and more transparent learning. Inductive logic programming (ILP) is another field of machine learning that is based on logic programming, and recent advance on ILP research have shown potential in many more applications. 
This paper examines a proof of concept called ILP(RL), which attempts to apply one of the ILP frameworks called Learning from Answer Sets, into RL senarios to complement some of the shortcoming of RL. We created a new pipeline using ILASP and proposed a new way of learning the model of the environment. The new pipeline was examined in a various simple maze games, and show that an agent learns faster than existing RL techniques. We also show that transfer learning successfully improve learning on a new but similar environment in a limited senarios. 

This proof of concept showpotentials for this new way of learning using ILP. 

 \end{abstract}
%
\cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}

I would like to thank Prof. Alessandra Russo for accepting to supervise my project, her enthusiasm for my work and invaluable guidance throughout. 

I would also like to thank Mark Law for his expertise on inductive logic programming and fruitful discussions, and for Ruben Verrecken for his expertise on reinforcement learning and for providng me with advice and assistance for technical implementation.

\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents

% ADD BLANK PAGE
\clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\listoffigures
\listoftables
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IMPERIAL LOGO
% \begin{figure}[tb]
% \centering
% \includegraphics[width = 0.4\hsize]{./figures/imperial}
% \caption{Imperial College Logo. It's nice blue, and the font is quite stylish. But you can choose a different one if you don't like it.}
% \label{fig:logo}
% \end{figure}
% Figure~\ref{fig:logo} is an example of a figure.

\chapter{Introduction}
\label{introduction}

% Including this part of chapter
\input{chapters/introduction}

\chapter{Background}
\label{background}

This chapter introduces necessary background of Inductive Logic Programming (Section \ref{ilp}) and Reinforcement Learning (Section \ref{rl}), which provide the foundations of our research.

\section{Inductive Logic Programming (ILP)}
\label{ilp}

\textit{Inductive Logic Programming (ILP)} is a subfield of machine learning research area aimed at the intersection between machine learning and logic programming \cite{Muggleton1991}. The purpose of ILP is to inductively derive a hypothesis H that is a solution of a learning task, which coveres all positive examples and none of negative examples, given a hypothesis language for search space and cover relation \cite{DeRaedt1997}. ILP is based on learning from entailment, as shown in Equation \ref{ilp_equation}.

\begin{equation}
B \wedge H \models E
\end{equation}
\label{ilp_equation}

where E contains all of the positive examples (E\textsuperscript{+}) and none of the negative examples (E\textsuperscript{-}).
One of the advantage of ILP over statistical machine learning is that the hypothesis that an agent learnt can be easily understood by a human, as it is expressed in first-order logic, making the learning process more transparent rather than black-box.
One of the limitations of ILP is learning efficiency and scalability. There are usually thousands or more examples in many real-world examples. Scaling ILP task to cope with large examples is a challenging task \cite{Muggleton1993}.

In this section, we briefly introduce foundation of Answer Set Programming (ASP) and inductive learning frameworks.

\subsection{Stable Model Semantics}

Having defined the syntax of clausal logic, we now introduce its semantics under the context of Stable Model. The semantics of the logic is based on the notion of interpretation, which is defined under a \textit{domain}. A domain contains all the objects that exist. In logic, it is convention to use a special interpretations called \textit{Herbrand interpretations} rather than general interpretations.

\begin{defn}
\textit{Herbrand Domain} (a.k.a \textit{Herbrand Universe}) of clause sets \textit{Th} is the set of all ground terms that are constants and function symbols appeared in \textit{Th}.
\end{defn}

\begin{defn}
\textit{Herbrand Base} of \textit{Th} is the set of all ground predicates that are formed by predicate symbols in \textit{Th} and terms in the Herbrand Domain.
\end{defn}

\begin{defn}
\textit{Herbrand Interpretation} of a set of definite clauses \textit{Th} is a subset of the Herbrand base of \textit{Th}, which is a set of ground atoms that are true in terms of interpretation.
\end{defn}

\begin{defn}
\textit{Herbrand Model} is a Herbrand interpretation if and only if a set \textit{Th} of clauses is satisfiable. In other words, the set of clauses \textit{Th} is unsatisfiable if no Herbrand model was found.
\end{defn}

\begin{defn}
\textit{Least Herbrand Model} (denoted as \textit{M(P)}) is an unique minimal Herbrand model for definite logic programs.  The Herbrand Model is a minimum Herbrand model if and only if none of its subsets is an Herbrand model.
\end{defn}
For normal logic programs, there may not be any least Herbrand Model.

%Interpretation evaluate it to true
%Interpretation evaluate it to false

\begin{examp} \normalfont (Herbrand Interpretation, Herbrand Model and M(P)) \\

P = $\begin{cases}
	p(X)  \leftarrow q(X) \\
	q (a).
      \end{cases}$
HD = \{ a \} , HB = \{ q(a), p(a) \}  \\

where HD is Herbrand Domain and HB is Herbrand Base.
Given above,  there are four Herbrand Interpretations = $\langle$ \{q(a)\}, \{p(a)\}, \{q(a), p(a)\}, \{\} $\rangle$, and one Herbrand Model (as well as M(P)) = \{q(a), p(a)\}

\end{examp}
% TODO: Use the same or similar examples for all of them.

\textit{Definite Logic Program} is a set of definite rules, and  a \textit{definite rule} is of the form \textit{h} $\leftarrow$ \textit{a}\textsubscript{1}, ..., \textit{a}\textsubscript{n}.  \textit{h} and  \textit{a}\textsubscript{1}, ..., \textit{a}\textsubscript{n} are all atoms. \textit{h} is the \textit{head} of the rule and \textit{a}\textsubscript{1}, ..., \textit{a}\textsubscript{n} are the \textit{body} of the rule.
\textit{Normal Logic Program} is a set of normal rules, and a normal rule is of the form \textit{h} $\leftarrow$ a\textsubscript{1}, ..., \textit{a}\textsubscript{n}, \textit{not b}\textsubscript{1}, ..., \textit{not  b}\textsubscript{n} where \textit{h} is the head of the rule,
 and \textit{a}\textsubscript{1}, ..., \textit{a}\textsubscript{n}, \textit{b}\textsubscript{1}, ..., \textit{b}\textsubscript{n} are the body of the rule (both the head and body are all atoms).

To solve a normal logic program \textit{Th}, the program P needs to be grounded. The \textit{grounding} of \textit{Th} is the set of all clauses that are c $\in$ \textit{Th} and variables are replaced by terms in the Herbrand Domain. 
\begin{defn}
The algorithm of grounding starts with an empty program Q = \{  \} and the relevant grounding is constructed by adding to each rule R to Q such that
\begin{itemize}
\item R is a ground instance of a rule in P.
\item Their positive body literals already occurs in the in the of rules in Q. 
\end{itemize}
The algorithm terminates when no more rules can be added to Q.

\end{defn}

\begin{examp} \normalfont Grounding \\

P = $\begin{cases}
%	p(X)  \leftarrow not \ q(X). \\
	q(X)  \leftarrow p(X). \\
	p(a).
      \end{cases}$ \\

ground(P) in this example is \{p(a), q(a)\}.

\end{examp}
\label{grounding}

%TODO Explain grounding in ASP context.
%The grounding of a normal logic program P can be obtained by replacing each rule in P with a ground instance of the rule, such that for each atom A in body\textsuperscript{+} (R) (TODO EXPLAIN WHAT THIS IS), already occurs in the head of another ground rule.
Not only the entire program needs to be grounded in order for an ASP solver to work, but also each rule must be \textit{safe}. A rule \textit{R} is safe if every variable that occurs in the head of the rule occurs at least once in body\textsuperscript{+}(R) .
Since there is no unique least Herbrand Model for a normal logic program, Stable Model of a normal logic program was defined in \cite{Gelfond1988}. In order to obtain the Stable Model of a program P, P needs to be converted using \textit{Reduct} with respect to an interpretation X. 
\begin{defn}
\begin{itemize}
The \textit{reduct} of P with respect to X can be constructed such that
\item If the body of any rule in P contains an atom which is not in X, those rules need to be removed. 
\item All default negation atoms in the remaining rules in P need to be removed.
\end{itemize}
\end{defn}

\begin{examp} \normalfont Reduct \\


P = $\begin{cases}
	p(X)  \leftarrow not\ q(X). \\
  	q (X) \leftarrow not\ p(X). \\
      \end{cases}$,  X = \{p(a), q(b)\}

Where X is a set of atoms. ground(P) is 

p(a)  $\leftarrow$ not\ q(a). \\
p(b)  $\leftarrow$ not\ q(b). \\
q(a) $\leftarrow$ not\ p(a). \\
q(b) $\leftarrow$ not\ p(b). \\

 The first step removes p(b)  $\leftarrow$ not\ q(b). and q(a) $\leftarrow$ not\ p(a).

p(a)  $\leftarrow$ not\ q(a). \\
q(b) $\leftarrow$ not\ p(b). \\

The second step removes negation atoms from the body. \\
Thus reduct P\textsuperscript{x} is (ground(P))\textsuperscript{x} =  \{p(a), q(b).\}
\end{examp}
\label{reduct}

%Any stable model is a minimal Herbrand model, and stable sets is stable models. The stable models can be found by constructing the result of the program with respect to sets of atoms X (P\textsuperscript{x} in the following 2 steps
A Stable Model of P is an interpretaiton X if and only if X is the unique least Herbrand Model of ground(P)\textsuperscript{x} in the logic program.

\subsection{Anwer Set Programming (ASP) Syntax}

\begin{defn}
Answer set of normal logic program P is a Stable Model, and Answer Set Programming (ASP) is a normal logic program with extensions: constraints, choice rules and optimisation statements. ASP program consists of a set of rules, where each rule consists of an atom and literals.
\end{defn}


A \textit{constraint} of the program P is of the form $\leftarrow$ \textit{a}\textsubscript{1}, ..., \textit{a}\textsubscript{n}, \textit{not b}\textsubscript{1}, ..., \textit{not b}\textsubscript{n}, where the rule has an empty head. The constraint filters any irrelevant answer sets. When computing ground(P)\textsubscript{x}, the empty head becomes $\perp$, which cannot be in the answer sets.
There are two types of constraints: \textit{hard constraints} and \textit{soft constraints}. Hard constraints are strictly satisfied, whereas soft constraints may not be satisfied but the sum of the violations should be minimised when solving ASP.

A \textit{choice rule} can express possible outcomes given an action choice, which is of the form
l\{h\textsubscript{1},...,h\textsubscript{m}\}u $\leftarrow$ a\textsubscript{1}, ..., a\textsubscript{n}, not b\textsubscript{1}, ..., not b\textsubscript{n} where  l and u are integers and h\textsubscript{i} for 1 $\leq$ i $\leq$ m are atoms. The head is called \textit{aggregates}.

\textit{Optimisation statement} is useful to sort the answer sets in terms of preference, which is of the form
\textit{\#minimize[a\textsubscript{1}=w\textsubscript{1},...a\textsubscript{n}=w\textsubscript{n}]} or \textit{\#maximize[a\textsubscript{1}=w\textsubscript{1},...a\textsubscript{n}=w\textsubscript{n}]} where \textit{w\textsubscript{1},..., w\textsubscript{n}} is integer weights and \textit{a\textsubscript{1},...,a\textsubscript{n}} is ground atoms.  ASP solvers compute the scores of the weighted sum of the sets of ground atoms based on the true answer sets, and find optimal answer sets which either maximise or minimise the score.

\textit{Clingo} is one of the modern ASP solvers that executes the ASP program and returns answer sets of the program (\cite{Gebser2011}), and we will use \textit{Clingo} for the implementation of this research.

%An answer set of ASP program is interpretations that make all the ruls true.
%Non-monotonicity.
%TODO ASP has true, false and unknown

\subsection{ILP under Answer Set Semantics}

There are several ILP non-monotonic learning frameworks under the answer set semantics . We first introduce two of them: \textit{Cautious Induction} and \textit{Brave Induction} (\cite{Sakama2009}), which are foundations of \textit{Learning from Answer Sets} discussed in Section \ref{section_lasp}, a state-of-art ILP framework that we will use for our research.  (for other non-monotonic ILP frameworks, see \cite{Otero2001}, \cite{Inoue2014}, \cite{Corapi2012} and \cite{DeRaedt1997}).
\subsubsection{Cautious Induction }
% Sakama 2008 has no concept of negative examples in this paper.
Cautious Induction task \footnote{This is more general definition of Cautious Induction than the one defined in \cite{Sakama2009}, as the concept of negative examples was not included in the original definition.} is of the form $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$, where B is the background knowledge, E\textsuperscript{+} is a set of positive examples and E\textsuperscript{-} is a set of negative examples.

 H $\in$ ILP\textsubscript{cautious} $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ if and only if  there is at least one answer set A of B $\cup$ H (B $\cup$ H is satisfiable) such that for every answer set A of B $\cup$ H: \\
\begin{enumerate}
\item $\forall$ e $\in$ E\textsuperscript{+} : e $\in$ A
\item $\forall$ e $\in$ E\textsuperscript{-} : e $\notin$ A
\end{enumerate}

\begin{examp} \normalfont Cautious Induction \\

B = $\begin{cases}
	exercises  \leftarrow not \ eat\_out. \\
	eat\_out \leftarrow exercises. \\
      \end{cases}$
E\textsuperscript{+} = \{tennis\},      E\textsuperscript{-} = \{eat\_out \} \\

One possible  H $\in$ ILP\textsubscript{cautious} is \{tennis$ \leftarrow$ exercises, $\leftarrow$ not tennis \}.
\end{examp}
\label{cautious_induction_example}

The limitation of Cautious Induction is that positive examples must be true for all answer sets and negative examples must not be included in any of the answer sets. These conditions may be too strict in some cases, and Cautious Induction is not able to accept a case where positive examples are true in some of the answer sets but not all answer sets of the program.

\begin{examp} \normalfont Limitation of Cautious Induction \\

B = $\begin{cases}
	1\{situation(P, awake), situation(P, sleep)\}1 \leftarrow person(P). \\
	person(john).
      \end{cases}$ \\

Neither of \textit{situation(john, awake)} nor \textit{situation(john, sleep)} is false in all answer sets. In this example, it only returns person(john). Thus no examples could be given to learn the choice rule.
\end{examp}

\label{limitation_cautious}

\subsubsection{Brave Induction}
Brave Induction task is of the form $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ where, B is the background knowledge, E\textsuperscript{+} is a set of positive examples and E\textsuperscript{-} is a set of negative examples.
 H $\in$ ILP\textsubscript{brave} $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ if and only if there is at least one answer set A of B $\cup$ H such that: \\
\begin{enumerate}
\item $\forall$ e $\in$ E\textsuperscript{+} : e $\in$ A \\
\item $\forall$ e $\in$ E\textsuperscript{-} : e $\notin$ A \\
\end{enumerate}

\begin{examp} \normalfont Brave Induction \\

B = $\begin{cases}
	exercises  \leftarrow not \ eat\_out. \\
	tennis \leftarrow holiday \\
      \end{cases}$
E\textsuperscript{+} = \{tennis\},   E\textsuperscript{-} = \{eat\_out \} \\

One possible  H $\in$ ILP\textsubscript{brave} is \{tennis\}, which returns \{tennis, holidy, exercises\} as answer sets.
\end{examp}
\label{brave_induction_example}

The limitation of Brave Induction that it cannot learn constraints, since the above conditions for the examples only apply to at least one answer set A, whereas constrains rules out all answer sets that meet  the conditions of the Brave Induction.

\begin{examp} \normalfont Limitation of Brave Induction (Example) \\

B = $\begin{cases}
	1\{situation(P, awake), situation(P, sleep)\}1 \leftarrow person(P). \\
	person(C) \leftarrow super\_person(C). \\
	super\_person(john).
	\end{cases}$ \\

In order to learn the  constraint hypothesis H = \{ $\leftarrow$ not situation(P, awake), super\_person(P)\}, it is not possible to find an optimal solution.
\end{examp}
\label{limitation_brave}

\subsection{Inductive Learning of Answer Set Programs (ILASP)}
\label{section_lasp}

\subsubsection{Learning from Answer Sets (LAS)}
\textit{Learning from Answer Sets (LAS)} was developed in \cite{Law2014} to faciliate more complex learning tasks that neither Cautious Induction nor Brave Induction could learn.
Examples used in LAS are \textit{Partial Interpretations}, which are of the form $\langle$ e\textsuperscript{inc}, e\textsuperscript{exc}$\rangle$. (called \textit{inclusions} and \textit{exclusions} of \textit{e} respectively).  A Herbrand Interpretation extends a partial interpretation if it includes all of e\textsuperscript{inc} and none of e\textsuperscript{exc}.
LAS is of the form $\langle$ B, S\textsubscript{M}, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$, where B is background knowledge, S\textsubscript{M} is hypothesis space, and E\textsuperscript{+} and E\textsuperscript{-} are examples of positive and negative partial interpretations. S\textsubscript{M} consists of a set of normal rules, choice rules and constraints. S\textsubscript{M} is specified by \textit{language bias} of the learning task using \textit{mode declaration}. Mode declaration specifies what can occur in a hypothesis by specifying the predicates, and consists of two parts: \textit{modeh} and \textit{modeb}.  \textit{modeh} and \textit{modeb} are the predicates that can occur in the head of the rule and body of the rule respectively. Language bias is the specification of the language in the hypothesis in order to reduce the search space for the hypothesis.

\begin{defn}{Learning from Answer Sets (LAS)}

Given a learning task T, the set of all possible inductive solutions of T is denoted as ILP\textsubscript{LAS}(T), and a hypothesis H is an inductive solution of ILP\textsubscript{LAS}(T) $\langle$ B, S\textsubscript{M}, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ such that:
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M}
\item $\forall$ e $\in$ E\textsuperscript{+} : $\exists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e
\item $\forall$ e $\in$ E\textsuperscript{-} : $\nexists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e
\end{enumerate}

\end{defn}


%S\textsubscript{M} consists of all rules given the language bias.

%\begin{examp} \normalfont LAS
%
%TODO
%%B = $\begin{cases}
%%	exercises  \leftarrow not \ eat\_out. \\
%%	tennis \leftarrow holiday \\
%%      \end{cases}$ \\
%%E\textsubscript{+} = tennis \\
%%E\textsubscript{-} = eat\_out \\
%
%\end{examp}
%\label{las_example}

% Limitation of LAS??

\subsubsection{Inductive Learning of Answer Set Programs (ILASP)}

\textit{Inductive Learning of Answer Set Programs (ILASP)} is an algorithm that is capable of solving LAS tasks, and is based on two fundamental concepts: \textit{positive solutions} and \textit{violating solutions}.

A hypothesis H is a positive solution if and only if
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M}
\item $\forall$ e\textsuperscript{+} $\in$ $\exists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e\textsuperscript{+}
\end{enumerate}
A hypothesis H is a violating solution if and only if
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M}
\item $\forall$ e\textsuperscript{+} $\in$ E\textsuperscript{+} $\exists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e\textsuperscript{+}
\item $\exists$ e\textsuperscript{-} $\in$ E\textsuperscript{-} $\exists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e\textsuperscript{-}\\
\end{enumerate}

Given both definitions of positive and violating solutions, ILP\textsubscript{LAS} $\langle$ B, S\textsubscript{M}, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$is positive solutions that are not violating solutions.

\subsubsection{A Context-dependent Learning from Answer Sets }
\textit{Context-dependent learning from ordered answer sets ($ILP_{LOAS}^{context}$)} is a further generalisation of ILP\textsubscript{LOAS} with \textit{context-dependent examples} \cite{Law2016}
Context-dependent examples are examples that each unique background knowledge (context) only applies to specific examples. This way the background knowledge is more structured rather than one fixed background knowledge that are applied to all examples.
Formally, partial interpretation is of the form $\langle$ e, C $\rangle$ (called \textit{context-dependent partial interpretation (CDPI)}), where \textit{e} is a partial interpretation and C is called \textit{context}, or an ASP program without weak constraints.
A \textit{context-dependent ordering example (CDOE)} is of the form $\langle$ $\langle$e\textsubscript{1}, C\textsubscript{1} $\rangle$, $\langle$ e\textsubscript{2}, C\textsubscript{2} $\rangle$$\rangle$, which is a pair of CDPI. An APS program P \textit{bravely respects o} if and only if
\begin{enumerate}
 \item $\exists$ $\langle$ A\textsubscript{1}, A\textsubscript{2} $\rangle$ such that A\textsubscript{1} $\in$ Answer Sets(P $\cup$ C\textsubscript{1}),  A\textsubscript{2}, $\in$ Answer Sets(P $\cup$ C\textsubscript{2}), A\textsubscript{1} extends e\textsubscript{1}, A\textsubscript{2} extends e\textsubscript{2} and A\textsubscript{1} $\prec$\textsubscript{P} A\textsubscript{2}
\end{enumerate}

 Similarly, an APS program P \textit{cautiously} respects \textit{o} if and only if
\begin{enumerate}
 \item $\forall$ $\langle$ A\textsubscript{1}, A\textsubscript{2} $\rangle$ such that A\textsubscript{1} $\in$ Answer Sets(P $\cup$ C\textsubscript{1}),  A\textsubscript{2}, $\in$ Answer Sets(P $\cup$ C\textsubscript{2}), A\textsubscript{1} extends e\textsubscript{1}, A\textsubscript{2} extends e\textsubscript{2} and A\textsubscript{1} $\prec$\textsubscript{P} A\textsubscript{2}
\end{enumerate}

$ILP_{LOAS}^{context}$ task is of the form T = $\langle$ B, S\textsubscript{M}, E\textsubscript{+}, E\textsubscript{-}, O\textsuperscript{b},O\textsuperscript{c}$\rangle$ where O\textsuperscript{b} and O\textsuperscript{c} are brave and cautious orderings respectively, which are sets of ordering examples over set of positive partial interpretations E\textsuperscript{+}.
A hypothesis H is an inductive solution of T if and only if
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M} in $ILP_{LOAS}^{context}$
\item $\forall$$\langle$ e, C$\rangle$ $\in$ E\textsuperscript{+}, $\exists$A $\exists$ A $\in$ Answer Sets (B $\cup$ C $\cup$ H) such that A extends e
\item $\forall$$\langle$ e, C$\rangle$ $\in$ E\textsuperscript{-}, $\nexists$A $\exists$ A $\in$ Answer Sets (B $\cup$ C $\cup$ H) such that A extends e
%\item $\forall$o $\in$ O\textsuperscript{b} B $\cup$ H bravely respects o
%\item $\forall$o $\in$ O\textsuperscript{c} B $\cup$ H cautiously respects o
\end{enumerate}

The two main advantages of adding contex-dependent are that it increases the efficiency of learning tasks, and more expressive structure of the background knowlege to particular examples. These features will be useful when a game agent is in two different environmets as discussed in Section \ref{model_generation_and_update}.

\section{Reinforcement Learning (RL)}
\label{rl}
\textit{Reinforcement learning (RL)} is a subfield of machine learning regarding how an agent behaves in an environment in order to maximise its total reward. As shown in Figure \ref{agent_env}, the agent interacts with an environment, and at each time step the agent takes an action and receives observation, which affects the environment state and the reward (or penalty) it receives as the action outcome. In this section, we briefly introduce the background in RL necessary for our research.

\begin{figure}[!htb]
\centering
\includegraphics[width=6cm, height=6cm]{./figures/agent_env}
\caption{Agent and Environment}
\label{agent_env}
\end{figure}

\subsection{Markov Decision Process (MDP)}
\label{mdp_subsection}
An agent interacts with an environment at a sequence of discrete time step, which is part of the sequential history of observations, actions and rewards. The sequential history is formalised as H\textsubscript{t} = O\textsubscript{1}, R\textsubscript{1}, A\textsubscript{1}, ..., A\textsubscript{t-1}, O\textsubscript{t}, R\textsubscript{t}.  A \textit{state} is a function of the history S\textsubscript{t} = f(H\textsubscript{t}), which determines the next environment.  A state S\textsubscript{t} is said to have \textit{Markov property} if and only if P[S\textsubscript{t+1} $\vert$ S\textsubscript{t}] = P[S\textsubscript{t+1} $\vert$ S\textsubscript{1}, ..., S\textsubscript{t}]. In other words, the probability of reaching S\textsubscript{t+1} depends only on S\textsubscript{t}, which captures all the relevant information from the earilier history (\cite{Puterman1994}).
When an agent must make a sequence of decision, the sequential decision problem can be formalised using \textit{Markov decision process (MDP)}. MDP formaly represents a fully observable environment of an agent for RL.

A MDP is of the form $\langle$ S, A, T\textsubscript{a}, R\textsubscript{a}, $\gamma$ $\rangle$ where:

\begin{itemize}
\item S is the set of finite states that is observable in the environment.
\item A is the set of finite actions taken by the agent.
\item T\textsubscript{a}(s, s$^\prime$) is a state transition in the form of probability matrix Pr(S\textsubscript{t+1} = s$^\prime$ $\vert$ s\textsubscript{t} = s, a\textsubscript{t} = a), which is the probablity that action \textit{a} in state \textit{s} at time \textit{t} will result in state \textit{s$^\prime$} at time \textit{t+1}.
\item R is a reward function R\textsubscript{a}(s, s$^\prime$) = $\displaystyle \E[R\textsubscript{t+1} $ $\vert$ S\textsubscript{t} = s, A\textsubscript{t} = a], the expected immediate reward that action \textit{a} in state \textit{s} at time \textit{t} will return.
\item $\gamma$ is a discount factor $\gamma$ $\in$ [0,1], which represents the preference of the agent for the present reward over future rewards.
\end{itemize}

%$\displaystyle \E[R\textsubscript{t+1} \vert$ S\textsubscript{t} = s]
\subsection{Policies and Value Functions}
\label{policy_value_functions_subsection}

\textit{Value functions} estimate the expected return, or expected future rewarded,  for a given action in a given state. The expected reward for an agent is dependent on the agent's action.  The state value function v\textsubscript{$\pi$}(s) of an MDP under a policy $\pi$ is the expected return starting from state \textit{s}, which is of the form:

\begin{equation}
v\textsubscript{$\pi$}(s) = \displaystyle \E [G\textsubscript{t} \vert S\textsubscript{t} = s]
\end{equation}

where G\textsubscript{t} = R\textsubscript{t+1} + $\gamma$R\textsubscript{t+2} + ... $\gamma$\textsuperscript{T-1}R\textsubscript{T} , or the total discounted reward from \textit{t}.

The optimal state-value function v\textsuperscript{*}(s) maximises the value function over all policies in the MDP, which is of the form:

\begin{equation}
v\textsuperscript{*}(s) = \underset{\pi}{\max} \ v\textsubscript{$\pi$}(s)
\end{equation}

The optimal action-value function q\textsuperscript{*}(s) maximises the action-value function over all policies in the MDP, which is of the form:

\begin{equation}
q\textsuperscript{*}(s, a) = \underset{\pi}{\max} \ q\textsubscript{$\pi$}(s, a)
\end{equation}

A solution to the sequential decision problem is called a \textit{policy $\pi$}, a sequence of actions that leads to a solution. An optimal policy achieves the optimal value function (or action-value function), and it can be computed by maximising over the optimal value function (or action-value function).

\textcolor{red}{TODO BELLMAN OPTIMALITY EQUATION}
%A policy $\pi$ is optimal if it maximises the action-value function q()
%The transition and reward functions are not necessary to be known to compute $\pi$.
%Among all possible value functions, an \textit{optimal policy $\pi^*$} is the one that maximise the total rewards in the environment.
%The optimal policy $\pi$\textsuperscript{*} corresponds to v\textsuperscript{*}(s)

%\begin{equation}
%\pi \textsuperscript{*} =
%%\pi \textsuperscript{*} = arg max v\textsubscript{\pi}(s)
%\end{equation}

%Reinforcement learning is a method to get approximated optimal solution.
%TODO on-policy and off-policy learning.

%A\textsubscript{t} = $\pi$(S\textsubscript{t})
%
%One of the common possibilities is that the agent chooses an action in order to maximise the discounted sum of future rewards.
%
%A\textsubscript{t} to maximise R\textsubscript{t+1} + $\gamma$ R\textsubscript{t+2} + $\gamma^2$ R\textsubscript{t+3} + ...
%
%An action-value function evaluate a particular state by taking an action according the policy
%
%q\textsubscript{$\pi$} = $\displaystyle \E[R\textsubscript{t+1} $[R\textsubscript{t+1} + $\gamma$ R\textsubscript{t+2} + $\gamma^2$ R\textsubscript{t+3} + ... $\vert$ S\textsubscript{t} = s, A\textsubscript{t} = a, A\textsubscript{t+1 } = a,]

\textcolor{red}{TODO Value iterations}

\subsection{Model-based and Model-free Reinforcement Learning}
\label{model_base_model_free_subsection}

\textcolor{red}{TODO delte dyna and focus more on model-based approach}

A model M is a representation of an environment that an agent can used to understand how the environment should look like . Model-based learning is that the agent learns the model and plan a solution using the learnt model. Once the agent learns the model, the problem to be solved becomes a planning problem for a series of actions to achieve the agent's goal.
Most of the reinforcement learning problems are model-free learning, where M is unknown and the agent learns to achieve the goal by solely interacting with the environment. Thus the agent knows only possible states and actions, and the transition state and reward probability functions are unknown.

The performance of model-based RL is limited to optimal policy given the model M. In other words, when the model is not a representation of the true MDP, the planning algorithms will not lead to the optimal policy, but a suboptimal policy.

One algorithm which combine both aspects of model-based and model-free learning to solve the issue of sub-optimality is called Dyna (\cite{Sutton1990}), which is shown in Figure \ref{dyna}.

\begin{figure}[!htb]
\centering
\includegraphics[width=8cm, height=6cm]{./figures/dyna}
%\caption{Relationships among learning, planning and acting \cite{Montague1999}}
\caption{Relationships among learning, planning and acting}
\label{dyna}
\end{figure}

Dyna learns a model from real experience and use the model to generate simulated experience to update the evaluation functions.
This approach is more effective because the simulated experience is relatively easy to generate compared building up real experience, thus less iterations are required.

\subsection{Temporal-Difference (TD) Learning}
\label{td_learning_section}

To solve a MDP, one of the approaches is called \textit{Temporal-Difference (TD) Learning}.
TD is an online model-free learning and learns directly from episodes of imcomplete experiences without a model of the environment.
TD updates the estimate by using the estimates of value function by bootstrap, which is formalised as

\begin{equation}
\centering
V(S\textsubscript{t}) \leftarrow V(S\textsubscript{t}) + \alpha[R\textsubscript{t+1} + \gamma V(S\textsubscript{t+1}) - V(S\textsubscript{t})]
\end{equation}

where R\textsubscript{t+1} + $\gamma$ V(S\textsubscript{t+1}) is the target for TD update, which is biased estimated of v\textsubscript{$\pi$} (S\textsubscript{t}), and $\delta$ = R\textsubscript{t+1} + $\gamma$ V(S\textsubscript{t+1}) - V(S\textsubscript{t}) is called TD error, which is the error in V(S\textsubscript{t}) available at time t+1.
Since TD methods only needs to know the estimate of one step ahead and does not need the final outcome of the episodes, it can learn online after every time step. TD also works without the terminal state, which is the goal for an agent.
TD(0) is proved to converge to v\textsubscript{$\pi$} in the table-based case (non-function approximation).
However, because bootstraping updates an estimate for an estimate, some bias are inevitable.
%In addition, TD method is sensitive to initial value, (but low variance).

%\begin{equation}
%\textbf{	V(S\textsubscript{t} \leftarrow V(S\textsubscript{t} + \alpha (R\textsubscript{t+1} + \gamma V(S\textsubscript{t+1}) - V(S\textsubscript{t}))
%}\end{equation}

\textit{Q-learning} is off-policy TD learning defined in \cite{Watkins}, where the agent only knows about the possible states and actionns. The transition states and reward probability functions are unknown to the agent.
It is of the form:

\begin{equation}
Q(s\textsubscript{t},a\textsubscript{t}) \leftarrow Q(s\textsubscript{t},a\textsubscript{t}) +  \alpha(R\textsubscript{t+1} + \gamma  max (a+t) Q(s\textsubscript{t+1}, a\textsubscript{t+1}) - Q(s\textsubscript{t}, a\textsubscript{t}))
\end{equation}

where $\alpha$ is the learning rate, $\gamma$ is a discount rate between 0 and 1. The equation is used to update the state-action value function called Q function. The function Q(S,A) predicts the best action A in state S to maximise the total cumulative rewards.

\begin{algorithm}
\caption{Q-learning (off-policy TD control)}\label{euclid}
\begin{algorithmic}[1]
\Procedure{ILASP(RL) (B and E)}{}

\State $\text{Initialise Q(s,a) arbitrarily}$
\State $\text{Repeat (for each episode)}$
\State $\text{Choose a from s using policy derived from Q (e.g, epsilon-greedy)}$
\State $\text{Take action a, observe r,  s$\prime$}$


%\begin{equation}
%Q(s,a) \leftarrow Q(s,a) +  \alpha(R\textsubscript{t+1} + \gamma  max (a+t) Q(s$\prime$, a$\prime$) - Q(s\textsubscript{t}, a\textsubscript{t}))
%s \gets s$\prime$
%\end{equation}

%    \State $\textit{H (inductive solutions)} \gets \text{run ILASP(T)}$
%    \State $\textit{plan(actions, states) answer sets} \gets \text{AS(B, H)}$
%    \While {actions in P}
%        \State $\textit{observed state} \gets \text{run clingo(T)}$
%        \If {$ \textit{observed state} \neq \textit{predicted state} $}
%            \State $\textit{H} \gets \text{run ILASP(T)}$
%            \EndIf
        % \If {$ \textit{observed state not equal \textit{predicted state $} 
        % \EndIf
%    \EndWhile
    % \If {$ new \ background \ encountered $}
    %     % \State $\textit{H} \gets \text{run ILASP(T)}$
    % \EndIf
    % \For{i from 0 to N} 
    %     \If {$ A[i]\ is \ in \ T$}
    %         \State \Return $FALSE$
    %     \Else 
    %         \State Add A[i] to T   
    %     \EndIf
    % \EndFor
% \State \Return $TRUE$
%\EndWhile

\EndProcedure
\caption{XXXX }
\end{algorithmic}
\end{algorithm}

\begin{equation}
Q(s\textsubscript{t},a\textsubscript{t}) = E [R\textsubscript{t+1} + \gamma R\textsubscript{t+2} + \gamma^2 R\textsubscript{t+3} + ... \vert s\textsubscript{t},a\textsubscript{t} ]
\end{equation}


Q-learning is guaranteed to converge to a optimal policy in a fininte tabulara representation.
\textcolor{red}{Paper Jaakkola et al. 1993}

The optimal Q-function Q\textsuperscript{*}(s,a) is directly approximated by the learned action-value function Q.

% Model free can be done using Monte Carlo Policy evaluation
% One way to solve the Bellman Optimality equation is Q-leraning
% U(s) = max a Q(s,a)
% The function is estimated by Q-learning, which repeately updates Q(s,a) using the Bellman Equation.
Q-learning learns the value of its deterministic greedy policy from the experience and gradually converge to the optimal Q-function. It also explored following \textit{$\epsilon$-greedy policy}, which is a stochastic greedy policy, but with the probability of $\epsilon$, the agent chooses an action randomly instea of the greedy action.

\subsection{Function Approximation}
\label{function_approximation}
Q-learning with tarbular method works when every state has Q(s,a). In case of very large MDPs, however, 
it may not be possible to represent all states with a lookup table.
For example,  robot arms has a continuous states in 3D dimentional space. 

These problems motivate the use of function approximation, which estimates value function with function approximation.  Not only it is  represented in tabular form, but also in the form of a parameterized function with weight vector w $\in \R\textsuperscript{d}$ where $\R\textsuperscript{d}$ is XXX

Unlike Q-table, changing one weight updates the estimated value of not only one state, but many states, and this generalisation makes it more flexible to apply different scenarios that tabular approach could not be applied. 

The reason we are introduing this function approximation is not because we will use it in our new algorithm, but for the benchmark that we compare our algorithm with. 

\subsubsection{The Prediction Objective ($\overline{VE}$)}
With function approximation, an update at one state changes many other states, and therefore the values of all states will not be exactly accurate, and there is a tradeoff among states as to which state we make it more accurate, while other might be  less accurate. 

The error in a state s is the squeare of the difference between the approximate value $\hat{v}$(s,w) and the true value v\textsubscript{$\pi$}(s). The objective function can be defined by weighting it over the statespace by $\mu$, the \textit{Mean Squared Value Error}, denoted $\overline{VE}$.

\begin{equation}
\overline{VE}(w) \doteq \sum_{s \in S} \mu (s) \big[ v\textsubscript{$\pi$}(s) - \hat{v}(s,w) \big]\textsuperscript{2}.
\end{equation}
\label{ve}

\subsubsection{Stochastic gradient descent (SGD)}
Stochastic gradient descent methods are commonly used to learn function approximation in value prediction, which works well for online reinforcement learning.
TODO EXPLAIN ONLINE VS OFFLINE LEARNING

\begin{equation}
 w \doteq  \begin{pmatrix}  w\textsubscript{1}  \\ w\textsubscript{2} \\ \vdots \\ w\textsubscript{n}   \end{pmatrix}
\end{equation}

and $\hat{v}$(s,w) is a differentiable function of w for all s $\in$ S. 

minimize the $\overline{VE}$ on the observed examples. \textit{Stochastic gradient-descent (SGD) } adjusts the weights vector by a fraction of alpha in the direction what will reduce the error on that example the most. Formally, it is defined as 

\begin{equation}
\begin{split}
w\textsubscript{t+1} & \doteq w\textsubscript{t} -  \frac{1}{2} \alpha  \bigtriangledown \big[ v\textsubscript{$\pi$}(S\textsubscript{t}) - \hat{v}(S\textsubscript{t}, w\textsubscript{t}) \big]\textsuperscript{2}. \\
& = w\textsubscript{t} -  \alpha  \big[ v\textsubscript{$\pi$}(S\textsubscript{t}) - \hat{v}(S\textsubscript{t}, w\textsubscript{t}) \big] \bigtriangledown \hat{v}(S\textsubscript{t}, w\textsubscript{t}).
\end{split}
\end{equation}

where $\alpha$ is step-size, 

The dradient of J(w) is defined as 

\begin{equation}
\bigtriangledown\textsubscript{w} J(w) =  \begin{pmatrix} \frac{\partial J(w)}{\partial w\textsubscript{1}}  \\ \vdots \\ \frac{\partial J(w)}{\partial w\textsubscript{n}}   \end{pmatrix}
\end{equation}

\begin{equation}
w\textsubscript{t+1} \doteq w\textsubscript{t} + \alpha \big[ U\textsubscript{t} - \hat{v}(S\textsubscript{t}, w\textsubscript{t})\big] x(S\textsubscript{t})
\end{equation}

\subsubsection{Linear Value Function Approximation}

Formally,
\begin{equation}
\hat{q}(s,a) \approx q\textsubscript{$\pi$} (s,a)
\end{equation}

Represent state by a \textit{feature vector}

\begin{equation}
x(S) = \begin{pmatrix} x\textsubscript{1}(S) \\ \vdots \\ x\textsubscript{n}(S)  \end{pmatrix}
\end{equation}

Use SGD updates with linear function approximation. The gradient of the approximate value function with respect to w is 

\textcolor{red}{Add proof here}

\begin{equation}
\bigtriangledown \hat{v}(s,w) = x(s)
\end{equation}

Thus the general SGD update defined in XX can be simplified to 

Represent value function by a linear combination of features

\begin{equation}
\hat{v}(S,w) = x(S)\textsuperscript{T}w = \sum_{j=1}^{n} x\textsubscript{j} (S) w\textsubscript{j}
\end{equation}

Objective function is 
The error in a state s is the square of the difference between the approximate value 
$\hat{v}$(S,w) and the true value v(S,w). 
\begin{equation}
J(w) = \displaystyle \E \textsubscript{$\pi$} \big[ (v\textsubscript{$\pi$}(S) -  \hat{v}(S,w))\textsuperscript{2} \big]
\end{equation}

Linear TD(0) is guaranteed to converge to clobal optimum

One disadvantage of the linear method is that it cannot express any relationship between features. For example, it cannot represent that feature \textit{i} is useful only if feature \textit{j} is not present. 

Nevertheless,  this approach is sufficient enough for our experiment, which will be described in Chapter XXX.

There are different linear methods to represents states as features, such as polynomials, fourier basis, or radial basis functions to name a few. Feature construction depends on a problem you are solving. In the next section, we introduce \textit{Tile Coding} which will be used for our benchmark. 

\subsubsection{Tile Coding \textcolor{red}{TODO REFERENCE OF THIS METHOD}}

State set is represented as a continuous two-dimentionala space. If a state is within the space, then the value of the  corrensponding feature is set to be 1 to indicate that the feature is present, while 0 indicates that the feature is absent. This way of representing the feature is called \textit{binary feature}.  \textit{Coarse coding} represents a state with which binary features are present within the space. 
One area is associated with one weight w, and training at a state will affect the weight of all the areas overlapping that state. the approximate value function will be updated within at all states within the union of the areas, and a point that has more overlap will be more affected, as illustrated in Figure XX.

The size and shape of the areas will determine the degree of the generalisation. Large areaas will have more generalisation
the change of the weight in that state will affect all other states within the intersection of the spaces. 

The degree of overlap within a space will determined the degree of the generalisation. 

The shape of the space also affect how it is generalised. 

\textit{Tile coding} is a type of coarse coding. \textit{Tiling} is a partition of state space, and each element of the partition is called a \textit{tile}. 

In order to do coarse coding with tile coding, multiple tilings are required, each tiling is offset from one another by a fraction of a tile width. 

As illustrated in Figure XXX, when a state occurs, several features with corresponding tiles become active, 

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{./figures/tile_coding}
\caption{Tiling illustration}
\label{dyna}
\end{figure}

Tile coding has computational advantage, since each component of tiling is binary value,  XXXX.

a trained state will be generalised to other states if they are within any of the same tiles. 

Similar to coarse coding, the size and shape of tiles will determine the degree of approximation. 

\subsection{Transfer Learning}
\label{transfer_learning}

Transfer learning is a method that knowledge learnt in one or more tasks can be used to learn a new task better than if without the knowlege in the first task. 

Transfer learning is an active research areas in machine learning, but not many have been done in RL. 
Since training tend to be time consuming and computational expensive, transfer learning allow the trained model to be applied in a different setting. 

Transfer learning in RL is particularly important since most of the RL research have been done in a simulation or game scenarios, and training RL models in a real physical environment is more expensive to conduct. 

Even in a virtual environments like games,  the transfer learning between different tasks will greatly will have a big impact on potential applications. 

This will also speed up learning 

Transfer learning in ILP domain have been proved to be successful in many fields, 

Since this project is combining ILP into RL senarios, this has a potential for extending this particular research. 

We conducted experiements on transfer learning capabilities, which we describe in XXX. 

One of the  purposes of transfer learning is so that the agent requires less time to learn a new task with the help of what was learn in previous tasks.  


Another goal would be to measure how effectively the agent reuses its knoledge in a new task. 
In this case the performan of learningon the first task is usally not measured. 

There are many different matrices used to measure the performance of the transfer learning.
Five common matrics are defined in XX as follow.


TODO source task selection

%Compare with human play, which is used in DeepMind Atari paper. 


\begin{itemize}
\item Jumpstart
\item Asymptotic Performance
\end{itemize}

Since each matric measures different aspect of transfer learning, using multiple metrics would provide more comprehensive views of the performance of an RL algorithm.

\begin{equation}
\begin{split}
r = \frac{Area under curve with transfer - area under curve without transfer}{area under curve without transfer}
\end{split}
\end{equation}
REFERENCE


\chapter{Framework}
\label{framework}
\input{chapters/framework}

\chapter{Evaluation}
\label{evaluation}
\input{chapters/evaluation}



\chapter{Related Work}
\label{related_work}
\input{chapters/related_work}


\chapter{Discussion}
\label{discussion}
\input{chapters/discussion}

\chapter{Conclusion}
\label{conclusion}

\clearpage
\appendix
\pagebreak
\begin{appendices}

\input{chapters/appendix_ethics}
\input{chapters/appendix_learning_task}
\input{chapters/appendix_asp}

\end{appendices}




%% bibliography
\bibliography{references}
\bibliographystyle{ieeetr}

\end{document}
